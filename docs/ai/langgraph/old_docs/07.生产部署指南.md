---
title: LangGraph 生产部署指南
date: 2025-09-30
permalink: /ai/langgraph/production-deployment.html
tags:
  - AI
  - LangGraph
  - 部署
  - DevOps
categories:
  - AI
  - LangGraph
---

# LangGraph 生产部署指南

## 概述

将 LangGraph 应用部署到生产环境需要考虑性能、可靠性、安全性和可维护性。本指南提供完整的生产部署方案。

```mermaid
graph TB
    subgraph "部署架构"
        A[负载均衡器] --> B[API Gateway]
        B --> C[LangGraph Service 1]
        B --> D[LangGraph Service 2]
        B --> E[LangGraph Service N]

        C --> F[Redis Cache]
        D --> F
        E --> F

        C --> G[PostgreSQL]
        D --> G
        E --> G

        C --> H[Vector DB]
        D --> H
        E --> H

        I[监控系统] --> C
        I --> D
        I --> E

        J[日志系统] --> C
        J --> D
        J --> E
    end

    style A fill:#f96,stroke:#333,stroke-width:2px
    style B fill:#9cf,stroke:#333,stroke-width:2px
    style I fill:#fc9,stroke:#333,stroke-width:2px
```

## 1. 容器化部署

### 1.1 Docker 配置

```dockerfile
# Dockerfile
FROM python:3.11-slim

# 设置工作目录
WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .

# 安装 Python 依赖
RUN pip install --no-cache-dir -r requirements.txt

# 复制应用代码
COPY . .

# 创建非 root 用户
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# 暴露端口
EXPOSE 8000

# 启动命令
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### 1.2 Docker Compose 配置

```yaml
# docker-compose.yml
version: '3.8'

services:
  langgraph-app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=production
      - DATABASE_URL=postgresql://user:password@postgres:5432/langgraph
      - REDIS_URL=redis://redis:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - postgres
      - redis
      - vectordb
    volumes:
      - ./logs:/app/logs
      - ./checkpoints:/app/checkpoints
    restart: unless-stopped
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: langgraph
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  vectordb:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    volumes:
      - vectordb_data:/qdrant/storage

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - langgraph-app

volumes:
  postgres_data:
  redis_data:
  vectordb_data:
```

### 1.3 Kubernetes 部署

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph-app
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: langgraph
  template:
    metadata:
      labels:
        app: langgraph
    spec:
      containers:
      - name: langgraph
        image: your-registry/langgraph:latest
        ports:
        - containerPort: 8000
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: langgraph-secrets
              key: database-url
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: langgraph-secrets
              key: openai-api-key
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: langgraph-service
  namespace: production
spec:
  selector:
    app: langgraph
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langgraph-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langgraph-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## 2. 应用架构

### 2.1 FastAPI 服务器

```python
# main.py
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from typing import Optional, Dict, Any
import logging
from langgraph.graph import StateGraph
import asyncio
from contextlib import asynccontextmanager

# 配置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 全局变量存储工作流
workflows = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """应用生命周期管理"""
    # 启动时初始化
    logger.info("Initializing LangGraph workflows...")
    workflows["main"] = create_main_workflow()
    workflows["secondary"] = create_secondary_workflow()
    yield
    # 关闭时清理
    logger.info("Cleaning up resources...")
    workflows.clear()

app = FastAPI(
    title="LangGraph Production API",
    version="1.0.0",
    lifespan=lifespan
)

# CORS 配置
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 请求模型
class WorkflowRequest(BaseModel):
    workflow_name: str
    input_data: Dict[str, Any]
    config: Optional[Dict[str, Any]] = None

class WorkflowResponse(BaseModel):
    request_id: str
    status: str
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

# 健康检查
@app.get("/health")
async def health_check():
    return {"status": "healthy", "workflows": list(workflows.keys())}

@app.get("/ready")
async def readiness_check():
    # 检查依赖服务
    try:
        # 检查数据库连接
        await check_database()
        # 检查缓存连接
        await check_cache()
        return {"status": "ready"}
    except Exception as e:
        raise HTTPException(status_code=503, detail=str(e))

# 主要端点
@app.post("/workflow/execute", response_model=WorkflowResponse)
async def execute_workflow(
    request: WorkflowRequest,
    background_tasks: BackgroundTasks
):
    """执行工作流"""
    try:
        workflow_name = request.workflow_name
        if workflow_name not in workflows:
            raise HTTPException(status_code=404, detail=f"Workflow {workflow_name} not found")

        workflow = workflows[workflow_name]
        request_id = generate_request_id()

        # 异步执行工作流
        background_tasks.add_task(
            run_workflow_async,
            request_id,
            workflow,
            request.input_data,
            request.config
        )

        return WorkflowResponse(
            request_id=request_id,
            status="processing",
            result=None
        )

    except Exception as e:
        logger.error(f"Error executing workflow: {e}")
        return WorkflowResponse(
            request_id="",
            status="error",
            error=str(e)
        )

@app.get("/workflow/status/{request_id}")
async def get_workflow_status(request_id: str):
    """获取工作流状态"""
    status = await get_status_from_cache(request_id)
    if not status:
        raise HTTPException(status_code=404, detail="Request not found")
    return status

# 流式响应
@app.post("/workflow/stream")
async def stream_workflow(request: WorkflowRequest):
    """流式执行工作流"""
    from fastapi.responses import StreamingResponse

    async def generate():
        workflow = workflows.get(request.workflow_name)
        if not workflow:
            yield f"data: {{\"error\": \"Workflow not found\"}}\n\n"
            return

        config = request.config or {}
        async for chunk in workflow.astream(request.input_data, config):
            yield f"data: {json.dumps(chunk)}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")

# 批处理端点
@app.post("/workflow/batch")
async def batch_execute(requests: List[WorkflowRequest]):
    """批量执行工作流"""
    results = []

    async def execute_single(req):
        try:
            workflow = workflows.get(req.workflow_name)
            if not workflow:
                return {"error": f"Workflow {req.workflow_name} not found"}

            result = await workflow.ainvoke(req.input_data, req.config)
            return {"success": True, "result": result}
        except Exception as e:
            return {"success": False, "error": str(e)}

    # 并行执行所有请求
    tasks = [execute_single(req) for req in requests]
    results = await asyncio.gather(*tasks)

    return {"batch_results": results}

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        workers=4,
        loop="uvloop",
        log_level="info"
    )
```

### 2.2 工作流管理

```python
# workflow_manager.py
from typing import Dict, Any, Optional
import pickle
import json
from datetime import datetime
import redis
from sqlalchemy import create_engine, Column, String, DateTime, JSON
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

Base = declarative_base()

class WorkflowExecution(Base):
    """工作流执行记录"""
    __tablename__ = 'workflow_executions'

    id = Column(String, primary_key=True)
    workflow_name = Column(String)
    status = Column(String)
    input_data = Column(JSON)
    output_data = Column(JSON)
    error = Column(String)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class WorkflowManager:
    """工作流管理器"""

    def __init__(self, db_url: str, redis_url: str):
        # 数据库连接
        self.engine = create_engine(db_url)
        Base.metadata.create_all(self.engine)
        self.SessionLocal = sessionmaker(bind=self.engine)

        # Redis 连接
        self.redis_client = redis.from_url(redis_url)

        # 工作流注册表
        self.workflows = {}

    def register_workflow(self, name: str, workflow: Any):
        """注册工作流"""
        self.workflows[name] = workflow
        # 序列化并存储到 Redis
        serialized = pickle.dumps(workflow)
        self.redis_client.set(f"workflow:{name}", serialized, ex=3600)

    def get_workflow(self, name: str) -> Optional[Any]:
        """获取工作流"""
        # 先从内存获取
        if name in self.workflows:
            return self.workflows[name]

        # 从 Redis 获取
        serialized = self.redis_client.get(f"workflow:{name}")
        if serialized:
            workflow = pickle.loads(serialized)
            self.workflows[name] = workflow
            return workflow

        return None

    async def execute_workflow(
        self,
        workflow_name: str,
        input_data: Dict,
        config: Optional[Dict] = None
    ) -> Dict:
        """执行工作流并记录"""
        execution_id = self.generate_execution_id()

        # 创建执行记录
        session = self.SessionLocal()
        execution = WorkflowExecution(
            id=execution_id,
            workflow_name=workflow_name,
            status="running",
            input_data=input_data
        )
        session.add(execution)
        session.commit()

        try:
            # 获取工作流
            workflow = self.get_workflow(workflow_name)
            if not workflow:
                raise ValueError(f"Workflow {workflow_name} not found")

            # 执行工作流
            result = await workflow.ainvoke(input_data, config)

            # 更新执行记录
            execution.status = "completed"
            execution.output_data = result
            session.commit()

            # 缓存结果
            self.cache_result(execution_id, result)

            return result

        except Exception as e:
            # 记录错误
            execution.status = "failed"
            execution.error = str(e)
            session.commit()
            raise

        finally:
            session.close()

    def cache_result(self, execution_id: str, result: Dict):
        """缓存执行结果"""
        self.redis_client.setex(
            f"result:{execution_id}",
            3600,  # 1小时过期
            json.dumps(result)
        )

    def get_execution_status(self, execution_id: str) -> Optional[Dict]:
        """获取执行状态"""
        # 先从缓存获取
        cached = self.redis_client.get(f"result:{execution_id}")
        if cached:
            return json.loads(cached)

        # 从数据库获取
        session = self.SessionLocal()
        execution = session.query(WorkflowExecution).filter_by(id=execution_id).first()
        if execution:
            return {
                "status": execution.status,
                "result": execution.output_data,
                "error": execution.error
            }
        session.close()

        return None

    def generate_execution_id(self) -> str:
        """生成执行ID"""
        import uuid
        return str(uuid.uuid4())
```

## 3. 性能优化

### 3.1 缓存策略

```python
# caching.py
from functools import lru_cache, wraps
import hashlib
import json
import redis
from typing import Any, Dict, Optional
import asyncio

class CacheManager:
    """缓存管理器"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.local_cache = {}

    def cache_key(self, prefix: str, params: Dict) -> str:
        """生成缓存键"""
        param_str = json.dumps(params, sort_keys=True)
        hash_str = hashlib.md5(param_str.encode()).hexdigest()
        return f"{prefix}:{hash_str}"

    def workflow_cache(self, ttl: int = 3600):
        """工作流缓存装饰器"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # 生成缓存键
                cache_key = self.cache_key(
                    func.__name__,
                    {"args": str(args), "kwargs": str(kwargs)}
                )

                # 检查缓存
                cached = self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)

                # 执行函数
                result = await func(*args, **kwargs)

                # 存储到缓存
                self.redis.setex(
                    cache_key,
                    ttl,
                    json.dumps(result)
                )

                return result

            return wrapper
        return decorator

    async def get_or_compute(
        self,
        key: str,
        compute_func: callable,
        ttl: int = 3600
    ) -> Any:
        """获取或计算"""
        # 检查缓存
        cached = self.redis.get(key)
        if cached:
            return json.loads(cached)

        # 使用分布式锁避免缓存击穿
        lock_key = f"lock:{key}"
        lock = self.redis.lock(lock_key, timeout=10)

        if lock.acquire(blocking=False):
            try:
                # 再次检查缓存（双重检查）
                cached = self.redis.get(key)
                if cached:
                    return json.loads(cached)

                # 计算结果
                result = await compute_func()

                # 存储到缓存
                self.redis.setex(key, ttl, json.dumps(result))

                return result
            finally:
                lock.release()
        else:
            # 等待其他进程完成计算
            await asyncio.sleep(0.1)
            return await self.get_or_compute(key, compute_func, ttl)

# 使用示例
cache_manager = CacheManager(redis.from_url("redis://localhost:6379"))

@cache_manager.workflow_cache(ttl=1800)
async def expensive_workflow(input_data: Dict) -> Dict:
    """昂贵的工作流操作"""
    # 执行复杂计算
    result = await complex_computation(input_data)
    return result
```

### 3.2 连接池管理

```python
# connection_pool.py
from contextlib import contextmanager
import psycopg2
from psycopg2 import pool
import redis
from typing import Optional
import threading

class ConnectionPoolManager:
    """连接池管理器"""

    def __init__(self):
        self._postgres_pool = None
        self._redis_pool = None
        self._lock = threading.Lock()

    def init_postgres_pool(
        self,
        host: str,
        port: int,
        database: str,
        user: str,
        password: str,
        min_conn: int = 2,
        max_conn: int = 10
    ):
        """初始化 PostgreSQL 连接池"""
        with self._lock:
            if not self._postgres_pool:
                self._postgres_pool = psycopg2.pool.ThreadedConnectionPool(
                    min_conn,
                    max_conn,
                    host=host,
                    port=port,
                    database=database,
                    user=user,
                    password=password
                )

    def init_redis_pool(
        self,
        host: str,
        port: int,
        db: int = 0,
        max_connections: int = 50
    ):
        """初始化 Redis 连接池"""
        with self._lock:
            if not self._redis_pool:
                self._redis_pool = redis.ConnectionPool(
                    host=host,
                    port=port,
                    db=db,
                    max_connections=max_connections,
                    decode_responses=True
                )

    @contextmanager
    def get_postgres_connection(self):
        """获取 PostgreSQL 连接"""
        conn = self._postgres_pool.getconn()
        try:
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            self._postgres_pool.putconn(conn)

    def get_redis_client(self) -> redis.Redis:
        """获取 Redis 客户端"""
        return redis.Redis(connection_pool=self._redis_pool)

    def close_all(self):
        """关闭所有连接池"""
        if self._postgres_pool:
            self._postgres_pool.closeall()
        if self._redis_pool:
            self._redis_pool.disconnect()

# 单例模式
_pool_manager = ConnectionPoolManager()

def get_pool_manager() -> ConnectionPoolManager:
    return _pool_manager
```

## 4. 监控和日志

### 4.1 Prometheus 监控

```python
# monitoring.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from prometheus_client import CONTENT_TYPE_LATEST
from fastapi import Response
import time
from functools import wraps

# 定义指标
workflow_executions = Counter(
    'langgraph_workflow_executions_total',
    'Total number of workflow executions',
    ['workflow_name', 'status']
)

workflow_duration = Histogram(
    'langgraph_workflow_duration_seconds',
    'Workflow execution duration in seconds',
    ['workflow_name']
)

active_workflows = Gauge(
    'langgraph_active_workflows',
    'Number of currently active workflows'
)

llm_requests = Counter(
    'langgraph_llm_requests_total',
    'Total number of LLM requests',
    ['model', 'status']
)

llm_tokens = Counter(
    'langgraph_llm_tokens_total',
    'Total number of tokens processed',
    ['model', 'type']
)

def monitor_workflow(workflow_name: str):
    """监控工作流执行"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # 增加活动工作流计数
            active_workflows.inc()

            # 记录开始时间
            start_time = time.time()

            try:
                # 执行工作流
                result = await func(*args, **kwargs)

                # 记录成功执行
                workflow_executions.labels(
                    workflow_name=workflow_name,
                    status='success'
                ).inc()

                return result

            except Exception as e:
                # 记录失败执行
                workflow_executions.labels(
                    workflow_name=workflow_name,
                    status='failure'
                ).inc()
                raise e

            finally:
                # 记录执行时间
                duration = time.time() - start_time
                workflow_duration.labels(workflow_name=workflow_name).observe(duration)

                # 减少活动工作流计数
                active_workflows.dec()

        return wrapper
    return decorator

# 添加 metrics 端点
@app.get("/metrics")
async def get_metrics():
    """Prometheus metrics endpoint"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

### 4.2 结构化日志

```python
# logging_config.py
import logging
import json
from pythonjsonlogger import jsonlogger
from typing import Any, Dict
import traceback

class CustomJsonFormatter(jsonlogger.JsonFormatter):
    """自定义 JSON 日志格式化器"""

    def add_fields(self, log_record: Dict, record: logging.LogRecord, message_dict: Dict):
        super().add_fields(log_record, record, message_dict)

        # 添加时间戳
        log_record['timestamp'] = record.created

        # 添加日志级别
        log_record['level'] = record.levelname

        # 添加模块和函数信息
        log_record['module'] = record.module
        log_record['function'] = record.funcName
        log_record['line'] = record.lineno

        # 添加追踪信息
        if hasattr(record, 'trace_id'):
            log_record['trace_id'] = record.trace_id

        # 添加异常信息
        if record.exc_info:
            log_record['exception'] = traceback.format_exception(*record.exc_info)

def setup_logging(log_level: str = "INFO"):
    """设置日志配置"""
    # 创建日志处理器
    handler = logging.StreamHandler()
    formatter = CustomJsonFormatter()
    handler.setFormatter(formatter)

    # 配置根日志器
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    root_logger.addHandler(handler)

    # 配置特定模块的日志级别
    logging.getLogger("langgraph").setLevel(logging.DEBUG)
    logging.getLogger("uvicorn.access").setLevel(logging.WARNING)

class WorkflowLogger:
    """工作流日志器"""

    def __init__(self, workflow_name: str):
        self.logger = logging.getLogger(f"workflow.{workflow_name}")
        self.workflow_name = workflow_name

    def log_execution_start(self, input_data: Dict, trace_id: str):
        """记录执行开始"""
        self.logger.info(
            "Workflow execution started",
            extra={
                "trace_id": trace_id,
                "workflow": self.workflow_name,
                "input": input_data
            }
        )

    def log_node_execution(self, node_name: str, duration: float, trace_id: str):
        """记录节点执行"""
        self.logger.debug(
            f"Node {node_name} executed",
            extra={
                "trace_id": trace_id,
                "workflow": self.workflow_name,
                "node": node_name,
                "duration_ms": duration * 1000
            }
        )

    def log_execution_complete(self, result: Dict, duration: float, trace_id: str):
        """记录执行完成"""
        self.logger.info(
            "Workflow execution completed",
            extra={
                "trace_id": trace_id,
                "workflow": self.workflow_name,
                "duration_ms": duration * 1000,
                "result_size": len(str(result))
            }
        )

    def log_error(self, error: Exception, trace_id: str):
        """记录错误"""
        self.logger.error(
            f"Workflow execution failed: {str(error)}",
            exc_info=True,
            extra={
                "trace_id": trace_id,
                "workflow": self.workflow_name,
                "error_type": type(error).__name__
            }
        )
```

## 5. 安全配置

### 5.1 认证和授权

```python
# security.py
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import JWTError, jwt
from datetime import datetime, timedelta
from typing import Optional
import os

# 配置
SECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

# Bearer token 认证
security = HTTPBearer()

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """创建访问令牌"""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """验证令牌"""
    token = credentials.credentials
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid authentication credentials",
            )
        return username
    except JWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials",
        )

# API 密钥认证
API_KEYS = {
    "key1": {"name": "service1", "permissions": ["read", "write"]},
    "key2": {"name": "service2", "permissions": ["read"]}
}

async def verify_api_key(api_key: str = Depends(security)):
    """验证 API 密钥"""
    if api_key not in API_KEYS:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Invalid API key"
        )
    return API_KEYS[api_key]

# 应用到路由
@app.post("/protected/workflow")
async def protected_workflow(
    request: WorkflowRequest,
    current_user: str = Depends(verify_token)
):
    """受保护的工作流端点"""
    # 执行工作流
    return {"user": current_user, "result": "workflow executed"}
```

### 5.2 环境变量管理

```python
# config.py
from pydantic import BaseSettings, Field
from typing import Optional

class Settings(BaseSettings):
    """应用配置"""

    # 应用配置
    app_name: str = "LangGraph Production"
    environment: str = Field("development", env="ENVIRONMENT")
    debug: bool = Field(False, env="DEBUG")

    # 数据库配置
    database_url: str = Field(..., env="DATABASE_URL")
    database_pool_size: int = Field(10, env="DB_POOL_SIZE")

    # Redis 配置
    redis_url: str = Field("redis://localhost:6379", env="REDIS_URL")
    redis_max_connections: int = Field(50, env="REDIS_MAX_CONNECTIONS")

    # OpenAI 配置
    openai_api_key: str = Field(..., env="OPENAI_API_KEY")
    openai_org_id: Optional[str] = Field(None, env="OPENAI_ORG_ID")

    # 向量数据库配置
    vector_db_url: str = Field("http://localhost:6333", env="VECTOR_DB_URL")

    # 安全配置
    secret_key: str = Field(..., env="SECRET_KEY")
    api_keys: str = Field("", env="API_KEYS")  # 逗号分隔

    # 监控配置
    enable_metrics: bool = Field(True, env="ENABLE_METRICS")
    log_level: str = Field("INFO", env="LOG_LEVEL")

    # 限流配置
    rate_limit_requests: int = Field(100, env="RATE_LIMIT_REQUESTS")
    rate_limit_period: int = Field(60, env="RATE_LIMIT_PERIOD")

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

# 单例模式
_settings = None

def get_settings() -> Settings:
    global _settings
    if _settings is None:
        _settings = Settings()
    return _settings
```

## 6. CI/CD 配置

### 6.1 GitHub Actions

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov

    - name: Run tests
      run: |
        pytest tests/ --cov=./ --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3

  build-and-push:
    needs: test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
    - uses: actions/checkout@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}

  deploy:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
    - name: Deploy to Kubernetes
      uses: azure/k8s-deploy@v4
      with:
        manifests: |
          k8s/deployment.yaml
          k8s/service.yaml
        images: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
```

## 7. 生产检查清单

```python
# production_checklist.py
"""
生产部署检查清单
"""

checklist = {
    "环境配置": [
        "✓ 所有环境变量已设置",
        "✓ 密钥已安全存储",
        "✓ 数据库连接已配置",
        "✓ 缓存服务已配置"
    ],
    "安全": [
        "✓ HTTPS 已启用",
        "✓ 认证机制已实施",
        "✓ API 限流已配置",
        "✓ CORS 已正确设置",
        "✓ 敏感信息已加密"
    ],
    "性能": [
        "✓ 连接池已配置",
        "✓ 缓存策略已实施",
        "✓ 异步处理已启用",
        "✓ 自动扩缩容已配置"
    ],
    "监控": [
        "✓ 日志收集已配置",
        "✓ 指标监控已启用",
        "✓ 健康检查已实施",
        "✓ 告警规则已设置"
    ],
    "备份恢复": [
        "✓ 数据库备份策略",
        "✓ 检查点备份",
        "✓ 灾难恢复计划",
        "✓ 回滚策略"
    ],
    "文档": [
        "✓ API 文档已更新",
        "✓ 部署文档已完成",
        "✓ 运维手册已准备",
        "✓ 故障排查指南"
    ]
}

def verify_production_readiness():
    """验证生产就绪性"""
    for category, items in checklist.items():
        print(f"\n{category}:")
        for item in items:
            print(f"  {item}")
```

## 总结

成功的生产部署需要：

1. **容器化和编排**：Docker、Kubernetes 提供可扩展性
2. **高可用架构**：负载均衡、故障转移、健康检查
3. **性能优化**：缓存、连接池、异步处理
4. **监控告警**：日志、指标、追踪、告警
5. **安全防护**：认证、授权、加密、限流
6. **自动化部署**：CI/CD、自动测试、滚动更新

遵循这些最佳实践，可以构建稳定、高效、安全的 LangGraph 生产系统。