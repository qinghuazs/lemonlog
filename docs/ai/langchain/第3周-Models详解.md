---
title: ç¬¬3å‘¨ - Models è¯¦è§£
date: 2025-01-29
permalink: /ai/langchain/week3-models-deep-dive.html
tags:
  - LangChain
  - LLM
  - AI Models
categories:
  - AI
  - LangChain
---

# ç¬¬3å‘¨ï¼šModels è¯¦è§£

::: tip æœ¬å‘¨å­¦ä¹ ç›®æ ‡
- ğŸ¤– ç†è§£ä¸åŒç±»å‹çš„è¯­è¨€æ¨¡å‹ï¼ˆLLM vs Chat Modelï¼‰
- ğŸ”§ æŒæ¡æ¨¡å‹å‚æ•°è°ƒä¼˜æŠ€å·§
- ğŸš€ å­¦ä¼šä½¿ç”¨æµå¼è¾“å‡ºï¼ˆStreamingï¼‰
- ğŸ’° ä¼˜åŒ– Token ä½¿ç”¨å’Œæˆæœ¬æ§åˆ¶
- ğŸŒ å¯¹æ¥å¤šä¸ª LLM æä¾›å•†
:::

## ä¸€ã€è¯­è¨€æ¨¡å‹åŸºç¡€

### 1.1 LLM vs Chat Models

LangChain ä¸­æœ‰ä¸¤ç§ä¸»è¦çš„æ¨¡å‹ç±»å‹ï¼š

```mermaid
graph TB
    A[è¯­è¨€æ¨¡å‹] --> B[LLM<br/>æ–‡æœ¬è¡¥å…¨æ¨¡å‹]
    A --> C[Chat Model<br/>å¯¹è¯æ¨¡å‹]

    B --> B1[è¾“å…¥: å­—ç¬¦ä¸²]
    B --> B2[è¾“å‡º: å­—ç¬¦ä¸²]
    B --> B3[ç¤ºä¾‹: GPT-3 text-davinci-003]

    C --> C1[è¾“å…¥: æ¶ˆæ¯åˆ—è¡¨]
    C --> C2[è¾“å‡º: æ¶ˆæ¯å¯¹è±¡]
    C --> C3[ç¤ºä¾‹: GPT-3.5-turbo, GPT-4]

    style A fill:#E3F2FD
    style B fill:#BBDEFB
    style C fill:#90CAF9
```

#### 1.1.1 LLMï¼ˆæ–‡æœ¬è¡¥å…¨æ¨¡å‹ï¼‰

```python
"""
LLM æ¨¡å‹ç¤ºä¾‹
ç‰¹ç‚¹ï¼šç®€å•çš„æ–‡æœ¬è¡¥å…¨ï¼Œè¾“å…¥å­—ç¬¦ä¸²ï¼Œè¾“å‡ºå­—ç¬¦ä¸²
é€‚ç”¨åœºæ™¯ï¼šç®€å•çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
"""
from langchain_openai import OpenAI

# åˆå§‹åŒ– LLM
llm = OpenAI(
    model="gpt-3.5-turbo-instruct",  # æ–‡æœ¬è¡¥å…¨æ¨¡å‹
    temperature=0.7,
    max_tokens=100
)

# ç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²
prompt = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼š"
response = llm.invoke(prompt)

print(f"è¾“å…¥ç±»å‹ï¼š{type(prompt)}")  # <class 'str'>
print(f"è¾“å‡ºç±»å‹ï¼š{type(response)}")  # <class 'str'>
print(f"å›ç­”ï¼š{response}")
```

#### 1.1.2 Chat Modelï¼ˆå¯¹è¯æ¨¡å‹ï¼‰

```python
"""
Chat Model ç¤ºä¾‹
ç‰¹ç‚¹ï¼šä¸“ä¸ºå¯¹è¯è®¾è®¡ï¼Œæ”¯æŒè§’è‰²ï¼ˆsystem/user/assistantï¼‰
é€‚ç”¨åœºæ™¯ï¼šéœ€è¦ä¸Šä¸‹æ–‡çš„å¯¹è¯ã€å¤šè½®äº¤äº’
"""
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage

# åˆå§‹åŒ– Chat Model
chat = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7
)

# ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªPythonä¸“å®¶"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"),
    AIMessage(content="è£…é¥°å™¨æ˜¯ä¸€ç§ä¿®æ”¹å‡½æ•°è¡Œä¸ºçš„è¯­æ³•ç³–"),
    HumanMessage(content="èƒ½ä¸¾ä¸ªä¾‹å­å—ï¼Ÿ")
]

response = chat.invoke(messages)

print(f"è¾“å…¥ç±»å‹ï¼š{type(messages)}")  # <class 'list'>
print(f"è¾“å‡ºç±»å‹ï¼š{type(response)}")  # <class 'AIMessage'>
print(f"å›ç­”ï¼š{response.content}")
```

#### 1.1.3 å¦‚ä½•é€‰æ‹©ï¼Ÿ

| å¯¹æ¯”é¡¹ | LLM | Chat Model |
|--------|-----|-----------|
| **è¾“å…¥æ ¼å¼** | çº¯æ–‡æœ¬å­—ç¬¦ä¸² | æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ”¯æŒè§’è‰²ï¼‰ |
| **è¾“å‡ºæ ¼å¼** | çº¯æ–‡æœ¬å­—ç¬¦ä¸² | æ¶ˆæ¯å¯¹è±¡ |
| **ä¸Šä¸‹æ–‡ç®¡ç†** | éœ€è¦æ‰‹åŠ¨æ‹¼æ¥ | å†…ç½®æ”¯æŒå¤šè½®å¯¹è¯ |
| **é€‚ç”¨åœºæ™¯** | ç®€å•æ–‡æœ¬ç”Ÿæˆ | å¯¹è¯ã€å®¢æœã€åŠ©æ‰‹ |
| **æˆæœ¬** | è¾ƒä½ï¼ˆè€æ¨¡å‹ï¼‰ | ç•¥é«˜ï¼ˆæ–°æ¨¡å‹ï¼‰ |
| **æ¨èç¨‹åº¦** | â­â­ | â­â­â­â­â­ |

**æ¨èï¼šä¼˜å…ˆä½¿ç”¨ Chat Model**ï¼Œå®ƒæ˜¯ä¸»æµè¶‹åŠ¿ï¼ŒåŠŸèƒ½æ›´å¼ºå¤§ã€‚

### 1.2 æ”¯æŒçš„æ¨¡å‹æä¾›å•†

LangChain æ”¯æŒå¤šä¸ª LLM æä¾›å•†ï¼š

```mermaid
graph LR
    A[LangChain] --> B[OpenAI]
    A --> C[Anthropic]
    A --> D[Google]
    A --> E[å¼€æºæ¨¡å‹]

    B --> B1[GPT-3.5/GPT-4]
    C --> C1[Claude 3]
    D --> D1[Gemini]
    E --> E1[Llama 2/Mistral]

    style A fill:#4CAF50
    style B fill:#00BCD4
    style C fill:#FF9800
    style D fill:#F44336
    style E fill:#9C27B0
```

#### 1.2.1 OpenAI é›†æˆ

```python
"""
OpenAI æ¨¡å‹é›†æˆ
"""
from langchain_openai import ChatOpenAI

# GPT-3.5 Turboï¼ˆæ¨èï¼Œæ€§ä»·æ¯”é«˜ï¼‰
gpt35 = ChatOpenAI(
    model="gpt-3.5-turbo",
    api_key="your-api-key",          # å¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
    base_url="https://api.openai.com/v1",  # å¯é€‰ï¼Œè‡ªå®šä¹‰ API ç«¯ç‚¹
    temperature=0.7,
    max_tokens=1000
)

# GPT-4ï¼ˆæ€§èƒ½æœ€å¼ºï¼Œæˆæœ¬è¾ƒé«˜ï¼‰
gpt4 = ChatOpenAI(
    model="gpt-4",
    temperature=0.5
)

# GPT-4 Turboï¼ˆæ›´å¿«ã€æ›´ä¾¿å®œï¼‰
gpt4_turbo = ChatOpenAI(
    model="gpt-4-turbo-preview",
    temperature=0.7
)
```

#### 1.2.2 Anthropic Claude é›†æˆ

```python
"""
Anthropic Claude é›†æˆ
ç‰¹ç‚¹ï¼šé•¿ä¸Šä¸‹æ–‡ï¼ˆ100K tokensï¼‰ã€æ›´å®‰å…¨
"""
from langchain_anthropic import ChatAnthropic

claude = ChatAnthropic(
    model="claude-3-opus-20240229",  # Claude 3 Opus
    anthropic_api_key="your-api-key",
    temperature=0.7,
    max_tokens=1024
)

# Claude æ¨¡å‹ç³»åˆ—
models = {
    "claude-3-opus-20240229": "æœ€å¼ºæ€§èƒ½",
    "claude-3-sonnet-20240229": "å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬",
    "claude-3-haiku-20240307": "æœ€å¿«æœ€ä¾¿å®œ"
}
```

#### 1.2.3 å¼€æºæ¨¡å‹é›†æˆï¼ˆOllamaï¼‰

```python
"""
ä½¿ç”¨æœ¬åœ°å¼€æºæ¨¡å‹ï¼ˆOllamaï¼‰
ä¼˜ç‚¹ï¼šå…è´¹ã€éšç§ã€æ— éœ€ç½‘ç»œ
å‰æï¼šéœ€è¦æœ¬åœ°å®‰è£… Ollama
"""
from langchain_community.llms import Ollama

# ä½¿ç”¨ Llama 2
llama = Ollama(
    model="llama2",      # æ¨¡å‹åç§°
    base_url="http://localhost:11434"  # Ollama æœåŠ¡åœ°å€
)

response = llama.invoke("è§£é‡Šä»€ä¹ˆæ˜¯ Docker")
print(response)

# å…¶ä»–å¯ç”¨æ¨¡å‹ï¼šmistral, codellama, phi, etc.
```

#### 1.2.4 å¤šæä¾›å•†ç»Ÿä¸€æ¥å£

```python
"""
ä½¿ç”¨å·¥å‚æ¨¡å¼æ”¯æŒå¤šä¸ªæä¾›å•†
ä¼˜ç‚¹ï¼šè½»æ¾åˆ‡æ¢æ¨¡å‹ï¼Œä¾¿äº A/B æµ‹è¯•
"""
from typing import Literal
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_community.llms import Ollama

def create_model(
    provider: Literal["openai", "anthropic", "ollama"],
    model_name: str = None,
    **kwargs
):
    """
    æ¨¡å‹å·¥å‚å‡½æ•°

    å‚æ•°:
        provider: æä¾›å•†åç§°
        model_name: æ¨¡å‹åç§°
        **kwargs: å…¶ä»–å‚æ•°
    """
    if provider == "openai":
        return ChatOpenAI(model=model_name or "gpt-3.5-turbo", **kwargs)
    elif provider == "anthropic":
        return ChatAnthropic(model=model_name or "claude-3-sonnet-20240229", **kwargs)
    elif provider == "ollama":
        return Ollama(model=model_name or "llama2", **kwargs)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}")

# ä½¿ç”¨ç¤ºä¾‹
model = create_model("openai", temperature=0.7)
# è½»æ¾åˆ‡æ¢åˆ°å…¶ä»–æä¾›å•†
# model = create_model("anthropic", temperature=0.7)
```

---

## äºŒã€æ¨¡å‹å‚æ•°è¯¦è§£

### 2.1 æ ¸å¿ƒå‚æ•°

#### å‚æ•°1ï¼štemperatureï¼ˆåˆ›é€ æ€§ï¼‰

**å®šä¹‰**ï¼šæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§å’Œåˆ›é€ æ€§ã€‚

```python
"""
Temperature å‚æ•°å®éªŒ
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

prompt = "ç”¨ä¸€å¥è¯æè¿°æ˜¥å¤©"

# ä½æ¸©åº¦ï¼ˆç¡®å®šæ€§ï¼‰
low_temp = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.0)
print("Temperature = 0.0 (ç¡®å®šæ€§)ï¼š")
for i in range(3):
    response = low_temp.invoke([HumanMessage(content=prompt)])
    print(f"  ç¬¬{i+1}æ¬¡ï¼š{response.content}")

# ä¸­ç­‰æ¸©åº¦ï¼ˆå¹³è¡¡ï¼‰
mid_temp = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
print("\nTemperature = 0.7 (å¹³è¡¡)ï¼š")
for i in range(3):
    response = mid_temp.invoke([HumanMessage(content=prompt)])
    print(f"  ç¬¬{i+1}æ¬¡ï¼š{response.content}")

# é«˜æ¸©åº¦ï¼ˆåˆ›é€ æ€§ï¼‰
high_temp = ChatOpenAI(model="gpt-3.5-turbo", temperature=1.5)
print("\nTemperature = 1.5 (åˆ›é€ æ€§)ï¼š")
for i in range(3):
    response = high_temp.invoke([HumanMessage(content=prompt)])
    print(f"  ç¬¬{i+1}æ¬¡ï¼š{response.content}")
```

**Temperature é€‰æ‹©æŒ‡å—ï¼š**

| Temperature | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ | ç¤ºä¾‹ |
|-------------|------|---------|------|
| **0.0 - 0.2** | é«˜åº¦ä¸€è‡´æ€§ | äº‹å®æ€§é—®ç­”ã€ç¿»è¯‘ã€æ•°æ®æå– | å®¢æœé—®ç­”ã€ä»£ç ç”Ÿæˆ |
| **0.3 - 0.6** | è¾ƒä¸ºç¨³å®š | æ•™å­¦ã€è§£é‡Šã€æ€»ç»“ | æŠ€æœ¯æ–‡æ¡£ã€æ–°é—»æ‘˜è¦ |
| **0.7 - 0.9** | å¹³è¡¡åˆ›é€ æ€§ | é€šç”¨å¯¹è¯ã€å†…å®¹ç”Ÿæˆ | èŠå¤©æœºå™¨äººã€æ–‡ç« å†™ä½œ |
| **1.0 - 2.0** | é«˜åº¦åˆ›é€ æ€§ | åˆ›æ„å†™ä½œã€å¤´è„‘é£æš´ | è¯—æ­Œåˆ›ä½œã€å¹¿å‘Šæ–‡æ¡ˆ |

#### å‚æ•°2ï¼šmax_tokensï¼ˆè¾“å‡ºé•¿åº¦ï¼‰

```python
"""
max_tokens å‚æ•°æ§åˆ¶
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

prompt = "è¯¦ç»†ä»‹ç» Python çš„å†å²å’Œç‰¹ç‚¹"

# çŸ­å›ç­”
short_model = ChatOpenAI(model="gpt-3.5-turbo", max_tokens=50)
response = short_model.invoke([HumanMessage(content=prompt)])
print(f"max_tokens=50:\n{response.content}\n")

# ä¸­ç­‰å›ç­”
medium_model = ChatOpenAI(model="gpt-3.5-turbo", max_tokens=200)
response = medium_model.invoke([HumanMessage(content=prompt)])
print(f"max_tokens=200:\n{response.content}\n")

# é•¿å›ç­”
long_model = ChatOpenAI(model="gpt-3.5-turbo", max_tokens=500)
response = long_model.invoke([HumanMessage(content=prompt)])
print(f"max_tokens=500:\n{response.content}\n")
```

::: warning é‡è¦æç¤º
- `max_tokens` åŒ…æ‹¬è¾“å…¥å’Œè¾“å‡ºçš„æ€» Token æ•°
- GPT-3.5-turbo ä¸Šé™ï¼š4096 tokens
- GPT-4 ä¸Šé™ï¼š8192 tokensï¼ˆéƒ¨åˆ†æ¨¡å‹æ”¯æŒ 32K/128Kï¼‰
- è¶…å‡ºé™åˆ¶ä¼šæŠ¥é”™ï¼š`Context length exceeded`
:::

#### å‚æ•°3ï¼štop_pï¼ˆæ ¸é‡‡æ ·ï¼‰

```python
"""
top_p (nucleus sampling) å‚æ•°
åŠŸèƒ½ï¼šåªè€ƒè™‘æ¦‚ç‡æ€»å’Œè¾¾åˆ° p çš„ token
ä¸ temperature ä½œç”¨ç±»ä¼¼ï¼Œä½†æœºåˆ¶ä¸åŒ
"""
from langchain_openai import ChatOpenAI

# top_p = 0.1ï¼šåªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„ 10% tokenï¼ˆä¿å®ˆï¼‰
conservative = ChatOpenAI(model="gpt-3.5-turbo", top_p=0.1, temperature=1)

# top_p = 0.9ï¼šè€ƒè™‘æ¦‚ç‡æ€»å’Œ 90% çš„ tokenï¼ˆå¹³è¡¡ï¼‰
balanced = ChatOpenAI(model="gpt-3.5-turbo", top_p=0.9, temperature=1)

# top_p = 1.0ï¼šè€ƒè™‘æ‰€æœ‰ tokenï¼ˆåˆ›é€ æ€§ï¼‰
creative = ChatOpenAI(model="gpt-3.5-turbo", top_p=1.0, temperature=1)
```

**Temperature vs Top_pï¼š**

```mermaid
graph TB
    A[æ§åˆ¶éšæœºæ€§] --> B[Temperature]
    A --> C[Top_p]

    B --> B1["æ”¹å˜æ¦‚ç‡åˆ†å¸ƒ<br/>ï¼ˆå¹³æ»‘æˆ–é”åŒ–ï¼‰"]
    C --> C1["æˆªæ–­æ¦‚ç‡å°¾éƒ¨<br/>ï¼ˆåªä¿ç•™é«˜æ¦‚ç‡tokenï¼‰"]

    B2[æ¨èï¼šé€šç”¨åœºæ™¯] --> B
    C2[æ¨èï¼šéœ€è¦ç¨³å®šæ€§] --> C

    style A fill:#E3F2FD
    style B fill:#81C784
    style C fill:#FFB74D
```

::: tip æœ€ä½³å®è·µ
é€šå¸¸åªéœ€è°ƒæ•´ **temperature** æˆ– **top_p** ä¹‹ä¸€ï¼Œä¸è¦åŒæ—¶è°ƒæ•´ã€‚
- å¤§å¤šæ•°åœºæ™¯ï¼šåªè°ƒ temperature
- éœ€è¦ç¨³å®šæ€§ï¼šä½¿ç”¨ top_p=0.1 + temperature=1
:::

#### å‚æ•°4ï¼špresence_penalty & frequency_penalty

```python
"""
æƒ©ç½šå‚æ•°ï¼šé¿å…é‡å¤å†…å®¹
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

prompt = "åˆ—ä¸¾ 10 ç§ç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹"

# æ— æƒ©ç½šï¼ˆå¯èƒ½é‡å¤ï¼‰
no_penalty = ChatOpenAI(
    model="gpt-3.5-turbo",
    presence_penalty=0,    # ä¸æƒ©ç½šå‡ºç°è¿‡çš„ token
    frequency_penalty=0    # ä¸æƒ©ç½šé«˜é¢‘ token
)

# æ·»åŠ æƒ©ç½šï¼ˆå‡å°‘é‡å¤ï¼‰
with_penalty = ChatOpenAI(
    model="gpt-3.5-turbo",
    presence_penalty=0.6,  # æƒ©ç½šå·²å‡ºç°çš„ tokenï¼ˆ-2.0 åˆ° 2.0ï¼‰
    frequency_penalty=0.6  # æƒ©ç½šé«˜é¢‘ tokenï¼ˆ-2.0 åˆ° 2.0ï¼‰
)

print("æ— æƒ©ç½šè¾“å‡ºï¼š")
print(no_penalty.invoke([HumanMessage(content=prompt)]).content)

print("\næœ‰æƒ©ç½šè¾“å‡ºï¼ˆæ›´å¤šæ ·åŒ–ï¼‰ï¼š")
print(with_penalty.invoke([HumanMessage(content=prompt)]).content)
```

**Penalty å‚æ•°å¯¹æ¯”ï¼š**

| å‚æ•° | ä½œç”¨ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| **presence_penalty** | é™ä½å·²å‡ºç°è¯æ±‡çš„æ¦‚ç‡ | é¿å…è¯é¢˜é‡å¤ |
| **frequency_penalty** | æ ¹æ®å‡ºç°é¢‘ç‡é™ä½æ¦‚ç‡ | é¿å…çŸ­è¯­é‡å¤ |

**æ¨èå€¼ï¼š**
- ä¸€èˆ¬å¯¹è¯ï¼š`0.0`ï¼ˆæ— æƒ©ç½šï¼‰
- åˆ›æ„å†™ä½œï¼š`0.5 - 1.0`
- é¿å…é‡å¤ï¼š`0.6 - 0.8`

### 2.2 é«˜çº§å‚æ•°

#### å‚æ•°5ï¼štimeout & request_timeout

```python
"""
è¶…æ—¶æ§åˆ¶ï¼šé¿å…é•¿æ—¶é—´ç­‰å¾…
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
import time

# è®¾ç½®è¶…æ—¶
model = ChatOpenAI(
    model="gpt-3.5-turbo",
    timeout=10,           # æ€»è¶…æ—¶ï¼ˆç§’ï¼‰
    request_timeout=5     # å•æ¬¡è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
)

try:
    start = time.time()
    response = model.invoke([HumanMessage(content="è¯¦ç»†ä»‹ç»é‡å­è®¡ç®—")])
    elapsed = time.time() - start
    print(f"è¯·æ±‚æˆåŠŸï¼Œè€—æ—¶ï¼š{elapsed:.2f}ç§’")
except Exception as e:
    print(f"è¯·æ±‚è¶…æ—¶ï¼š{e}")
```

#### å‚æ•°6ï¼šmax_retries

```python
"""
è‡ªåŠ¨é‡è¯•ï¼šæé«˜ç¨³å®šæ€§
"""
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-3.5-turbo",
    max_retries=3,  # å¤±è´¥åè‡ªåŠ¨é‡è¯• 3 æ¬¡
    timeout=30
)

# å³ä½¿ç½‘ç»œä¸ç¨³å®šï¼Œä¹Ÿä¼šè‡ªåŠ¨é‡è¯•
response = model.invoke([HumanMessage(content="Hello")])
```

#### å‚æ•°7ï¼šstreamingï¼ˆæµå¼è¾“å‡ºï¼‰

å°†åœ¨ä¸‹ä¸€èŠ‚è¯¦ç»†ä»‹ç»ã€‚

### 2.3 å‚æ•°ç»„åˆæœ€ä½³å®è·µ

```python
"""
ä¸åŒåœºæ™¯çš„æ¨èå‚æ•°é…ç½®
"""
from langchain_openai import ChatOpenAI

# é…ç½®1ï¼šå®¢æœæœºå™¨äººï¼ˆç¨³å®šã€å¿«é€Ÿï¼‰
customer_service = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.3,         # ä½æ¸©åº¦ï¼Œç¨³å®šå›ç­”
    max_tokens=300,          # é™åˆ¶é•¿åº¦ï¼Œå¿«é€Ÿå“åº”
    presence_penalty=0.2,    # è½»å¾®é¿å…é‡å¤
    timeout=10
)

# é…ç½®2ï¼šå†…å®¹åˆ›ä½œï¼ˆåˆ›é€ æ€§ã€å¤šæ ·æ€§ï¼‰
content_creator = ChatOpenAI(
    model="gpt-4",
    temperature=0.8,         # é«˜æ¸©åº¦ï¼Œåˆ›é€ æ€§å¼º
    max_tokens=2000,         # å…è®¸é•¿æ–‡æœ¬
    frequency_penalty=0.7,   # é¿å…é‡å¤çŸ­è¯­
    presence_penalty=0.5
)

# é…ç½®3ï¼šä»£ç ç”Ÿæˆï¼ˆå‡†ç¡®æ€§ä¼˜å…ˆï¼‰
code_generator = ChatOpenAI(
    model="gpt-4",
    temperature=0.0,         # é›¶æ¸©åº¦ï¼Œç¡®å®šæ€§è¾“å‡º
    max_tokens=1500,
    top_p=0.1               # åªé€‰æœ€å¯èƒ½çš„ token
)

# é…ç½®4ï¼šæ•°æ®åˆ†æï¼ˆå¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ï¼‰
data_analyst = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.5,
    max_tokens=1000,
    request_timeout=30
)
```

---

## ä¸‰ã€æµå¼è¾“å‡ºï¼ˆStreamingï¼‰

### 3.1 ä»€ä¹ˆæ˜¯æµå¼è¾“å‡ºï¼Ÿ

**æµå¼è¾“å‡º**æ˜¯æŒ‡ AI é€å­—é€å¥ç”Ÿæˆå†…å®¹ï¼Œè€Œéç­‰å¾…å…¨éƒ¨å†…å®¹ç”Ÿæˆå®Œæ¯•åä¸€æ¬¡æ€§è¿”å›ã€‚

```mermaid
sequenceDiagram
    participant C as Client
    participant M as Model

    Note over C,M: ä¼ ç»Ÿæ–¹å¼ï¼ˆé˜»å¡ï¼‰
    C->>M: å‘é€è¯·æ±‚
    M->>M: ç”Ÿæˆå®Œæ•´å›ç­”
    M-->>C: è¿”å›å®Œæ•´ç»“æœ
    Note over C: ç”¨æˆ·ç­‰å¾…æ—¶é—´é•¿

    Note over C,M: æµå¼è¾“å‡ºï¼ˆéé˜»å¡ï¼‰
    C->>M: å‘é€è¯·æ±‚
    M-->>C: é€å—è¿”å›
    M-->>C: é€å—è¿”å›
    M-->>C: é€å—è¿”å›
    Note over C: ç”¨æˆ·ç«‹å³çœ‹åˆ°å†…å®¹
```

**ä¼˜åŠ¿ï¼š**
- âœ… **æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ**ï¼šç«‹å³çœ‹åˆ°è¾“å‡ºï¼Œå‡å°‘ç­‰å¾…æ„Ÿ
- âœ… **é€‚åˆé•¿æ–‡æœ¬**ï¼šç”Ÿæˆæ–‡ç« ã€æŠ¥å‘Šç­‰
- âœ… **å®æ—¶åé¦ˆ**ï¼šå¯ä»¥æå‰åˆ¤æ–­è¾“å‡ºè´¨é‡

### 3.2 å®ç°æµå¼è¾“å‡º

#### æ–¹æ³•1ï¼šä½¿ç”¨ stream() æ–¹æ³•

```python
"""
åŸºç¡€æµå¼è¾“å‡º
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

llm = ChatOpenAI(model="gpt-3.5-turbo", streaming=True)

prompt = "è¯·è¯¦ç»†ä»‹ç» Python è¯­è¨€çš„å†å²å’Œå‘å±•"

# æµå¼è¾“å‡º
print("AI å›ç­”ï¼š", end="", flush=True)
for chunk in llm.stream([HumanMessage(content=prompt)]):
    print(chunk.content, end="", flush=True)
print()  # æ¢è¡Œ
```

#### æ–¹æ³•2ï¼šä½¿ç”¨ Callbacks

```python
"""
ä½¿ç”¨ Streaming Callback Handler
æ›´çµæ´»ï¼Œå¯ä»¥è‡ªå®šä¹‰å¤„ç†é€»è¾‘
"""
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# åˆ›å»ºå¸¦å›è°ƒçš„æ¨¡å‹
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]  # è‡ªåŠ¨æ‰“å°åˆ°æ ‡å‡†è¾“å‡º
)

prompt = "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"
response = llm.invoke([HumanMessage(content=prompt)])
# å†…å®¹ä¼šå®æ—¶æ‰“å°ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†
```

#### æ–¹æ³•3ï¼šè‡ªå®šä¹‰ Callback

```python
"""
è‡ªå®šä¹‰ Streaming Callback
å¯ä»¥å®ç°æ›´å¤æ‚çš„é€»è¾‘ï¼Œå¦‚ï¼š
- å®æ—¶ä¿å­˜åˆ°æ–‡ä»¶
- å‘é€åˆ° WebSocket
- å®æ—¶ç»Ÿè®¡ Token
"""
from langchain.callbacks.base import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from typing import Any, Dict

class CustomStreamHandler(BaseCallbackHandler):
    """è‡ªå®šä¹‰æµå¼å¤„ç†å™¨"""

    def __init__(self):
        self.tokens = []
        self.token_count = 0

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """æ¯ç”Ÿæˆä¸€ä¸ªæ–° token æ—¶è°ƒç”¨"""
        self.tokens.append(token)
        self.token_count += 1

        # å®æ—¶æ‰“å°ï¼ˆå¯ä»¥æ”¹ä¸ºå…¶ä»–æ“ä½œï¼‰
        print(f"[Token {self.token_count}] {token}", end="", flush=True)

    def on_llm_end(self, response, **kwargs: Any) -> None:
        """ç”Ÿæˆç»“æŸæ—¶è°ƒç”¨"""
        print(f"\n\næ€»å…±ç”Ÿæˆ {self.token_count} ä¸ª token")

# ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†å™¨
handler = CustomStreamHandler()
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    streaming=True,
    callbacks=[handler]
)

response = llm.invoke([HumanMessage(content="è§£é‡Šä»€ä¹ˆæ˜¯åŒºå—é“¾")])
print(f"\nå®Œæ•´å†…å®¹ï¼š\n{''.join(handler.tokens)}")
```

### 3.3 å®æˆ˜ï¼šæ„å»ºå®æ—¶èŠå¤©ç•Œé¢

```python
"""
å®æˆ˜é¡¹ç›®ï¼šå¸¦è¿›åº¦æ˜¾ç¤ºçš„æµå¼èŠå¤©
åŠŸèƒ½ï¼š
1. å®æ—¶æ˜¾ç¤º AI å›ç­”
2. æ˜¾ç¤ºç”Ÿæˆè¿›åº¦
3. ç»Ÿè®¡ Token ä½¿ç”¨
"""
import sys
import time
from langchain.callbacks.base import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

class ProgressStreamHandler(BaseCallbackHandler):
    """å¸¦è¿›åº¦æ˜¾ç¤ºçš„æµå¼å¤„ç†å™¨"""

    def __init__(self):
        self.tokens = []
        self.start_time = None

    def on_llm_start(self, serialized: Dict, prompts, **kwargs) -> None:
        """å¼€å§‹ç”Ÿæˆæ—¶è°ƒç”¨"""
        self.start_time = time.time()
        print("\nğŸ¤– AI æ­£åœ¨æ€è€ƒ...\n")

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """æ–° token ç”Ÿæˆ"""
        self.tokens.append(token)
        sys.stdout.write(token)
        sys.stdout.flush()

    def on_llm_end(self, response, **kwargs) -> None:
        """ç”Ÿæˆç»“æŸ"""
        elapsed = time.time() - self.start_time
        token_count = len(self.tokens)
        speed = token_count / elapsed if elapsed > 0 else 0

        print(f"\n\n{'='*60}")
        print(f"âœ… ç”Ÿæˆå®Œæˆ")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"   - Token æ•°é‡ï¼š{token_count}")
        print(f"   - è€—æ—¶ï¼š{elapsed:.2f}ç§’")
        print(f"   - é€Ÿåº¦ï¼š{speed:.1f} tokens/ç§’")
        print(f"{'='*60}\n")

def main():
    """ä¸»å‡½æ•°"""
    handler = ProgressStreamHandler()
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7,
        streaming=True,
        callbacks=[handler]
    )

    # å¯¹è¯å†å²
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹")
    ]

    print("=" * 60)
    print("å®æ—¶æµå¼èŠå¤©ç³»ç»Ÿ")
    print("è¾“å…¥ 'exit' é€€å‡º")
    print("=" * 60)

    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("\nä½ : ").strip()

        if user_input.lower() == 'exit':
            print("å†è§ï¼ğŸ‘‹")
            break

        if not user_input:
            continue

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        messages.append(HumanMessage(content=user_input))

        # é‡ç½® handler
        handler.tokens = []

        # æµå¼ç”Ÿæˆå›ç­”
        print("\nAI: ", end="")
        response = llm.invoke(messages)

        # æ·»åŠ  AI å›å¤åˆ°å†å²
        messages.append(response)

if __name__ == "__main__":
    main()
```

### 3.4 æµå¼è¾“å‡ºçš„æ³¨æ„äº‹é¡¹

::: warning æ€§èƒ½è€ƒè™‘
1. **ç½‘ç»œå¼€é”€**ï¼šæµå¼è¾“å‡ºä¼šå¢åŠ ç½‘ç»œè¯·æ±‚æ¬¡æ•°
2. **å»¶è¿Ÿ**ï¼šæ¯ä¸ª token éƒ½æœ‰ç½‘ç»œå¾€è¿”æ—¶é—´
3. **é€‚ç”¨åœºæ™¯**ï¼šä¸»è¦ç”¨äºç”¨æˆ·ç•Œé¢ï¼Œåå°å¤„ç†ä¸æ¨è
:::

**ä½•æ—¶ä½¿ç”¨æµå¼è¾“å‡ºï¼Ÿ**

| åœºæ™¯ | æ˜¯å¦ä½¿ç”¨ | åŸå›  |
|------|---------|------|
| Web èŠå¤©ç•Œé¢ | âœ… æ¨è | æå‡ç”¨æˆ·ä½“éªŒ |
| å‘½ä»¤è¡Œäº¤äº’ | âœ… æ¨è | å®æ—¶åé¦ˆ |
| é•¿æ–‡æœ¬ç”Ÿæˆ | âœ… æ¨è | å‡å°‘ç­‰å¾…æ„Ÿ |
| æ‰¹é‡å¤„ç† | âŒ ä¸æ¨è | å¢åŠ å¼€é”€ |
| API è°ƒç”¨ | âŒ ä¸æ¨è | å¤æ‚åº¦é«˜ |
| æ•°æ®åˆ†æ | âŒ ä¸æ¨è | ä¸éœ€è¦å®æ—¶æ€§ |

---

## å››ã€Token ä¼˜åŒ–ä¸æˆæœ¬æ§åˆ¶

### 4.1 ç†è§£ Token

**Token** æ˜¯ LLM å¤„ç†æ–‡æœ¬çš„åŸºæœ¬å•ä½ï¼Œé€šå¸¸ï¼š
- 1 ä¸ªè‹±æ–‡å•è¯ â‰ˆ 1-2 tokens
- 1 ä¸ªä¸­æ–‡å­—ç¬¦ â‰ˆ 2-3 tokens
- æ ‡ç‚¹ç¬¦å·é€šå¸¸æ˜¯ 1 token

```python
"""
Token è®¡æ•°å·¥å…·
"""
import tiktoken

def count_tokens(text: str, model: str = "gpt-3.5-turbo") -> int:
    """è®¡ç®—æ–‡æœ¬çš„ Token æ•°é‡"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# æµ‹è¯•
texts = [
    "Hello, world!",
    "ä½ å¥½ï¼Œä¸–ç•Œï¼",
    "def hello(): print('Hello')"
]

for text in texts:
    token_count = count_tokens(text)
    print(f"æ–‡æœ¬ï¼š{text}")
    print(f"Token æ•°ï¼š{token_count}")
    print(f"å­—ç¬¦æ•°ï¼š{len(text)}")
    print(f"æ¯”ä¾‹ï¼š{token_count / len(text):.2f} tokens/å­—ç¬¦\n")
```

### 4.2 æˆæœ¬è®¡ç®—

ä¸åŒæ¨¡å‹çš„ä»·æ ¼ï¼ˆ2025å¹´1æœˆï¼‰ï¼š

| æ¨¡å‹ | è¾“å…¥ä»·æ ¼ | è¾“å‡ºä»·æ ¼ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|---------|
| GPT-3.5-turbo | $0.0015/1K | $0.002/1K | é€šç”¨ä»»åŠ¡ |
| GPT-4 | $0.03/1K | $0.06/1K | å¤æ‚æ¨ç† |
| GPT-4-turbo | $0.01/1K | $0.03/1K | å¹³è¡¡æ€§èƒ½ |
| Claude-3-haiku | $0.00025/1K | $0.00125/1K | é«˜æ€§ä»·æ¯” |
| Claude-3-sonnet | $0.003/1K | $0.015/1K | å¹³è¡¡é€‰æ‹© |

```python
"""
æˆæœ¬è®¡ç®—å™¨
"""
from langchain_openai import ChatOpenAI
from langchain.callbacks import get_openai_callback
from langchain.schema import HumanMessage

def estimate_cost(prompt: str, model_name: str = "gpt-3.5-turbo"):
    """
    ä¼°ç®—å•æ¬¡è¯·æ±‚æˆæœ¬

    å‚æ•°:
        prompt: è¾“å…¥æ–‡æœ¬
        model_name: æ¨¡å‹åç§°

    è¿”å›:
        æˆæœ¬ä¿¡æ¯å­—å…¸
    """
    llm = ChatOpenAI(model=model_name)

    with get_openai_callback() as cb:
        response = llm.invoke([HumanMessage(content=prompt)])

        return {
            "prompt_tokens": cb.prompt_tokens,
            "completion_tokens": cb.completion_tokens,
            "total_tokens": cb.total_tokens,
            "total_cost": cb.total_cost,
            "response": response.content
        }

# æµ‹è¯•
prompt = "è¯¦ç»†ä»‹ç» Python çš„ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯"
result = estimate_cost(prompt)

print(f"è¾“å…¥ Tokenï¼š{result['prompt_tokens']}")
print(f"è¾“å‡º Tokenï¼š{result['completion_tokens']}")
print(f"æ€»è®¡ Tokenï¼š{result['total_tokens']}")
print(f"æˆæœ¬ï¼š${result['total_cost']:.6f}")
```

### 4.3 Token ä¼˜åŒ–æŠ€å·§

#### æŠ€å·§1ï¼šç²¾ç®€ Prompt

```python
"""
ä¼˜åŒ–å‰åå¯¹æ¯”
"""
# âŒ å†—é•¿çš„ Prompt
verbose_prompt = """
ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªç”¨æˆ·ã€‚æˆ‘ç°åœ¨æœ‰ä¸€ä¸ªé—®é¢˜æƒ³è¦å’¨è¯¢ä½ ã€‚
æˆ‘æƒ³çŸ¥é“ï¼Œåœ¨ Python ç¼–ç¨‹è¯­è¨€ä¸­ï¼Œæœ‰å“ªäº›å¸¸ç”¨çš„æ•°æ®ç»“æ„ï¼Ÿ
è¯·ä½ è¯¦ç»†åœ°ã€å®Œæ•´åœ°ã€å°½å¯èƒ½æ¸…æ™°åœ°ä¸ºæˆ‘è§£é‡Šä¸€ä¸‹ã€‚
è°¢è°¢ä½ çš„å¸®åŠ©ï¼
"""

# âœ… ç²¾ç®€çš„ Prompt
concise_prompt = "åˆ—ä¸¾ Python çš„å¸¸ç”¨æ•°æ®ç»“æ„å¹¶ç®€è¦è¯´æ˜"

# Token å¯¹æ¯”
import tiktoken
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

verbose_tokens = len(encoding.encode(verbose_prompt))
concise_tokens = len(encoding.encode(concise_prompt))

print(f"å†—é•¿ Promptï¼š{verbose_tokens} tokens")
print(f"ç²¾ç®€ Promptï¼š{concise_tokens} tokens")
print(f"èŠ‚çœï¼š{verbose_tokens - concise_tokens} tokens ({(1 - concise_tokens/verbose_tokens)*100:.1f}%)")
```

#### æŠ€å·§2ï¼šä½¿ç”¨æ‘˜è¦ï¼ˆSummaryï¼‰

```python
"""
é•¿å¯¹è¯æ‘˜è¦ï¼šé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage, SystemMessage

llm = ChatOpenAI(model="gpt-3.5-turbo")

# æ¨¡æ‹Ÿé•¿å¯¹è¯å†å²
long_conversation = [
    HumanMessage(content="ä»‹ç»ä¸€ä¸‹ Python"),
    AIMessage(content="Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€...ï¼ˆå‡è®¾å¾ˆé•¿ï¼‰"),
    HumanMessage(content="å®ƒæœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ"),
    AIMessage(content="Python çš„ç‰¹ç‚¹åŒ…æ‹¬...ï¼ˆå‡è®¾å¾ˆé•¿ï¼‰"),
    # ... æ›´å¤šå¯¹è¯
]

# å½“å¯¹è¯è¿‡é•¿æ—¶ï¼Œç”Ÿæˆæ‘˜è¦
if len(long_conversation) > 10:
    summary_prompt = """è¯·ç®€è¦æ€»ç»“ä»¥ä¸‹å¯¹è¯çš„æ ¸å¿ƒå†…å®¹ï¼š

å¯¹è¯å†å²ï¼š
{conversation}

æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š"""

    # ç”Ÿæˆæ‘˜è¦
    summary = llm.invoke([HumanMessage(content=summary_prompt)])

    # ç”¨æ‘˜è¦æ›¿æ¢æ—§å¯¹è¯
    new_conversation = [
        SystemMessage(content=f"ä¹‹å‰çš„å¯¹è¯æ‘˜è¦ï¼š{summary.content}")
    ]
```

#### æŠ€å·§3ï¼šé€‰æ‹©åˆé€‚çš„æ¨¡å‹

```python
"""
ä»»åŠ¡å¤æ‚åº¦ä¸æ¨¡å‹é€‰æ‹©
"""
from langchain_openai import ChatOpenAI

class SmartModelSelector:
    """æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨"""

    def __init__(self):
        self.gpt35 = ChatOpenAI(model="gpt-3.5-turbo")
        self.gpt4 = ChatOpenAI(model="gpt-4")

    def select_model(self, task: str, complexity: str = "simple"):
        """
        æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æ¨¡å‹

        å‚æ•°:
            task: ä»»åŠ¡æè¿°
            complexity: simple/medium/complex
        """
        if complexity == "simple":
            print("ä½¿ç”¨ GPT-3.5ï¼ˆæˆæœ¬ä½ï¼‰")
            return self.gpt35
        elif complexity == "complex":
            print("ä½¿ç”¨ GPT-4ï¼ˆæ€§èƒ½å¼ºï¼‰")
            return self.gpt4
        else:
            # å¯ä»¥ç”¨ GPT-3.5 å…ˆåˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦
            judge_prompt = f"ä»¥ä¸‹ä»»åŠ¡æ˜¯ç®€å•è¿˜æ˜¯å¤æ‚ï¼Ÿåªå›ç­”'ç®€å•'æˆ–'å¤æ‚'ï¼š{task}"
            response = self.gpt35.invoke([HumanMessage(content=judge_prompt)])
            is_complex = "å¤æ‚" in response.content

            if is_complex:
                print("è‡ªåŠ¨åˆ¤æ–­ï¼šå¤æ‚ä»»åŠ¡ï¼Œä½¿ç”¨ GPT-4")
                return self.gpt4
            else:
                print("è‡ªåŠ¨åˆ¤æ–­ï¼šç®€å•ä»»åŠ¡ï¼Œä½¿ç”¨ GPT-3.5")
                return self.gpt35

# ä½¿ç”¨ç¤ºä¾‹
selector = SmartModelSelector()

# ç®€å•ä»»åŠ¡
simple_task = "å°† 'Hello' ç¿»è¯‘æˆä¸­æ–‡"
model = selector.select_model(simple_task, "simple")

# å¤æ‚ä»»åŠ¡
complex_task = "è®¾è®¡ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„"
model = selector.select_model(complex_task, "complex")
```

#### æŠ€å·§4ï¼šæ‰¹é‡å¤„ç†

```python
"""
æ‰¹é‡è°ƒç”¨ï¼šå‡å°‘ç½‘ç»œå¼€é”€
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
import time

llm = ChatOpenAI(model="gpt-3.5-turbo")

questions = [
    "1+1ç­‰äºå‡ ï¼Ÿ",
    "Pythonä¹‹çˆ¶æ˜¯è°ï¼Ÿ",
    "LangChainæ˜¯ä»€ä¹ˆï¼Ÿ"
]

# âŒ é€ä¸ªè°ƒç”¨ï¼ˆæ…¢ï¼‰
start = time.time()
for q in questions:
    response = llm.invoke([HumanMessage(content=q)])
sequential_time = time.time() - start

# âœ… æ‰¹é‡è°ƒç”¨ï¼ˆå¿«ï¼‰
start = time.time()
messages_batch = [[HumanMessage(content=q)] for q in questions]
responses = llm.batch(messages_batch)
batch_time = time.time() - start

print(f"é€ä¸ªè°ƒç”¨è€—æ—¶ï¼š{sequential_time:.2f}ç§’")
print(f"æ‰¹é‡è°ƒç”¨è€—æ—¶ï¼š{batch_time:.2f}ç§’")
print(f"æé€Ÿï¼š{(sequential_time / batch_time):.1f}x")
```

#### æŠ€å·§5ï¼šç¼“å­˜

```python
"""
ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è°ƒç”¨
"""
from langchain.cache import InMemoryCache, SQLiteCache
from langchain.globals import set_llm_cache
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
import time

llm = ChatOpenAI(model="gpt-3.5-turbo")

# æ–¹æ³•1ï¼šå†…å­˜ç¼“å­˜ï¼ˆå¿«ï¼Œä½†é‡å¯ä¸¢å¤±ï¼‰
set_llm_cache(InMemoryCache())

# æ–¹æ³•2ï¼šSQLite ç¼“å­˜ï¼ˆæŒä¹…åŒ–ï¼‰
# set_llm_cache(SQLiteCache(database_path=".langchain.db"))

prompt = "è§£é‡Šä»€ä¹ˆæ˜¯ Docker"

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ…¢ï¼Œéœ€è¦è¯·æ±‚ APIï¼‰
start = time.time()
response1 = llm.invoke([HumanMessage(content=prompt)])
first_time = time.time() - start

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆå¿«ï¼Œä»ç¼“å­˜è¯»å–ï¼‰
start = time.time()
response2 = llm.invoke([HumanMessage(content=prompt)])
second_time = time.time() - start

print(f"ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼š{first_time:.2f}ç§’")
print(f"ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆç¼“å­˜ï¼‰ï¼š{second_time:.2f}ç§’")
print(f"æé€Ÿï¼š{(first_time / second_time):.1f}x")
print(f"ä¸¤æ¬¡ç»“æœç›¸åŒï¼š{response1.content == response2.content}")
```

---

## äº”ã€æœ¬å‘¨ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šå‚æ•°å®éªŒï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰

**ä»»åŠ¡**ï¼šå¯¹æ¯”ä¸åŒ temperature å’Œ top_p ç»„åˆçš„è¾“å‡ºæ•ˆæœã€‚

**è¦æ±‚**ï¼š
1. ä½¿ç”¨åŒä¸€ä¸ª Prompt
2. æµ‹è¯•è‡³å°‘ 6 ç§å‚æ•°ç»„åˆ
3. åˆ†æè¾“å‡ºçš„å·®å¼‚

<details>
<summary>æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

prompt = "å†™ä¸€é¦–å››è¡Œå°è¯—ï¼Œä¸»é¢˜æ˜¯ç§‹å¤©"

configs = [
    {"temperature": 0.0, "top_p": 1.0, "name": "ç¡®å®šæ€§"},
    {"temperature": 0.5, "top_p": 1.0, "name": "ä½åˆ›é€ æ€§"},
    {"temperature": 1.0, "top_p": 1.0, "name": "é«˜åˆ›é€ æ€§"},
    {"temperature": 1.0, "top_p": 0.1, "name": "ç¨³å®šéšæœº"},
    {"temperature": 1.0, "top_p": 0.5, "name": "ä¸­ç­‰éšæœº"},
    {"temperature": 1.5, "top_p": 1.0, "name": "æé«˜åˆ›é€ æ€§"},
]

for config in configs:
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=config["temperature"],
        top_p=config["top_p"]
    )

    print(f"\n{'='*60}")
    print(f"{config['name']} (temp={config['temperature']}, top_p={config['top_p']})")
    print('='*60)

    for i in range(2):  # æ¯ä¸ªé…ç½®ç”Ÿæˆ2æ¬¡
        response = llm.invoke([HumanMessage(content=prompt)])
        print(f"\nç¬¬{i+1}æ¬¡ï¼š\n{response.content}")
```
</details>

### ç»ƒä¹ 2ï¼šæˆæœ¬ä¼˜åŒ–ï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰

**ä»»åŠ¡**ï¼šå®ç°ä¸€ä¸ªæ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œæ ¹æ®é—®é¢˜å¤æ‚åº¦è‡ªåŠ¨é€‰æ‹©æ¨¡å‹ï¼Œæœ€å°åŒ–æˆæœ¬ã€‚

**è¦æ±‚**ï¼š
1. ç®€å•é—®é¢˜ç”¨ GPT-3.5
2. å¤æ‚é—®é¢˜ç”¨ GPT-4
3. ç»Ÿè®¡æ€»æˆæœ¬

<details>
<summary>æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from langchain.callbacks import get_openai_callback

class CostOptimizedQA:
    def __init__(self):
        self.gpt35 = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        self.gpt4 = ChatOpenAI(model="gpt-4", temperature=0)
        self.total_cost = 0

    def judge_complexity(self, question: str) -> str:
        """åˆ¤æ–­é—®é¢˜å¤æ‚åº¦"""
        judge_prompt = f"""åˆ¤æ–­ä»¥ä¸‹é—®é¢˜æ˜¯"ç®€å•"è¿˜æ˜¯"å¤æ‚"ï¼Ÿ
ç®€å•ï¼šäº‹å®æŸ¥è¯¢ã€åŸºç¡€çŸ¥è¯†
å¤æ‚ï¼šéœ€è¦æ¨ç†ã€åˆ›é€ æ€§ã€å¤šæ­¥éª¤

é—®é¢˜ï¼š{question}
åªå›ç­”"ç®€å•"æˆ–"å¤æ‚"ï¼š"""

        with get_openai_callback() as cb:
            response = self.gpt35.invoke([HumanMessage(content=judge_prompt)])
            self.total_cost += cb.total_cost

        return response.content.strip()

    def answer(self, question: str):
        """å›ç­”é—®é¢˜"""
        complexity = self.judge_complexity(question)

        if "å¤æ‚" in complexity:
            model = self.gpt4
            model_name = "GPT-4"
        else:
            model = self.gpt35
            model_name = "GPT-3.5"

        print(f"[ä½¿ç”¨ {model_name}]")

        with get_openai_callback() as cb:
            response = model.invoke([HumanMessage(content=question)])
            self.total_cost += cb.total_cost

        return response.content

# æµ‹è¯•
qa = CostOptimizedQA()

questions = [
    "Python ä¹‹çˆ¶æ˜¯è°ï¼Ÿ",
    "è®¾è®¡ä¸€ä¸ªé«˜å¹¶å‘çš„ç”µå•†ç³»ç»Ÿæ¶æ„",
    "1+1ç­‰äºå‡ ï¼Ÿ"
]

for q in questions:
    print(f"\né—®é¢˜ï¼š{q}")
    answer = qa.answer(q)
    print(f"å›ç­”ï¼š{answer}\n")

print(f"\næ€»æˆæœ¬ï¼š${qa.total_cost:.6f}")
```
</details>

### ç»ƒä¹ 3ï¼šæµå¼èŠå¤©æœºå™¨äººï¼ˆéš¾åº¦ï¼šâ­â­â­ï¼‰

**ä»»åŠ¡**ï¼šæ„å»ºä¸€ä¸ªå¸¦æµå¼è¾“å‡ºçš„å¤šè½®å¯¹è¯æœºå™¨äººã€‚

**è¦æ±‚**ï¼š
1. æ”¯æŒæµå¼æ˜¾ç¤º AI å›ç­”
2. è®°å½•å¯¹è¯å†å²
3. æ˜¾ç¤º Token ä½¿ç”¨ç»Ÿè®¡
4. æ”¯æŒå¯¼å‡ºå¯¹è¯

<details>
<summary>æŸ¥çœ‹æç¤º</summary>

å‚è€ƒæœ¬å‘¨ 3.3 èŠ‚çš„"å®æˆ˜ï¼šæ„å»ºå®æ—¶èŠå¤©ç•Œé¢"ï¼Œå¹¶æ·»åŠ ï¼š
- å¯¹è¯å†å²ç®¡ç†
- Token ç»Ÿè®¡
- å¯¼å‡ºåŠŸèƒ½
</details>

---

## å…­ã€æœ¬å‘¨æ€»ç»“

### 6.1 çŸ¥è¯†ç‚¹æ¸…å•

- [x] LLM vs Chat Model åŒºåˆ«
- [x] å¤šä¸ª LLM æä¾›å•†é›†æˆ
- [x] æ ¸å¿ƒå‚æ•°ï¼štemperature, max_tokens, top_p
- [x] é«˜çº§å‚æ•°ï¼špresence_penalty, frequency_penalty
- [x] æµå¼è¾“å‡ºå®ç°æ–¹æ³•
- [x] Token è®¡æ•°å’Œæˆæœ¬ä¼°ç®—
- [x] æˆæœ¬ä¼˜åŒ–æŠ€å·§

### 6.2 å‚æ•°é€ŸæŸ¥è¡¨

| å‚æ•° | èŒƒå›´ | é»˜è®¤å€¼ | ä½œç”¨ |
|------|------|--------|------|
| temperature | 0-2 | 0.7 | æ§åˆ¶éšæœºæ€§ |
| max_tokens | 1-æ¨¡å‹ä¸Šé™ | æ— é™åˆ¶ | é™åˆ¶è¾“å‡ºé•¿åº¦ |
| top_p | 0-1 | 1.0 | æ ¸é‡‡æ · |
| presence_penalty | -2 to 2 | 0 | é¿å…è¯é¢˜é‡å¤ |
| frequency_penalty | -2 to 2 | 0 | é¿å…çŸ­è¯­é‡å¤ |
| timeout | ç§’ | æ— é™åˆ¶ | è¯·æ±‚è¶…æ—¶ |
| max_retries | æ•´æ•° | 2 | å¤±è´¥é‡è¯•æ¬¡æ•° |

### 6.3 ä¸‹å‘¨é¢„ä¹ 

**ç¬¬4å‘¨ä¸»é¢˜ï¼šChains åŸºç¡€**

é¢„ä¹ å†…å®¹ï¼š
1. ä»€ä¹ˆæ˜¯ Chainï¼Ÿ
2. LLMChainã€SequentialChain çš„åŒºåˆ«
3. LCEL (LangChain Expression Language)

**æ€è€ƒé—®é¢˜**ï¼š
- å¦‚ä½•å°†å¤šä¸ª LLM è°ƒç”¨ä¸²è”èµ·æ¥ï¼Ÿ
- å¦‚ä½•å®ç°å¤æ‚çš„ä¸šåŠ¡æµç¨‹ï¼Ÿ

---

::: tip å­¦ä¹ å»ºè®®
1. **åŠ¨æ‰‹å®éªŒ**ï¼šå°è¯•ä¸åŒå‚æ•°ç»„åˆï¼Œè§‚å¯Ÿæ•ˆæœ
2. **æˆæœ¬æ„è¯†**ï¼šå§‹ç»ˆå…³æ³¨ Token ä½¿ç”¨å’Œæˆæœ¬
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆç†ä½¿ç”¨ç¼“å­˜å’Œæ‰¹é‡å¤„ç†
4. **ç”¨æˆ·ä½“éªŒ**ï¼šåœ¨éœ€è¦çš„åœºæ™¯ä½¿ç”¨æµå¼è¾“å‡º
:::

**æœ¬å‘¨è¾›è‹¦äº†ï¼ä¸‹å‘¨ç»§ç»­ï¼ğŸš€**
