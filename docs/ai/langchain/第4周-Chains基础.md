---
title: ç¬¬4å‘¨ - Chains åŸºç¡€
date: 2025-02-05
permalink: /ai/langchain/week4-chains-basics.html
tags:
  - LangChain
  - Chains
  - LCEL
categories:
  - AI
  - LangChain
---

# ç¬¬4å‘¨ï¼šChains åŸºç¡€

::: tip æœ¬å‘¨å­¦ä¹ ç›®æ ‡
- ğŸ”— ç†è§£ Chain çš„æ¦‚å¿µå’Œä½œç”¨
- ğŸ› ï¸ æŒæ¡ LLMChainã€SequentialChain ç­‰åŸºç¡€ Chain
- ğŸš€ å­¦ä¹  LCEL (LangChain Expression Language)
- ğŸ¯ èƒ½å¤Ÿç»„åˆå¤šä¸ªç»„ä»¶æ„å»ºå¤æ‚æµç¨‹
- ğŸ’¡ å®ç°å®é™…ä¸šåŠ¡åœºæ™¯çš„ Chain åº”ç”¨
:::

## ä¸€ã€Chain åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ Chainï¼Ÿ

**Chainï¼ˆé“¾ï¼‰** æ˜¯ LangChain çš„æ ¸å¿ƒæŠ½è±¡ï¼Œç”¨äºå°†å¤šä¸ªç»„ä»¶ï¼ˆLLMã€Promptsã€Toolsç­‰ï¼‰æŒ‰ç…§ç‰¹å®šé¡ºåºè¿æ¥èµ·æ¥ï¼Œå½¢æˆå®Œæ•´çš„å·¥ä½œæµç¨‹ã€‚

```mermaid
graph LR
    A[è¾“å…¥] --> B[æ­¥éª¤1: Prompt]
    B --> C[æ­¥éª¤2: LLM]
    C --> D[æ­¥éª¤3: Parser]
    D --> E[è¾“å‡º]

    style A fill:#E3F2FD
    style E fill:#C8E6C9
    style B fill:#FFF9C4
    style C fill:#FFE0B2
    style D fill:#F8BBD0
```

#### ä¸ºä»€ä¹ˆéœ€è¦ Chainï¼Ÿ

**å¯¹æ¯”ï¼šæ—  Chain vs æœ‰ Chain**

```python
"""
åœºæ™¯ï¼šç¿»è¯‘å¹¶æ€»ç»“ä¸€æ®µæ–‡æœ¬
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

llm = ChatOpenAI(model="gpt-3.5-turbo")

# âŒ æ–¹å¼1ï¼šæ‰‹åŠ¨ç®¡ç†å¤šä¸ªæ­¥éª¤ï¼ˆç¹çï¼‰
def manual_translate_and_summarize(text: str) -> str:
    # æ­¥éª¤1ï¼šç¿»è¯‘
    translate_prompt = f"å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼š\n{text}"
    translation = llm.invoke([HumanMessage(content=translate_prompt)])

    # æ­¥éª¤2ï¼šæ€»ç»“
    summary_prompt = f"æ€»ç»“ä»¥ä¸‹æ–‡æœ¬ï¼ˆ50å­—ä»¥å†…ï¼‰ï¼š\n{translation.content}"
    summary = llm.invoke([HumanMessage(content=summary_prompt)])

    return summary.content

# âœ… æ–¹å¼2ï¼šä½¿ç”¨ Chainï¼ˆä¼˜é›…ï¼‰
from langchain.chains import LLMChain, SimpleSequentialChain
from langchain.prompts import PromptTemplate

# å®šä¹‰ç¿»è¯‘é“¾
translate_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate.from_template("å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼š\n{text}")
)

# å®šä¹‰æ€»ç»“é“¾
summary_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate.from_template("æ€»ç»“ä»¥ä¸‹æ–‡æœ¬ï¼ˆ50å­—ä»¥å†…ï¼‰ï¼š\n{text}")
)

# ç»„åˆæˆé¡ºåºé“¾
overall_chain = SimpleSequentialChain(
    chains=[translate_chain, summary_chain],
    verbose=True  # æ˜¾ç¤ºä¸­é—´æ­¥éª¤
)

# ä½¿ç”¨
result = overall_chain.run("äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œï¼Œå®ƒåœ¨åŒ»ç–—ã€æ•™è‚²ã€äº¤é€šç­‰é¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚")
```

**Chain çš„ä¼˜åŠ¿ï¼š**

| ç‰¹æ€§ | æ‰‹åŠ¨ç®¡ç† | ä½¿ç”¨ Chain |
|------|---------|-----------|
| **ä»£ç å¤ç”¨** | âŒ é‡å¤ä»£ç å¤š | âœ… é«˜åº¦å¤ç”¨ |
| **å¯ç»´æŠ¤æ€§** | âŒ éš¾ä»¥ç»´æŠ¤ | âœ… ç»“æ„æ¸…æ™° |
| **è°ƒè¯•** | âŒ éš¾ä»¥è¿½è¸ª | âœ… å†…ç½®è°ƒè¯•æ”¯æŒ |
| **æ‰©å±•æ€§** | âŒ ä¿®æ”¹å›°éš¾ | âœ… æ˜“äºæ‰©å±• |
| **é”™è¯¯å¤„ç†** | âŒ æ‰‹åŠ¨å¤„ç† | âœ… è‡ªåŠ¨å¤„ç† |

### 1.2 Chain çš„ç±»å‹

LangChain æä¾›äº†å¤šç§é¢„å®šä¹‰çš„ Chainï¼š

```mermaid
graph TB
    A[Chain ç±»å‹] --> B[åŸºç¡€ Chain]
    A --> C[é¡ºåº Chain]
    A --> D[è·¯ç”± Chain]
    A --> E[ç‰¹æ®Šç”¨é€” Chain]

    B --> B1[LLMChain<br/>æœ€åŸºç¡€çš„é“¾]
    B --> B2[TransformChain<br/>æ•°æ®è½¬æ¢é“¾]

    C --> C1[SimpleSequentialChain<br/>ç®€å•é¡ºåºé“¾]
    C --> C2[SequentialChain<br/>å¤æ‚é¡ºåºé“¾]

    D --> D1[RouterChain<br/>æ¡ä»¶è·¯ç”±]
    D --> D2[MultiPromptChain<br/>å¤šæç¤ºè·¯ç”±]

    E --> E1[ConversationChain<br/>å¯¹è¯é“¾]
    E --> E2[RetrievalQA<br/>é—®ç­”é“¾]
    E --> E3[SummarizationChain<br/>æ‘˜è¦é“¾]

    style A fill:#E3F2FD
    style B fill:#BBDEFB
    style C fill:#90CAF9
    style D fill:#64B5F6
    style E fill:#42A5F5
```

---

## äºŒã€åŸºç¡€ Chain

### 2.1 LLMChain

**LLMChain** æ˜¯æœ€åŸºç¡€çš„ Chainï¼Œå°† Prompt å’Œ LLM ç»„åˆåœ¨ä¸€èµ·ã€‚

#### 2.1.1 åŸºæœ¬ç”¨æ³•

```python
"""
LLMChain åŸºç¡€ç¤ºä¾‹
"""
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# 1. åˆ›å»º LLM
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

# 2. åˆ›å»º Prompt æ¨¡æ¿
prompt = PromptTemplate(
    input_variables=["topic"],
    template="è¯·ç”¨3å¥è¯ä»‹ç»{topic}"
)

# 3. åˆ›å»º LLMChain
chain = LLMChain(
    llm=llm,
    prompt=prompt,
    verbose=True  # æ˜¾ç¤ºæ‰§è¡Œè¿‡ç¨‹
)

# 4. è¿è¡Œ
result = chain.run(topic="é‡å­è®¡ç®—")
print(f"ç»“æœï¼š{result}")

# æˆ–è€…ä½¿ç”¨ invokeï¼ˆæ¨èï¼Œè¿”å›æ›´å¤šä¿¡æ¯ï¼‰
result = chain.invoke({"topic": "é‡å­è®¡ç®—"})
print(f"ç»“æœï¼š{result['text']}")
```

#### 2.1.2 å¤šå˜é‡ Prompt

```python
"""
å¤šå˜é‡ LLMChain
"""
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")

# å¤šä¸ªè¾“å…¥å˜é‡
prompt = PromptTemplate(
    input_variables=["language", "topic", "level"],
    template="""ä½œä¸ºä¸€å{language}ä¸“å®¶ï¼Œè¯·ç”¨{level}çš„è¯­è¨€è§£é‡Š{topic}ã€‚

è¦æ±‚ï¼š
- è¯­è¨€ï¼š{language}
- éš¾åº¦ï¼š{level}
- ä¸»é¢˜ï¼š{topic}

è¯·å¼€å§‹ï¼š"""
)

chain = LLMChain(llm=llm, prompt=prompt)

# æä¾›æ‰€æœ‰å˜é‡
result = chain.invoke({
    "language": "Python",
    "topic": "è£…é¥°å™¨",
    "level": "åˆå­¦è€…"
})

print(result['text'])
```

#### 2.1.3 æ‰¹é‡å¤„ç†

```python
"""
LLMChain æ‰¹é‡å¤„ç†
"""
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")
prompt = PromptTemplate.from_template("å°†{word}ç¿»è¯‘æˆè‹±æ–‡")
chain = LLMChain(llm=llm, prompt=prompt)

# æ‰¹é‡ç¿»è¯‘
words = ["è‹¹æœ", "é¦™è•‰", "æ©™å­", "è‘¡è„"]
inputs = [{"word": word} for word in words]

results = chain.batch(inputs)

for word, result in zip(words, results):
    print(f"{word} -> {result['text']}")
```

### 2.2 TransformChain

**TransformChain** ç”¨äºæ•°æ®è½¬æ¢ï¼Œä¸æ¶‰åŠ LLM è°ƒç”¨ã€‚

```python
"""
TransformChain ç¤ºä¾‹ï¼šæ•°æ®é¢„å¤„ç†
"""
from langchain.chains import TransformChain

def preprocess_text(inputs: dict) -> dict:
    """
    æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
    - å»é™¤å¤šä½™ç©ºæ ¼
    - è½¬æ¢ä¸ºå°å†™
    - ç§»é™¤æ ‡ç‚¹
    """
    text = inputs["text"]

    # å¤„ç†
    text = text.strip()
    text = text.lower()
    text = ''.join(c for c in text if c.isalnum() or c.isspace())

    return {"processed_text": text}

# åˆ›å»ºè½¬æ¢é“¾
transform_chain = TransformChain(
    input_variables=["text"],
    output_variables=["processed_text"],
    transform=preprocess_text
)

# ä½¿ç”¨
result = transform_chain.run(text="  Hello, World!  ")
print(f"é¢„å¤„ç†ç»“æœï¼š{result}")
```

---

## ä¸‰ã€é¡ºåº Chain

### 3.1 SimpleSequentialChain

**SimpleSequentialChain** ç”¨äºç®€å•çš„é¡ºåºæ‰§è¡Œï¼Œæ¯ä¸ª Chain çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªçš„è¾“å…¥ã€‚

```python
"""
SimpleSequentialChain ç¤ºä¾‹ï¼šæ–‡ç« ç”Ÿæˆæµæ°´çº¿
æµç¨‹ï¼šç”Ÿæˆå¤§çº² -> æ‰©å†™ç¬¬ä¸€æ®µ -> æ¶¦è‰²æ–‡æœ¬
"""
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain, SimpleSequentialChain
from langchain.prompts import PromptTemplate

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

# Chain 1: ç”Ÿæˆå¤§çº²
outline_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate.from_template(
        "ä¸ºä¸»é¢˜'{topic}'ç”Ÿæˆä¸€ä¸ª3ç‚¹å¤§çº²"
    )
)

# Chain 2: æ‰©å†™ç¬¬ä¸€æ®µ
expand_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate.from_template(
        "æ ¹æ®ä»¥ä¸‹å¤§çº²ï¼Œæ‰©å†™ç¬¬ä¸€æ®µï¼ˆ100å­—å·¦å³ï¼‰ï¼š\n{outline}"
    )
)

# Chain 3: æ¶¦è‰²æ–‡æœ¬
polish_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate.from_template(
        "æ¶¦è‰²ä»¥ä¸‹æ–‡æœ¬ï¼Œä½¿å…¶æ›´åŠ æµç•…ï¼š\n{text}"
    )
)

# ç»„åˆæˆé¡ºåºé“¾
overall_chain = SimpleSequentialChain(
    chains=[outline_chain, expand_chain, polish_chain],
    verbose=True
)

# è¿è¡Œ
result = overall_chain.run("äººå·¥æ™ºèƒ½çš„æœªæ¥")
print(f"\næœ€ç»ˆç»“æœï¼š\n{result}")
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
> Entering new SimpleSequentialChain chain...

1. äººå·¥æ™ºèƒ½çš„å®šä¹‰å’Œå‘å±•å†ç¨‹
2. å½“å‰äººå·¥æ™ºèƒ½çš„ä¸»è¦åº”ç”¨é¢†åŸŸ
3. æœªæ¥äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿å’ŒæŒ‘æˆ˜

äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æŠ€æœ¯...ï¼ˆç¬¬ä¸€æ®µæ‰©å†™ï¼‰

ç»è¿‡æ¶¦è‰²çš„æ–‡æœ¬ï¼š
äººå·¥æ™ºèƒ½ä½œä¸ºè®¡ç®—æœºç§‘å­¦çš„å‰æ²¿é¢†åŸŸ...ï¼ˆæ¶¦è‰²åçš„æ–‡æœ¬ï¼‰

> Finished chain.
```

::: warning æ³¨æ„
SimpleSequentialChain çš„é™åˆ¶ï¼š
- åªèƒ½æœ‰ä¸€ä¸ªè¾“å…¥å’Œä¸€ä¸ªè¾“å‡º
- æ¯ä¸ª Chain çš„è¾“å‡ºå¿…é¡»æ˜¯å­—ç¬¦ä¸²
- ä¸èƒ½ä¿ç•™ä¸­é—´ç»“æœ
:::

### 3.2 SequentialChain

**SequentialChain** æ˜¯æ›´å¼ºå¤§çš„é¡ºåºé“¾ï¼Œæ”¯æŒï¼š
- å¤šä¸ªè¾“å…¥å’Œè¾“å‡º
- ä¿ç•™ä¸­é—´ç»“æœ
- æ›´çµæ´»çš„æ•°æ®æµ

```python
"""
SequentialChain ç¤ºä¾‹ï¼šå•†å“è¯„è®ºåˆ†æ
æµç¨‹ï¼šæå–å…³é”®è¯ -> æƒ…æ„Ÿåˆ†æ -> ç”Ÿæˆå›å¤
"""
from langchain.chains import LLMChain, SequentialChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

# Chain 1: æå–å…³é”®è¯
keyword_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["review"],
        template="ä»ä»¥ä¸‹è¯„è®ºä¸­æå–3-5ä¸ªå…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼š\n{review}\n\nå…³é”®è¯ï¼š"
    ),
    output_key="keywords"  # æŒ‡å®šè¾“å‡ºé”®å
)

# Chain 2: æƒ…æ„Ÿåˆ†æ
sentiment_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["review"],
        template="åˆ¤æ–­ä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰ï¼š\n{review}\n\næƒ…æ„Ÿï¼š"
    ),
    output_key="sentiment"
)

# Chain 3: ç”Ÿæˆå›å¤
reply_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["review", "keywords", "sentiment"],
        template="""ä½œä¸ºå®¢æœï¼Œæ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆå›å¤ï¼š

è¯„è®ºï¼š{review}
å…³é”®è¯ï¼š{keywords}
æƒ…æ„Ÿï¼š{sentiment}

å›å¤ï¼ˆ50å­—ä»¥å†…ï¼‰ï¼š"""
    ),
    output_key="reply"
)

# ç»„åˆæˆ SequentialChain
overall_chain = SequentialChain(
    chains=[keyword_chain, sentiment_chain, reply_chain],
    input_variables=["review"],  # åˆå§‹è¾“å…¥
    output_variables=["keywords", "sentiment", "reply"],  # éœ€è¦ä¿ç•™çš„è¾“å‡º
    verbose=True
)

# æµ‹è¯•
review = "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œç‰©æµä¹Ÿå¾ˆå¿«ï¼Œä½†æ˜¯ä»·æ ¼æœ‰ç‚¹è´µã€‚å®¢æœæ€åº¦ä¸é”™ã€‚"
result = overall_chain.invoke({"review": review})

print("\n" + "=" * 60)
print("åˆ†æç»“æœï¼š")
print(f"å…³é”®è¯ï¼š{result['keywords']}")
print(f"æƒ…æ„Ÿï¼š{result['sentiment']}")
print(f"å›å¤ï¼š{result['reply']}")
print("=" * 60)
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
============================================================
åˆ†æç»“æœï¼š
å…³é”®è¯ï¼šè´¨é‡å¥½, ç‰©æµå¿«, ä»·æ ¼è´µ, å®¢æœæ€åº¦
æƒ…æ„Ÿï¼šæ­£é¢
å›å¤ï¼šæ„Ÿè°¢æ‚¨çš„è®¤å¯ï¼æˆ‘ä»¬ä¼šç»§ç»­ä¿æŒä¼˜è´¨çš„äº§å“å’ŒæœåŠ¡ï¼ŒåŒæ—¶ä¹Ÿä¼šè€ƒè™‘ä»·æ ¼ä¼˜åŒ–ã€‚
============================================================
```

### 3.3 é¡ºåºé“¾çš„æ•°æ®æµ

```mermaid
graph LR
    A[è¾“å…¥: review] --> B[Chain 1<br/>æå–å…³é”®è¯]
    B --> C[è¾“å‡º: keywords]
    A --> D[Chain 2<br/>æƒ…æ„Ÿåˆ†æ]
    D --> E[è¾“å‡º: sentiment]

    A --> F[Chain 3<br/>ç”Ÿæˆå›å¤]
    C --> F
    E --> F
    F --> G[è¾“å‡º: reply]

    style A fill:#E3F2FD
    style G fill:#C8E6C9
    style B fill:#FFE0B2
    style D fill:#FFE0B2
    style F fill:#FFE0B2
```

---

## å››ã€LCEL (LangChain Expression Language)

### 4.1 ä»€ä¹ˆæ˜¯ LCELï¼Ÿ

**LCEL** æ˜¯ LangChain çš„æ–°è¯­æ³•ï¼Œç”¨æ›´ç®€æ´çš„æ–¹å¼æ„å»º Chainã€‚

**ä¼ ç»Ÿæ–¹å¼ vs LCELï¼š**

```python
"""
å¯¹æ¯”ï¼šä¼ ç»Ÿ Chain vs LCEL
"""
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema.output_parser import StrOutputParser

llm = ChatOpenAI(model="gpt-3.5-turbo")
prompt = PromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")

# âŒ ä¼ ç»Ÿæ–¹å¼ï¼ˆç¹çï¼‰
traditional_chain = LLMChain(llm=llm, prompt=prompt)
result = traditional_chain.run(topic="ç¨‹åºå‘˜")

# âœ… LCEL æ–¹å¼ï¼ˆç®€æ´ï¼‰
lcel_chain = prompt | llm | StrOutputParser()
result = lcel_chain.invoke({"topic": "ç¨‹åºå‘˜"})
```

### 4.2 LCEL åŸºç¡€è¯­æ³•

#### è¯­æ³•1ï¼šç®¡é“æ“ä½œç¬¦ `|`

```python
"""
LCEL ç®¡é“æ“ä½œç¬¦
ä½œç”¨ï¼šå°†ç»„ä»¶ä¸²è”èµ·æ¥
"""
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# å®šä¹‰ç»„ä»¶
prompt = ChatPromptTemplate.from_template("ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}")
llm = ChatOpenAI(model="gpt-3.5-turbo")
output_parser = StrOutputParser()

# ä½¿ç”¨ | è¿æ¥
chain = prompt | llm | output_parser

# ç­‰ä»·äºï¼š
# 1. prompt.format(text="ä½ å¥½") -> messages
# 2. llm.invoke(messages) -> AIMessage
# 3. output_parser.parse(AIMessage) -> str

result = chain.invoke({"text": "ä½ å¥½ï¼Œä¸–ç•Œï¼"})
print(result)  # "Hello, World!"
```

#### è¯­æ³•2ï¼šå¹¶è¡Œæ‰§è¡Œ `RunnableParallel`

```python
"""
LCEL å¹¶è¡Œæ‰§è¡Œ
åœºæ™¯ï¼šåŒæ—¶æ‰§è¡Œå¤šä¸ªç‹¬ç«‹ä»»åŠ¡
"""
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableParallel

llm = ChatOpenAI(model="gpt-3.5-turbo")

# å®šä¹‰å¤šä¸ªç‹¬ç«‹çš„é“¾
translate_chain = (
    ChatPromptTemplate.from_template("ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}")
    | llm
    | StrOutputParser()
)

summarize_chain = (
    ChatPromptTemplate.from_template("æ€»ç»“ï¼ˆ20å­—ä»¥å†…ï¼‰ï¼š{text}")
    | llm
    | StrOutputParser()
)

sentiment_chain = (
    ChatPromptTemplate.from_template("åˆ¤æ–­æƒ…æ„Ÿï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰ï¼š{text}")
    | llm
    | StrOutputParser()
)

# å¹¶è¡Œæ‰§è¡Œ
parallel_chain = RunnableParallel(
    translation=translate_chain,
    summary=summarize_chain,
    sentiment=sentiment_chain
)

result = parallel_chain.invoke({
    "text": "è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼å‰§æƒ…ç´§å‡‘ï¼Œæ¼”å‘˜æ¼”æŠ€å‡ºè‰²ï¼Œè§†è§‰æ•ˆæœéœ‡æ’¼ã€‚å¼ºçƒˆæ¨èï¼"
})

print("ç¿»è¯‘ï¼š", result["translation"])
print("æ‘˜è¦ï¼š", result["summary"])
print("æƒ…æ„Ÿï¼š", result["sentiment"])
```

#### è¯­æ³•3ï¼šæ¡ä»¶åˆ†æ”¯ `RunnableBranch`

```python
"""
LCEL æ¡ä»¶åˆ†æ”¯
åœºæ™¯ï¼šæ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒçš„å¤„ç†è·¯å¾„
"""
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableBranch

llm = ChatOpenAI(model="gpt-3.5-turbo")

# å®šä¹‰ä¸åŒçš„å¤„ç†é“¾
positive_chain = (
    ChatPromptTemplate.from_template("æ„Ÿè°¢æ‚¨çš„æ­£é¢è¯„ä»·ï¼š{review}\n\nç”Ÿæˆæ„Ÿè°¢å›å¤ï¼š")
    | llm
    | StrOutputParser()
)

negative_chain = (
    ChatPromptTemplate.from_template("æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸å¥½çš„ä½“éªŒï¼š{review}\n\nç”Ÿæˆé“æ­‰å’Œæ”¹è¿›æ–¹æ¡ˆï¼š")
    | llm
    | StrOutputParser()
)

neutral_chain = (
    ChatPromptTemplate.from_template("æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼š{review}\n\nç”Ÿæˆä¸­æ€§å›å¤ï¼š")
    | llm
    | StrOutputParser()
)

# åˆ¤æ–­æƒ…æ„Ÿçš„å‡½æ•°
def classify_sentiment(inputs):
    review = inputs["review"]
    if "å¥½" in review or "æ£’" in review or "èµ" in review:
        return "positive"
    elif "å·®" in review or "çƒ‚" in review or "ç³Ÿ" in review:
        return "negative"
    else:
        return "neutral"

# åˆ›å»ºåˆ†æ”¯
branch = RunnableBranch(
    (lambda x: classify_sentiment(x) == "positive", positive_chain),
    (lambda x: classify_sentiment(x) == "negative", negative_chain),
    neutral_chain  # é»˜è®¤åˆ†æ”¯
)

# æµ‹è¯•
reviews = [
    {"review": "äº§å“è´¨é‡å¾ˆå¥½ï¼Œéå¸¸æ»¡æ„ï¼"},
    {"review": "è´¨é‡å¤ªå·®äº†ï¼Œå®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼ã€‚"},
    {"review": "ä¸€èˆ¬èˆ¬ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„ã€‚"}
]

for review in reviews:
    result = branch.invoke(review)
    print(f"è¯„è®ºï¼š{review['review']}")
    print(f"å›å¤ï¼š{result}\n")
```

### 4.3 LCEL é«˜çº§ç‰¹æ€§

#### ç‰¹æ€§1ï¼šæ•°æ®ä¼ é€’å’Œè½¬æ¢

```python
"""
LCEL æ•°æ®ä¼ é€’
ä½¿ç”¨ RunnablePassthrough å’Œ RunnableLambda
"""
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

llm = ChatOpenAI(model="gpt-3.5-turbo")

# åœºæ™¯ï¼šç”Ÿæˆä»£ç å¹¶æ·»åŠ æ³¨é‡Š
chain = (
    {
        "code": ChatPromptTemplate.from_template("ç”¨Pythonå†™ä¸€ä¸ª{function}å‡½æ•°")
                | llm
                | StrOutputParser(),
        "language": RunnablePassthrough()  # ç›´æ¥ä¼ é€’è¾“å…¥
    }
    | RunnableLambda(lambda x: f"# è¯­è¨€ï¼š{x['language']}\n{x['code']}")  # è‡ªå®šä¹‰å¤„ç†
)

result = chain.invoke({"function": "è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—", "language": "Python"})
print(result)
```

#### ç‰¹æ€§2ï¼šæµå¼è¾“å‡º

```python
"""
LCEL æµå¼è¾“å‡º
"""
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

llm = ChatOpenAI(model="gpt-3.5-turbo", streaming=True)

chain = (
    ChatPromptTemplate.from_template("å†™ä¸€ç¯‡å…³äº{topic}çš„çŸ­æ–‡")
    | llm
    | StrOutputParser()
)

# æµå¼è¾“å‡º
print("ç”Ÿæˆä¸­ï¼š", end="", flush=True)
for chunk in chain.stream({"topic": "äººå·¥æ™ºèƒ½"}):
    print(chunk, end="", flush=True)
print()
```

#### ç‰¹æ€§3ï¼šæ‰¹é‡å¤„ç†

```python
"""
LCEL æ‰¹é‡å¤„ç†
"""
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

chain = (
    ChatPromptTemplate.from_template("å°†{word}ç¿»è¯‘æˆè‹±æ–‡")
    | ChatOpenAI(model="gpt-3.5-turbo")
    | StrOutputParser()
)

# æ‰¹é‡ç¿»è¯‘
words = [{"word": "è‹¹æœ"}, {"word": "é¦™è•‰"}, {"word": "æ©™å­"}]
results = chain.batch(words)

for word, result in zip(words, results):
    print(f"{word['word']} -> {result}")
```

### 4.4 LCEL vs ä¼ ç»Ÿ Chain

| å¯¹æ¯”é¡¹ | ä¼ ç»Ÿ Chain | LCEL |
|--------|-----------|------|
| **è¯­æ³•** | å¯¹è±¡å¼ï¼Œç¹ç | ç®¡é“å¼ï¼Œç®€æ´ |
| **å¯è¯»æ€§** | â­â­ | â­â­â­â­â­ |
| **æµå¼æ”¯æŒ** | éœ€è¦ç‰¹æ®Šå¤„ç† | åŸç”Ÿæ”¯æŒ |
| **æ‰¹é‡å¤„ç†** | éœ€è¦æ‰‹åŠ¨å®ç° | å†…ç½®æ”¯æŒ |
| **å¹¶è¡Œæ‰§è¡Œ** | å¤æ‚ | ç®€å• |
| **æ¨èç¨‹åº¦** | â­â­ | â­â­â­â­â­ |

---

## äº”ã€å®æˆ˜é¡¹ç›®

### 5.1 é¡¹ç›®ï¼šæ™ºèƒ½æ–‡ç« ç”Ÿæˆç³»ç»Ÿ

```python
"""
é¡¹ç›®ï¼šæ™ºèƒ½æ–‡ç« ç”Ÿæˆç³»ç»Ÿ
åŠŸèƒ½ï¼š
1. ç”Ÿæˆæ–‡ç« å¤§çº²
2. é€æ®µæ‰©å†™
3. æ·»åŠ æ€»ç»“
4. æ ¼å¼åŒ–è¾“å‡º
"""
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough

class ArticleGenerator:
    """æ–‡ç« ç”Ÿæˆå™¨"""

    def __init__(self, model: str = "gpt-3.5-turbo"):
        self.llm = ChatOpenAI(model=model, temperature=0.7)

    def generate_outline(self, topic: str, num_points: int = 3) -> str:
        """ç”Ÿæˆå¤§çº²"""
        chain = (
            ChatPromptTemplate.from_template(
                "ä¸ºä¸»é¢˜'{topic}'ç”Ÿæˆ{num_points}ç‚¹å¤§çº²ï¼Œæ¯ç‚¹ç”¨ä¸€è¡Œï¼Œæ ¼å¼ï¼š\n1. ...\n2. ..."
            )
            | self.llm
            | StrOutputParser()
        )
        return chain.invoke({"topic": topic, "num_points": num_points})

    def expand_point(self, point: str) -> str:
        """æ‰©å†™å•ä¸ªè¦ç‚¹"""
        chain = (
            ChatPromptTemplate.from_template(
                "å°†ä»¥ä¸‹è¦ç‚¹æ‰©å†™æˆä¸€æ®µè¯ï¼ˆ100-150å­—ï¼‰ï¼š\n{point}"
            )
            | self.llm
            | StrOutputParser()
        )
        return chain.invoke({"point": point})

    def generate_conclusion(self, article: str) -> str:
        """ç”Ÿæˆç»“è®º"""
        chain = (
            ChatPromptTemplate.from_template(
                "ä¸ºä»¥ä¸‹æ–‡ç« å†™ä¸€ä¸ªç®€çŸ­çš„ç»“è®ºï¼ˆ50å­—ä»¥å†…ï¼‰ï¼š\n\n{article}"
            )
            | self.llm
            | StrOutputParser()
        )
        return chain.invoke({"article": article})

    def generate_article(self, topic: str, num_points: int = 3) -> dict:
        """
        ç”Ÿæˆå®Œæ•´æ–‡ç« 

        è¿”å›:
            åŒ…å« title, outline, body, conclusion çš„å­—å…¸
        """
        print(f"æ­£åœ¨ç”Ÿæˆæ–‡ç« ï¼š{topic}")

        # æ­¥éª¤1ï¼šç”Ÿæˆå¤§çº²
        print("  [1/4] ç”Ÿæˆå¤§çº²...")
        outline = self.generate_outline(topic, num_points)
        print(f"  å¤§çº²ï¼š\n{outline}\n")

        # æ­¥éª¤2ï¼šæ‰©å†™æ¯ä¸ªè¦ç‚¹
        print("  [2/4] æ‰©å†™è¦ç‚¹...")
        points = [line.strip() for line in outline.split("\n") if line.strip()]
        paragraphs = []

        for i, point in enumerate(points, 1):
            print(f"    æ‰©å†™ç¬¬ {i} ç‚¹...")
            paragraph = self.expand_point(point)
            paragraphs.append(paragraph)

        body = "\n\n".join(paragraphs)

        # æ­¥éª¤3ï¼šç”Ÿæˆç»“è®º
        print("  [3/4] ç”Ÿæˆç»“è®º...")
        conclusion = self.generate_conclusion(body)

        # æ­¥éª¤4ï¼šæ ¼å¼åŒ–
        print("  [4/4] æ ¼å¼åŒ–è¾“å‡º...")
        full_article = f"""# {topic}

## å¤§çº²
{outline}

## æ­£æ–‡
{body}

## ç»“è®º
{conclusion}
"""

        return {
            "title": topic,
            "outline": outline,
            "body": body,
            "conclusion": conclusion,
            "full_article": full_article
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    generator = ArticleGenerator()

    result = generator.generate_article(
        topic="äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨",
        num_points=3
    )

    print("\n" + "=" * 60)
    print(result["full_article"])
    print("=" * 60)
```

### 5.2 é¡¹ç›®ï¼šå¤šè¯­è¨€ç¿»è¯‘æ ¡å¯¹ç³»ç»Ÿ

```python
"""
é¡¹ç›®ï¼šå¤šè¯­è¨€ç¿»è¯‘æ ¡å¯¹ç³»ç»Ÿ
åŠŸèƒ½ï¼š
1. ç¿»è¯‘æ–‡æœ¬
2. å›è¯‘æ ¡å¯¹
3. è¯„ä¼°ç¿»è¯‘è´¨é‡
4. æä¾›æ”¹è¿›å»ºè®®
"""
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableParallel

class TranslationReviewer:
    """ç¿»è¯‘æ ¡å¯¹ç³»ç»Ÿ"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

    def translate(self, text: str, target_language: str) -> str:
        """ç¿»è¯‘æ–‡æœ¬"""
        chain = (
            ChatPromptTemplate.from_template(
                "å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼š\n{text}"
            )
            | self.llm
            | StrOutputParser()
        )
        return chain.invoke({"text": text, "target_language": target_language})

    def back_translate(self, text: str, original_language: str) -> str:
        """å›è¯‘æ–‡æœ¬"""
        chain = (
            ChatPromptTemplate.from_template(
                "å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{original_language}ï¼š\n{text}"
            )
            | self.llm
            | StrOutputParser()
        )
        return chain.invoke({"text": text, "original_language": original_language})

    def evaluate_quality(self, original: str, back_translated: str) -> dict:
        """è¯„ä¼°ç¿»è¯‘è´¨é‡"""
        chain = (
            ChatPromptTemplate.from_template(
                """æ¯”è¾ƒåŸæ–‡å’Œå›è¯‘æ–‡æœ¬ï¼Œè¯„ä¼°ç¿»è¯‘è´¨é‡ï¼š

åŸæ–‡ï¼š{original}
å›è¯‘ï¼š{back_translated}

è¯·æä¾›ï¼š
1. ç›¸ä¼¼åº¦è¯„åˆ†ï¼ˆ0-100ï¼‰
2. ä¸»è¦å·®å¼‚
3. æ”¹è¿›å»ºè®®

æ ¼å¼ï¼š
è¯„åˆ†: XX
å·®å¼‚: ...
å»ºè®®: ..."""
            )
            | self.llm
            | StrOutputParser()
        )
        evaluation = chain.invoke({
            "original": original,
            "back_translated": back_translated
        })

        # è§£æè¯„ä¼°ç»“æœ
        lines = evaluation.split("\n")
        score = None
        differences = ""
        suggestions = ""

        for line in lines:
            if "è¯„åˆ†" in line or "score" in line.lower():
                import re
                match = re.search(r'\d+', line)
                if match:
                    score = int(match.group())
            elif "å·®å¼‚" in line or "difference" in line.lower():
                differences = line.split(":", 1)[1].strip() if ":" in line else ""
            elif "å»ºè®®" in line or "suggestion" in line.lower():
                suggestions = line.split(":", 1)[1].strip() if ":" in line else ""

        return {
            "score": score or 0,
            "differences": differences,
            "suggestions": suggestions,
            "full_evaluation": evaluation
        }

    def review_translation(
        self,
        text: str,
        target_language: str,
        original_language: str = "ä¸­æ–‡"
    ) -> dict:
        """
        å®Œæ•´çš„ç¿»è¯‘æ ¡å¯¹æµç¨‹

        è¿”å›:
            åŒ…å«ç¿»è¯‘ç»“æœå’Œè´¨é‡è¯„ä¼°çš„å­—å…¸
        """
        print(f"å¼€å§‹ç¿»è¯‘æ ¡å¯¹æµç¨‹...")
        print(f"  åŸæ–‡ï¼š{text}")

        # æ­¥éª¤1ï¼šç¿»è¯‘
        print(f"\n[1/3] ç¿»è¯‘æˆ{target_language}...")
        translation = self.translate(text, target_language)
        print(f"  è¯‘æ–‡ï¼š{translation}")

        # æ­¥éª¤2ï¼šå›è¯‘
        print(f"\n[2/3] å›è¯‘æˆ{original_language}...")
        back_translation = self.back_translate(translation, original_language)
        print(f"  å›è¯‘ï¼š{back_translation}")

        # æ­¥éª¤3ï¼šè¯„ä¼°
        print(f"\n[3/3] è¯„ä¼°ç¿»è¯‘è´¨é‡...")
        evaluation = self.evaluate_quality(text, back_translation)

        print(f"\nè¯„ä¼°ç»“æœï¼š")
        print(f"  ç›¸ä¼¼åº¦è¯„åˆ†ï¼š{evaluation['score']}/100")
        print(f"  ä¸»è¦å·®å¼‚ï¼š{evaluation['differences']}")
        print(f"  æ”¹è¿›å»ºè®®ï¼š{evaluation['suggestions']}")

        return {
            "original": text,
            "translation": translation,
            "back_translation": back_translation,
            "evaluation": evaluation
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    reviewer = TranslationReviewer()

    result = reviewer.review_translation(
        text="äººå·¥æ™ºèƒ½æ­£åœ¨æ·±åˆ»æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼å’Œå·¥ä½œæ–¹å¼ã€‚",
        target_language="è‹±æ–‡",
        original_language="ä¸­æ–‡"
    )

    print("\n" + "=" * 60)
    print("å®Œæ•´æŠ¥å‘Šï¼š")
    print(f"åŸæ–‡ï¼š{result['original']}")
    print(f"è¯‘æ–‡ï¼š{result['translation']}")
    print(f"å›è¯‘ï¼š{result['back_translation']}")
    print(f"è´¨é‡è¯„åˆ†ï¼š{result['evaluation']['score']}/100")
    print("=" * 60)
```

---

## å…­ã€æœ¬å‘¨ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šæ„å»ºç®€å• Chainï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰

**ä»»åŠ¡**ï¼šåˆ›å»ºä¸€ä¸ª"ä»£ç å®¡æŸ¥ Chain"ï¼ŒåŒ…å«ï¼š
1. æ£€æŸ¥ä»£ç é£æ ¼
2. åˆ†ææ½œåœ¨ bug
3. æä¾›ä¼˜åŒ–å»ºè®®

<details>
<summary>æŸ¥çœ‹æç¤º</summary>

ä½¿ç”¨ SequentialChainï¼Œåˆ›å»ºä¸‰ä¸ª LLMChain åˆ†åˆ«å¤„ç†ä¸Šè¿°ä»»åŠ¡ã€‚
</details>

### ç»ƒä¹ 2ï¼šLCEL å®è·µï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰

**ä»»åŠ¡**ï¼šç”¨ LCEL å®ç°å¹¶è¡Œçš„å¤šè¯­è¨€ç¿»è¯‘ï¼ˆåŒæ—¶ç¿»è¯‘æˆè‹±æ–‡ã€æ—¥æ–‡ã€æ³•æ–‡ï¼‰ã€‚

<details>
<summary>æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableParallel

llm = ChatOpenAI(model="gpt-3.5-turbo")

# å®šä¹‰å„è¯­è¨€çš„ç¿»è¯‘é“¾
en_chain = (
    ChatPromptTemplate.from_template("ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}")
    | llm | StrOutputParser()
)

ja_chain = (
    ChatPromptTemplate.from_template("ç¿»è¯‘æˆæ—¥æ–‡ï¼š{text}")
    | llm | StrOutputParser()
)

fr_chain = (
    ChatPromptTemplate.from_template("ç¿»è¯‘æˆæ³•æ–‡ï¼š{text}")
    | llm | StrOutputParser()
)

# å¹¶è¡Œæ‰§è¡Œ
parallel_chain = RunnableParallel(
    english=en_chain,
    japanese=ja_chain,
    french=fr_chain
)

result = parallel_chain.invoke({"text": "ä½ å¥½ï¼Œä¸–ç•Œï¼"})
print(result)
```
</details>

### ç»ƒä¹ 3ï¼šå®æˆ˜é¡¹ç›®ï¼ˆéš¾åº¦ï¼šâ­â­â­ï¼‰

**ä»»åŠ¡**ï¼šæ„å»º"æ™ºèƒ½ç®€å†ä¼˜åŒ–ç³»ç»Ÿ"ï¼ŒåŒ…å«ï¼š
1. åˆ†æç®€å†ç»“æ„
2. è¯†åˆ«è–„å¼±ç¯èŠ‚
3. ç”Ÿæˆä¼˜åŒ–å»ºè®®
4. æ”¹å†™å…³é”®éƒ¨åˆ†

<details>
<summary>æŸ¥çœ‹æç¤º</summary>

1. ä½¿ç”¨ SequentialChain ç»„ç»‡æµç¨‹
2. ç”¨ RunnableParallel å¹¶è¡Œåˆ†æä¸åŒéƒ¨åˆ†
3. æœ€åæ±‡æ€»ç”ŸæˆæŠ¥å‘Š
</details>

---

## ä¸ƒã€æœ¬å‘¨æ€»ç»“

### 7.1 çŸ¥è¯†ç‚¹æ¸…å•

- [x] Chain çš„æ¦‚å¿µå’Œä½œç”¨
- [x] LLMChain åŸºç¡€ç”¨æ³•
- [x] SimpleSequentialChain å’Œ SequentialChain
- [x] LCEL è¯­æ³•å’Œé«˜çº§ç‰¹æ€§
- [x] å¹¶è¡Œæ‰§è¡Œå’Œæ¡ä»¶åˆ†æ”¯
- [x] å®æˆ˜é¡¹ç›®å¼€å‘

### 7.2 Chain é€‰æ‹©æŒ‡å—

```mermaid
graph TD
    A[é€‰æ‹© Chain] --> B{å‡ ä¸ªæ­¥éª¤?}
    B -->|å•æ­¥éª¤| C[LLMChain]
    B -->|å¤šæ­¥éª¤| D{éœ€è¦ä¿ç•™ä¸­é—´ç»“æœ?}

    D -->|ä¸éœ€è¦| E[SimpleSequentialChain]
    D -->|éœ€è¦| F[SequentialChain æˆ– LCEL]

    F --> G{æ˜¯å¦æœ‰æ¡ä»¶åˆ†æ”¯?}
    G -->|æœ‰| H[LCEL + RunnableBranch]
    G -->|æ— | I[LCEL æˆ– SequentialChain]

    style C fill:#C8E6C9
    style E fill:#FFE082
    style F fill:#FFE082
    style H fill:#81C784
    style I fill:#81C784
```

### 7.3 ä¸‹å‘¨é¢„ä¹ 

**ç¬¬5å‘¨ä¸»é¢˜ï¼šDocuments æ–‡æ¡£å¤„ç†**

é¢„ä¹ å†…å®¹ï¼š
1. DocumentLoader çš„ä½œç”¨
2. TextSplitter çš„åˆ†å‰²ç­–ç•¥
3. å¦‚ä½•å¤„ç† PDFã€Word ç­‰æ–‡ä»¶

**æ€è€ƒé—®é¢˜**ï¼š
- ä¸ºä»€ä¹ˆè¦åˆ†å‰²æ–‡æ¡£ï¼Ÿ
- å¦‚ä½•é€‰æ‹©åˆé€‚çš„åˆ†å‰²å¤§å°ï¼Ÿ

---

::: tip å­¦ä¹ å»ºè®®
1. **å¤šç»ƒä¹  LCEL**ï¼šè¿™æ˜¯æœªæ¥çš„ä¸»æµè¯­æ³•
2. **ç†è§£æ•°æ®æµ**ï¼šææ¸…æ¥šæ¯ä¸ª Chain çš„è¾“å…¥è¾“å‡º
3. **æ¨¡å—åŒ–æ€ç»´**ï¼šæŠŠå¤æ‚ä»»åŠ¡åˆ†è§£æˆå°çš„ Chain
4. **å®æˆ˜ä¸ºä¸»**ï¼šé€šè¿‡å®é™…é¡¹ç›®å·©å›ºçŸ¥è¯†
:::

**æœ¬å‘¨å®Œæˆï¼ç»§ç»­åŠ æ²¹ï¼ğŸš€**
