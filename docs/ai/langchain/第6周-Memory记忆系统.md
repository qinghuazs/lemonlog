---
title: ç¬¬6å‘¨ - Memory è®°å¿†ç³»ç»Ÿ
date: 2025-02-19
permalink: /ai/langchain/week6-memory.html
tags:
  - LangChain
  - Memory
  - å¯¹è¯ç®¡ç†
categories:
  - AI
  - LangChain
---

# ç¬¬6å‘¨ï¼šMemory è®°å¿†ç³»ç»Ÿ

::: tip æœ¬å‘¨å­¦ä¹ ç›®æ ‡
- ğŸ§  ç†è§£ Memory çš„æ¦‚å¿µå’Œé‡è¦æ€§
- ğŸ’¾ æŒæ¡å¤šç§ Memory ç±»å‹
- ğŸ”„ å­¦ä¼šå¯¹è¯å†å²ç®¡ç†
- ğŸ¯ å®ç°é•¿æœŸè®°å¿†å’ŒçŸ­æœŸè®°å¿†
- ğŸ’¡ æ„å»ºæœ‰è®°å¿†çš„èŠå¤©æœºå™¨äºº
:::

## ä¸€ã€Memory åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ Memoryï¼Ÿ

**Memoryï¼ˆè®°å¿†ï¼‰** æ˜¯ LangChain ä¸­ç”¨äºå­˜å‚¨å’Œç®¡ç†å¯¹è¯å†å²ã€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç»„ä»¶ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦ Memoryï¼Ÿ

LLM æœ¬èº«æ˜¯**æ— çŠ¶æ€**çš„ï¼Œæ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹çš„ï¼š

```mermaid
graph LR
    subgraph æ— è®°å¿†
    A1[ç”¨æˆ·: æˆ‘å«å°æ˜] --> B1[AI: ä½ å¥½!]
    A2[ç”¨æˆ·: æˆ‘å«ä»€ä¹ˆ?] --> B2[AI: æˆ‘ä¸çŸ¥é“]
    end

    subgraph æœ‰è®°å¿†
    C1[ç”¨æˆ·: æˆ‘å«å°æ˜] --> D1[AI: ä½ å¥½å°æ˜!]
    C2[ç”¨æˆ·: æˆ‘å«ä»€ä¹ˆ?] --> D2[AI: ä½ å«å°æ˜]
    D1 -.è®°å¿†.-> D2
    end

    style A2 fill:#FFCDD2
    style B2 fill:#FFCDD2
    style C2 fill:#C8E6C9
    style D2 fill:#C8E6C9
```

**å¯¹æ¯”ï¼šæ—  Memory vs æœ‰ Memory**

```python
"""
å¯¹æ¯”ç¤ºä¾‹ï¼šæ— è®°å¿† vs æœ‰è®°å¿†
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

print("=" * 60)
print("åœºæ™¯1ï¼šæ— è®°å¿†ï¼ˆæ¯æ¬¡è°ƒç”¨ç‹¬ç«‹ï¼‰")
print("=" * 60)

# ç¬¬ä¸€æ¬¡å¯¹è¯
response1 = llm.invoke([
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"),
    HumanMessage(content="æˆ‘å«å°æ˜ï¼Œä»Šå¹´25å²")
])
print(f"ç”¨æˆ·: æˆ‘å«å°æ˜ï¼Œä»Šå¹´25å²")
print(f"AI: {response1.content}\n")

# ç¬¬äºŒæ¬¡å¯¹è¯ï¼ˆAI ä¸è®°å¾—ä¹‹å‰çš„å†…å®¹ï¼‰
response2 = llm.invoke([
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"),
    HumanMessage(content="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
])
print(f"ç”¨æˆ·: æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
print(f"AI: {response2.content}")
print("âŒ AI æ— æ³•å›ç­”ï¼Œå› ä¸ºæ²¡æœ‰è®°å¿†\n")

print("=" * 60)
print("åœºæ™¯2ï¼šæœ‰è®°å¿†ï¼ˆä½¿ç”¨ ConversationChainï¼‰")
print("=" * 60)

# åˆ›å»ºå¸¦è®°å¿†çš„å¯¹è¯é“¾
conversation = ConversationChain(
    llm=llm,
    memory=ConversationBufferMemory(),
    verbose=True  # æ˜¾ç¤ºå†…éƒ¨å¤„ç†è¿‡ç¨‹
)

# ç¬¬ä¸€æ¬¡å¯¹è¯
response1 = conversation.predict(input="æˆ‘å«å°æ˜ï¼Œä»Šå¹´25å²")
print(f"ç”¨æˆ·: æˆ‘å«å°æ˜ï¼Œä»Šå¹´25å²")
print(f"AI: {response1}\n")

# ç¬¬äºŒæ¬¡å¯¹è¯ï¼ˆAI èƒ½è®°ä½ä¹‹å‰çš„å†…å®¹ï¼‰
response2 = conversation.predict(input="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
print(f"ç”¨æˆ·: æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
print(f"AI: {response2}")
print("âœ… AI æ­£ç¡®å›ç­”ï¼Œå› ä¸ºæœ‰è®°å¿†")
```

### 1.2 Memory çš„æ ¸å¿ƒåŠŸèƒ½

```mermaid
graph TB
    A[Memory æ ¸å¿ƒåŠŸèƒ½] --> B[å­˜å‚¨ Save]
    A --> C[åŠ è½½ Load]
    A --> D[æ¸…é™¤ Clear]
    A --> E[æœç´¢ Search]

    B --> B1[ä¿å­˜å¯¹è¯å†å²]
    C --> C1[åŠ è½½å†å²ä¾› LLM ä½¿ç”¨]
    D --> D1[æ¸…ç©ºè®°å¿†é‡æ–°å¼€å§‹]
    E --> E1[æ£€ç´¢ç›¸å…³å†å²]

    style A fill:#E3F2FD
    style B fill:#C8E6C9
    style C fill:#FFE082
    style D fill:#FFCDD2
    style E fill:#CE93D8
```

#### Memory çš„ç”Ÿå‘½å‘¨æœŸ

```python
"""
Memory çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ
"""
from langchain.memory import ConversationBufferMemory

# 1. åˆ›å»ºè®°å¿†
memory = ConversationBufferMemory()

# 2. ä¿å­˜å¯¹è¯ï¼ˆSaveï¼‰
memory.save_context(
    inputs={"input": "ä½ å¥½ï¼Œæˆ‘æ˜¯ç”¨æˆ·"},
    outputs={"output": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}
)

memory.save_context(
    inputs={"input": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"},
    outputs={"output": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è·å–å®æ—¶å¤©æ°”ä¿¡æ¯"}
)

# 3. åŠ è½½è®°å¿†ï¼ˆLoadï¼‰
history = memory.load_memory_variables({})
print("å¯¹è¯å†å²ï¼š")
print(history['history'])

# 4. æ¸…é™¤è®°å¿†ï¼ˆClearï¼‰
memory.clear()
print("\næ¸…é™¤åçš„å†å²ï¼š")
print(memory.load_memory_variables({})['history'])
```

---

## äºŒã€åŸºç¡€ Memory ç±»å‹

### 2.1 ConversationBufferMemory

**ConversationBufferMemory** æ˜¯æœ€ç®€å•çš„è®°å¿†ç±»å‹ï¼Œå®Œæ•´ä¿å­˜æ‰€æœ‰å¯¹è¯å†å²ã€‚

#### 2.1.1 åŸºæœ¬ç”¨æ³•

```python
"""
ConversationBufferMemory åŸºç¡€ç¤ºä¾‹
ç‰¹ç‚¹ï¼šå®Œæ•´ä¿å­˜æ‰€æœ‰å¯¹è¯
"""
from langchain.memory import ConversationBufferMemory

# åˆ›å»ºè®°å¿†å®ä¾‹
memory = ConversationBufferMemory()

# æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
conversations = [
    ("ä½ å¥½", "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"),
    ("æˆ‘æƒ³å­¦ä¹  Python", "å¾ˆå¥½ï¼Python æ˜¯ä¸€é—¨ä¼˜ç§€çš„è¯­è¨€"),
    ("ä»å“ªé‡Œå¼€å§‹ï¼Ÿ", "å»ºè®®ä»åŸºç¡€è¯­æ³•å¼€å§‹ï¼Œç„¶åé€æ­¥æ·±å…¥"),
]

# ä¿å­˜å¯¹è¯
for user_input, ai_output in conversations:
    memory.save_context(
        inputs={"input": user_input},
        outputs={"output": ai_output}
    )

# æŸ¥çœ‹å®Œæ•´å†å²
history = memory.load_memory_variables({})
print("å®Œæ•´å¯¹è¯å†å²ï¼š")
print(history['history'])
print(f"\næ€»å­—ç¬¦æ•°ï¼š{len(history['history'])}")
```

#### 2.1.2 è¿”å›æ¶ˆæ¯å¯¹è±¡

```python
"""
è¿”å›æ¶ˆæ¯å¯¹è±¡æ ¼å¼ï¼ˆç”¨äº Chat Modelsï¼‰
"""
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    return_messages=True  # è¿”å›æ¶ˆæ¯å¯¹è±¡è€Œéå­—ç¬¦ä¸²
)

memory.save_context(
    {"input": "ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"},
    {"output": "æˆ‘æ˜¯ AI åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜"}
)

history = memory.load_memory_variables({})
print("æ¶ˆæ¯æ ¼å¼çš„å†å²ï¼š")
for msg in history['history']:
    print(f"{msg.__class__.__name__}: {msg.content}")
```

#### 2.1.3 è‡ªå®šä¹‰é”®å

```python
"""
è‡ªå®šä¹‰è¾“å…¥è¾“å‡ºé”®å
"""
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    input_key="question",      # è‡ªå®šä¹‰è¾“å…¥é”®
    output_key="answer",       # è‡ªå®šä¹‰è¾“å‡ºé”®
    memory_key="chat_history"  # è‡ªå®šä¹‰è®°å¿†é”®
)

memory.save_context(
    {"question": "ä»€ä¹ˆæ˜¯ AIï¼Ÿ"},
    {"answer": "AI æ˜¯äººå·¥æ™ºèƒ½çš„ç¼©å†™"}
)

history = memory.load_memory_variables({})
print(f"å†å²ï¼ˆé”®å: {list(history.keys())}ï¼‰:")
print(history['chat_history'])
```

#### 2.1.4 ä¸ Chain é›†æˆ

```python
"""
ConversationBufferMemory ä¸ Chain é›†æˆ
"""
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

# åˆ›å»ºå¸¦è®°å¿†çš„å¯¹è¯é“¾
conversation = ConversationChain(
    llm=llm,
    memory=ConversationBufferMemory(),
    verbose=True  # æ˜¾ç¤ºå®Œæ•´çš„ prompt
)

# å¤šè½®å¯¹è¯
print("=== å¯¹è¯å¼€å§‹ ===\n")

response1 = conversation.predict(input="æˆ‘æ˜¯ä¸€å Python å¼€å‘è€…")
print(f"å›ç­”1: {response1}\n")

response2 = conversation.predict(input="æˆ‘çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ")
print(f"å›ç­”2: {response2}\n")

response3 = conversation.predict(input="ç»™æˆ‘æ¨èä¸€æœ¬ä¹¦")
print(f"å›ç­”3: {response3}")
```

::: warning ConversationBufferMemory çš„é—®é¢˜
**ä¼˜ç‚¹ï¼š**
- ç®€å•ç›´è§‚
- å®Œæ•´ä¿ç•™æ‰€æœ‰ä¿¡æ¯

**ç¼ºç‚¹ï¼š**
- éšç€å¯¹è¯å¢é•¿ï¼ŒToken æ¶ˆè€—å¢åŠ 
- å¯èƒ½è¶…å‡ºä¸Šä¸‹æ–‡çª—å£é™åˆ¶
- æˆæœ¬ä¸æ–­ä¸Šå‡

**é€‚ç”¨åœºæ™¯ï¼š**
- çŸ­å¯¹è¯ï¼ˆ<10 è½®ï¼‰
- è°ƒè¯•å’Œæµ‹è¯•
- ä¸å…³å¿ƒæˆæœ¬çš„åœºæ™¯
:::

### 2.2 ConversationBufferWindowMemory

**ConversationBufferWindowMemory** åªä¿ç•™æœ€è¿‘ K è½®å¯¹è¯ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ã€‚

```python
"""
ConversationBufferWindowMemory ç¤ºä¾‹
ç‰¹ç‚¹ï¼šåªä¿ç•™æœ€è¿‘ K è½®å¯¹è¯
"""
from langchain.memory import ConversationBufferWindowMemory

# åªä¿ç•™æœ€è¿‘ 2 è½®å¯¹è¯
memory = ConversationBufferWindowMemory(k=2)

# æ¨¡æ‹Ÿ 5 è½®å¯¹è¯
conversations = [
    ("ç¬¬1è½®ï¼šä½ å¥½", "ä½ å¥½ï¼"),
    ("ç¬¬2è½®ï¼šæˆ‘å«å°æ˜", "å¾ˆé«˜å…´è®¤è¯†ä½ ï¼Œå°æ˜"),
    ("ç¬¬3è½®ï¼šæˆ‘ä»Šå¹´25å²", "çŸ¥é“äº†"),
    ("ç¬¬4è½®ï¼šæˆ‘æ˜¯ç¨‹åºå‘˜", "ç¨‹åºå‘˜æ˜¯ä¸ªå¥½èŒä¸š"),
    ("ç¬¬5è½®ï¼šæˆ‘å«ä»€ä¹ˆï¼Ÿ", "..."),
]

for user_input, ai_output in conversations[:-1]:
    memory.save_context(
        {"input": user_input},
        {"output": ai_output}
    )

# æŸ¥çœ‹è®°å¿†ï¼ˆåªæœ‰æœ€è¿‘2è½®ï¼‰
history = memory.load_memory_variables({})
print("è®°å¿†ä¸­çš„å¯¹è¯ï¼ˆæœ€è¿‘2è½®ï¼‰ï¼š")
print(history['history'])
print("\nâŒ ç¬¬1ã€2ã€3è½®çš„ä¿¡æ¯å·²è¢«ä¸¢å¼ƒ")
```

**å¯è§†åŒ–çª—å£ç§»åŠ¨ï¼š**

```
å¯¹è¯åºåˆ—ï¼š[1] [2] [3] [4] [5] [6]
çª—å£å¤§å° k=2

åˆå§‹ï¼š[1] [2]
æ·»åŠ 3ï¼š    [2] [3]  ï¼ˆ1è¢«ä¸¢å¼ƒï¼‰
æ·»åŠ 4ï¼š        [3] [4]  ï¼ˆ2è¢«ä¸¢å¼ƒï¼‰
æ·»åŠ 5ï¼š            [4] [5]
æ·»åŠ 6ï¼š                [5] [6]
```

#### å®Œæ•´ç¤ºä¾‹ï¼šçª—å£è®°å¿†èŠå¤©

```python
"""
ä½¿ç”¨çª—å£è®°å¿†çš„èŠå¤©æœºå™¨äºº
"""
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferWindowMemory

llm = ChatOpenAI(model="gpt-3.5-turbo")

# åˆ›å»ºçª—å£è®°å¿†ï¼ˆåªä¿ç•™æœ€è¿‘3è½®ï¼‰
conversation = ConversationChain(
    llm=llm,
    memory=ConversationBufferWindowMemory(k=3),
    verbose=False
)

# æ¨¡æ‹Ÿé•¿å¯¹è¯
dialogues = [
    "æˆ‘å«å¼ ä¸‰",
    "æˆ‘æ˜¯ç¨‹åºå‘˜",
    "æˆ‘åœ¨åŒ—äº¬å·¥ä½œ",
    "æˆ‘å–œæ¬¢Python",
    "æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ",  # è¶…å‡ºçª—å£ï¼Œå¯èƒ½ç­”ä¸ä¸Šæ¥
]

for i, user_input in enumerate(dialogues, 1):
    print(f"\nç¬¬{i}è½® - ç”¨æˆ·: {user_input}")
    response = conversation.predict(input=user_input)
    print(f"AI: {response}")

print("\n" + "=" * 60)
print("æœ€åçš„è®°å¿†å†…å®¹ï¼ˆæœ€è¿‘3è½®ï¼‰ï¼š")
print(conversation.memory.load_memory_variables({})['history'])
```

::: tip ConversationBufferWindowMemory çš„ç‰¹ç‚¹
**ä¼˜ç‚¹ï¼š**
- å›ºå®š Token æ¶ˆè€—
- ä¸ä¼šè¶…å‡ºä¸Šä¸‹æ–‡é™åˆ¶
- æˆæœ¬å¯æ§

**ç¼ºç‚¹ï¼š**
- æ—©æœŸä¿¡æ¯ä¼šä¸¢å¤±
- å¯èƒ½æ— æ³•å›ç­”å…³äºæ—©æœŸå¯¹è¯çš„é—®é¢˜

**é€‚ç”¨åœºæ™¯ï¼š**
- é•¿æ—¶é—´å¯¹è¯
- æˆæœ¬æ•æ„Ÿçš„åº”ç”¨
- åªå…³å¿ƒæœ€è¿‘ä¸Šä¸‹æ–‡çš„åœºæ™¯
:::

### 2.3 ConversationSummaryMemory

**ConversationSummaryMemory** ä½¿ç”¨ LLM æ€»ç»“å¯¹è¯å†å²ï¼Œå‹ç¼©ä¿¡æ¯ã€‚

```python
"""
ConversationSummaryMemory ç¤ºä¾‹
ç‰¹ç‚¹ï¼šè‡ªåŠ¨æ€»ç»“å¯¹è¯ï¼Œå‹ç¼©å†å²
"""
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryMemory

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# åˆ›å»ºæ€»ç»“è®°å¿†
memory = ConversationSummaryMemory(
    llm=llm,
    return_messages=False
)

# æ·»åŠ å¯¹è¯
conversations = [
    ("æˆ‘å«æå››ï¼Œæ˜¯ä¸€åæ•°æ®ç§‘å­¦å®¶", "å¾ˆé«˜å…´è®¤è¯†ä½ ï¼Œæå››"),
    ("æˆ‘æœ‰5å¹´çš„æœºå™¨å­¦ä¹ ç»éªŒ", "ç»éªŒä¸°å¯Œï¼"),
    ("æˆ‘æ“…é•¿Pythonå’ŒRè¯­è¨€", "è¿™ä¸¤é—¨è¯­è¨€å¾ˆé€‚åˆæ•°æ®ç§‘å­¦"),
    ("æˆ‘ç›®å‰åœ¨ç ”ç©¶æ·±åº¦å­¦ä¹ ", "æ·±åº¦å­¦ä¹ æ˜¯å¾ˆæœ‰å‰æ™¯çš„é¢†åŸŸ"),
]

for user_input, ai_output in conversations:
    memory.save_context(
        {"input": user_input},
        {"output": ai_output}
    )

# æŸ¥çœ‹æ€»ç»“åçš„è®°å¿†
history = memory.load_memory_variables({})
print("æ€»ç»“åçš„å¯¹è¯å†å²ï¼š")
print(history['history'])
print(f"\nåŸå§‹å­—ç¬¦æ•°ï¼ˆçº¦ï¼‰ï¼š{sum(len(u) + len(a) for u, a in conversations)}")
print(f"æ€»ç»“åå­—ç¬¦æ•°ï¼š{len(history['history'])}")
```

**æ€»ç»“ç¤ºä¾‹ï¼š**

```
åŸå§‹å¯¹è¯ï¼ˆ200+ å­—ç¬¦ï¼‰ï¼š
ç”¨æˆ·ï¼šæˆ‘å«æå››ï¼Œæ˜¯ä¸€åæ•°æ®ç§‘å­¦å®¶
AIï¼šå¾ˆé«˜å…´è®¤è¯†ä½ ï¼Œæå››
ç”¨æˆ·ï¼šæˆ‘æœ‰5å¹´çš„æœºå™¨å­¦ä¹ ç»éªŒ
AIï¼šç»éªŒä¸°å¯Œï¼
...

æ€»ç»“åï¼ˆçº¦100å­—ç¬¦ï¼‰ï¼š
æå››æ˜¯ä¸€ä½æœ‰5å¹´ç»éªŒçš„æ•°æ®ç§‘å­¦å®¶ï¼Œæ“…é•¿Pythonå’ŒRè¯­è¨€ï¼Œ
ç›®å‰ä¸“æ³¨äºæ·±åº¦å­¦ä¹ ç ”ç©¶ã€‚
```

#### ConversationSummaryMemory åŸç†

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant M as Memory
    participant L as LLM

    U->>M: æ–°å¯¹è¯
    M->>M: æ·»åŠ åˆ°ç¼“å­˜
    M->>L: è¯·æ±‚æ€»ç»“å†å²
    L->>M: è¿”å›æ€»ç»“
    M->>M: æ›´æ–°æ€»ç»“
    Note over M: ä¿å­˜æ€»ç»“è€ŒéåŸå§‹å¯¹è¯
```

#### è‡ªå®šä¹‰æ€»ç»“æç¤ºè¯

```python
"""
è‡ªå®šä¹‰æ€»ç»“æç¤ºè¯
"""
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryMemory
from langchain.prompts import PromptTemplate

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# è‡ªå®šä¹‰æ€»ç»“æç¤ºè¯
summary_prompt = PromptTemplate(
    input_variables=["summary", "new_lines"],
    template="""å·²æœ‰æ‘˜è¦ï¼š
{summary}

æ–°å¯¹è¯ï¼š
{new_lines}

è¯·æ›´æ–°æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼ˆäººç‰©ã€äº‹ä»¶ã€æ•°å­—ç­‰ï¼‰ï¼š"""
)

memory = ConversationSummaryMemory(
    llm=llm,
    prompt=summary_prompt
)

# æµ‹è¯•
memory.save_context(
    {"input": "æˆ‘ä»¬å…¬å¸Q1è¥æ”¶1000ä¸‡"},
    {"output": "ä¸é”™çš„æˆç»©"}
)

memory.save_context(
    {"input": "Q2ç›®æ ‡æ˜¯1500ä¸‡"},
    {"output": "å¢é•¿50%ï¼Œæœ‰æŒ‘æˆ˜ä½†å¯è¡Œ"}
)

print("æ€»ç»“ï¼š")
print(memory.load_memory_variables({})['history'])
```

::: tip ConversationSummaryMemory çš„ç‰¹ç‚¹
**ä¼˜ç‚¹ï¼š**
- ä¿ç•™å…³é”®ä¿¡æ¯
- Token æ¶ˆè€—å¯æ§
- æ”¯æŒé•¿æ—¶é—´å¯¹è¯

**ç¼ºç‚¹ï¼š**
- éœ€è¦é¢å¤–çš„ LLM è°ƒç”¨ï¼ˆæˆæœ¬å¢åŠ ï¼‰
- æ€»ç»“å¯èƒ½ä¸¢å¤±ç»†èŠ‚
- ç¨æ…¢ï¼ˆéœ€è¦æ€»ç»“æ—¶é—´ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š**
- éœ€è¦ä¿ç•™æ—©æœŸå…³é”®ä¿¡æ¯
- é•¿æ—¶é—´å®¢æœå¯¹è¯
- ä¿¡æ¯å¯†é›†å‹å¯¹è¯
:::

### 2.4 ConversationSummaryBufferMemory

**ConversationSummaryBufferMemory** ç»“åˆäº†çª—å£å’Œæ€»ç»“çš„ä¼˜ç‚¹ï¼š
- ä¿ç•™æœ€è¿‘çš„åŸå§‹å¯¹è¯
- æ€»ç»“æ›´æ—©çš„å¯¹è¯

```python
"""
ConversationSummaryBufferMemory ç¤ºä¾‹
ç‰¹ç‚¹ï¼šæ··åˆç­–ç•¥ï¼ˆæ€»ç»“ + åŸå§‹ï¼‰
"""
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryBufferMemory

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# åˆ›å»ºæ··åˆè®°å¿†
memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=100,  # æœ€å¤§ token æ•°
    return_messages=False
)

# æ·»åŠ å¤šè½®å¯¹è¯
conversations = [
    ("ç¬¬1è½®ï¼šæˆ‘åœ¨åŒ—äº¬", "å¥½çš„"),
    ("ç¬¬2è½®ï¼šæˆ‘æ˜¯å·¥ç¨‹å¸ˆ", "çŸ¥é“äº†"),
    ("ç¬¬3è½®ï¼šæˆ‘å–œæ¬¢ç¼–ç¨‹", "å¾ˆå¥½"),
    ("ç¬¬4è½®ï¼šæˆ‘ç”¨Python", "Pythonå¾ˆæµè¡Œ"),
    ("ç¬¬5è½®ï¼šæˆ‘åœ¨å­¦AI", "AIå¾ˆæœ‰å‰é€”"),
]

for user_input, ai_output in conversations:
    memory.save_context(
        {"input": user_input},
        {"output": ai_output}
    )
    print(f"\næ·»åŠ : {user_input}")
    print("å½“å‰è®°å¿†ï¼š")
    print(memory.load_memory_variables({})['history'])
    print("-" * 60)
```

**æ··åˆè®°å¿†ç»“æ„ï¼š**

```
[æ€»ç»“éƒ¨åˆ†] + [æœ€è¿‘åŸå§‹å¯¹è¯]

ç¤ºä¾‹ï¼š
æ€»ç»“ï¼šç”¨æˆ·æ˜¯åŒ—äº¬çš„å·¥ç¨‹å¸ˆï¼Œå–œæ¬¢ç¼–ç¨‹ã€‚
åŸå§‹ï¼š
ç”¨æˆ·ï¼šæˆ‘ç”¨Python
AIï¼šPythonå¾ˆæµè¡Œ
ç”¨æˆ·ï¼šæˆ‘åœ¨å­¦AI
AIï¼šAIå¾ˆæœ‰å‰é€”
```

#### å®Œæ•´å¯¹è¯ç¤ºä¾‹

```python
"""
ä½¿ç”¨æ··åˆè®°å¿†çš„å®Œæ•´å¯¹è¯ç³»ç»Ÿ
"""
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationSummaryBufferMemory

llm = ChatOpenAI(model="gpt-3.5-turbo")

conversation = ConversationChain(
    llm=llm,
    memory=ConversationSummaryBufferMemory(
        llm=llm,
        max_token_limit=150
    ),
    verbose=True
)

# é•¿å¯¹è¯æµ‹è¯•
dialogues = [
    "æˆ‘å«ç‹äº”ï¼Œæ¥è‡ªä¸Šæµ·",
    "æˆ‘æ˜¯ä¸€åäº§å“ç»ç†",
    "æˆ‘åœ¨ä¸€å®¶ç§‘æŠ€å…¬å¸å·¥ä½œ",
    "æˆ‘è´Ÿè´£AIäº§å“çº¿",
    "æˆ‘çš„å›¢é˜Ÿæœ‰10ä¸ªäºº",
    "æˆ‘çš„åå­—å’ŒèŒä½æ˜¯ä»€ä¹ˆï¼Ÿ",  # æµ‹è¯•æ—©æœŸä¿¡æ¯
]

for i, user_input in enumerate(dialogues, 1):
    print(f"\n{'='*60}")
    print(f"ç¬¬{i}è½®å¯¹è¯")
    print('='*60)
    response = conversation.predict(input=user_input)
    print(f"ç”¨æˆ·: {user_input}")
    print(f"AI: {response}")
```

---

## ä¸‰ã€é«˜çº§ Memory ç±»å‹

### 3.1 ConversationEntityMemory

**ConversationEntityMemory** ä¸“é—¨æå–å’Œå­˜å‚¨å¯¹è¯ä¸­çš„å®ä½“ä¿¡æ¯ï¼ˆäººç‰©ã€åœ°ç‚¹ã€ç»„ç»‡ç­‰ï¼‰ã€‚

```python
"""
ConversationEntityMemory ç¤ºä¾‹
ç‰¹ç‚¹ï¼šæå–å¹¶è®°ä½å®ä½“ä¿¡æ¯
"""
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationEntityMemory
from langchain.chains import ConversationChain

llm = ChatOpenAI(model="gpt-3.5-turbo")

# åˆ›å»ºå®ä½“è®°å¿†
memory = ConversationEntityMemory(llm=llm)

conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# å¯¹è¯ä¸­åŒ…å«å¤šä¸ªå®ä½“
dialogues = [
    "æˆ‘å«å¼ ä¼Ÿï¼Œåœ¨é˜¿é‡Œå·´å·´å·¥ä½œ",
    "æˆ‘çš„åŒäº‹æå¨œåœ¨å­—èŠ‚è·³åŠ¨",
    "æˆ‘ä»¬éƒ½åœ¨æ­å·",
    "å‘Šè¯‰æˆ‘å…³äºå¼ ä¼Ÿçš„ä¿¡æ¯",
    "æå¨œåœ¨å“ªä¸ªå…¬å¸ï¼Ÿ",
]

for dialogue in dialogues:
    print(f"\nç”¨æˆ·: {dialogue}")
    response = conversation.predict(input=dialogue)
    print(f"AI: {response}")

# æŸ¥çœ‹æå–çš„å®ä½“
print("\n" + "=" * 60)
print("æå–çš„å®ä½“ï¼š")
print(memory.entity_store.store)
```

**å®ä½“è®°å¿†å­˜å‚¨ç»“æ„ï¼š**

```python
{
    "å¼ ä¼Ÿ": "åœ¨é˜¿é‡Œå·´å·´å·¥ä½œ",
    "æå¨œ": "æ˜¯å¼ ä¼Ÿçš„åŒäº‹ï¼Œåœ¨å­—èŠ‚è·³åŠ¨å·¥ä½œ",
    "é˜¿é‡Œå·´å·´": "å¼ ä¼Ÿçš„å…¬å¸",
    "å­—èŠ‚è·³åŠ¨": "æå¨œçš„å…¬å¸",
    "æ­å·": "å¼ ä¼Ÿå’Œæå¨œå·¥ä½œçš„åŸå¸‚"
}
```

### 3.2 VectorStoreRetrieverMemory

**VectorStoreRetrieverMemory** ä½¿ç”¨å‘é‡æ£€ç´¢æ¥æŸ¥æ‰¾ç›¸å…³çš„å†å²å¯¹è¯ã€‚

```python
"""
VectorStoreRetrieverMemory ç¤ºä¾‹
ç‰¹ç‚¹ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢å†å²
"""
# å®‰è£…ï¼špip install chromadb
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.memory import VectorStoreRetrieverMemory
from langchain.vectorstores import Chroma

# åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)

# åˆ›å»ºæ£€ç´¢è®°å¿†
retriever = vectorstore.as_retriever(search_kwargs=dict(k=2))
memory = VectorStoreRetrieverMemory(retriever=retriever)

# æ·»åŠ å†å²å¯¹è¯
memory.save_context(
    {"input": "æˆ‘å–œæ¬¢è¶³çƒ"},
    {"output": "è¶³çƒæ˜¯ä¸€é¡¹å¾ˆæ£’çš„è¿åŠ¨"}
)

memory.save_context(
    {"input": "æˆ‘å…»äº†ä¸€åªçŒ«"},
    {"output": "çŒ«æ˜¯å¾ˆå¯çˆ±çš„å® ç‰©"}
)

memory.save_context(
    {"input": "æˆ‘åœ¨å­¦Python"},
    {"output": "Pythonæ˜¯å¾ˆå¥½çš„ç¼–ç¨‹è¯­è¨€"}
)

# åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢
# æé—®ï¼šå…³äºè¿åŠ¨çš„é—®é¢˜
relevant_history = memory.load_memory_variables(
    {"input": "ä½ çŸ¥é“æˆ‘å–œæ¬¢ä»€ä¹ˆè¿åŠ¨å—ï¼Ÿ"}
)

print("ç›¸å…³å†å²ï¼ˆåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰ï¼š")
print(relevant_history['history'])
```

**å‘é‡è®°å¿†åŸç†ï¼š**

```mermaid
graph TB
    A[æ–°å¯¹è¯] --> B[å‘é‡åŒ–<br/>Embedding]
    B --> C[å­˜å‚¨åˆ°<br/>Vector Store]

    D[æŸ¥è¯¢] --> E[å‘é‡åŒ–æŸ¥è¯¢]
    E --> F[ç›¸ä¼¼åº¦æ£€ç´¢<br/>Top K]
    F --> G[è¿”å›ç›¸å…³å†å²]

    style A fill:#E3F2FD
    style D fill:#FFE082
    style G fill:#C8E6C9
```

---

## å››ã€è‡ªå®šä¹‰ Memory

### 4.1 ç»§æ‰¿ BaseChatMemory

```python
"""
è‡ªå®šä¹‰ Memoryï¼šå…³é”®è¯è®°å¿†
åŠŸèƒ½ï¼šåªè®°ä½åŒ…å«ç‰¹å®šå…³é”®è¯çš„å¯¹è¯
"""
from langchain.memory.chat_memory import BaseChatMemory
from langchain.schema import BaseMessage, HumanMessage, AIMessage
from typing import List, Dict, Any

class KeywordMemory(BaseChatMemory):
    """
    å…³é”®è¯è®°å¿†ï¼šåªä¿å­˜åŒ…å«å…³é”®è¯çš„å¯¹è¯
    """

    keywords: List[str] = []  # å…³é”®è¯åˆ—è¡¨
    chat_memory: List[BaseMessage] = []

    def __init__(self, keywords: List[str], **kwargs):
        super().__init__(**kwargs)
        self.keywords = keywords
        self.chat_memory = []

    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -> None:
        """ä¿å­˜ä¸Šä¸‹æ–‡ï¼ˆåªä¿å­˜åŒ…å«å…³é”®è¯çš„ï¼‰"""
        user_input = inputs.get("input", "")
        ai_output = outputs.get("output", "")

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
        contains_keyword = any(kw in user_input or kw in ai_output for kw in self.keywords)

        if contains_keyword:
            self.chat_memory.append(HumanMessage(content=user_input))
            self.chat_memory.append(AIMessage(content=ai_output))
            print(f"âœ… å·²ä¿å­˜ï¼ˆåŒ…å«å…³é”®è¯ï¼‰: {user_input[:30]}...")
        else:
            print(f"âŒ è·³è¿‡ï¼ˆæ— å…³é”®è¯ï¼‰: {user_input[:30]}...")

    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """åŠ è½½è®°å¿†"""
        if self.return_messages:
            return {"history": self.chat_memory}
        else:
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
            history_str = "\n".join([
                f"{'Human' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}"
                for msg in self.chat_memory
            ])
            return {"history": history_str}

    def clear(self) -> None:
        """æ¸…é™¤è®°å¿†"""
        self.chat_memory = []

# ä½¿ç”¨è‡ªå®šä¹‰è®°å¿†
custom_memory = KeywordMemory(keywords=["Python", "ç¼–ç¨‹", "ä»£ç "])

# æµ‹è¯•
test_conversations = [
    ("ä»Šå¤©å¤©æ°”ä¸é”™", "æ˜¯çš„ï¼Œé˜³å…‰æ˜åªš"),
    ("æˆ‘åœ¨å­¦Pythonç¼–ç¨‹", "Pythonæ˜¯å¾ˆå¥½çš„è¯­è¨€"),  # åŒ…å«å…³é”®è¯
    ("ä½ å–œæ¬¢çœ‹ç”µå½±å—", "æˆ‘å–œæ¬¢ç§‘å¹»ç”µå½±"),
    ("èƒ½å¸®æˆ‘å†™æ®µä»£ç å—", "å½“ç„¶å¯ä»¥"),  # åŒ…å«å…³é”®è¯
]

for user_input, ai_output in test_conversations:
    custom_memory.save_context(
        {"input": user_input},
        {"output": ai_output}
    )

print("\næœ€ç»ˆè®°å¿†ï¼š")
print(custom_memory.load_memory_variables({})['history'])
```

### 4.2 å®æˆ˜ï¼šæ™ºèƒ½æ‘˜è¦è®°å¿†

```python
"""
è‡ªå®šä¹‰ï¼šæ™ºèƒ½æ‘˜è¦è®°å¿†
åŠŸèƒ½ï¼š
1. æœ€è¿‘3è½®ä¿ç•™åŸæ–‡
2. æ›´æ—©çš„å¯¹è¯è‡ªåŠ¨æ€»ç»“
3. æ”¯æŒæ‰‹åŠ¨æ ‡è®°é‡è¦ä¿¡æ¯
"""
from langchain.memory.chat_memory import BaseChatMemory
from langchain.schema import HumanMessage, AIMessage, BaseMessage
from langchain_openai import ChatOpenAI
from typing import List, Dict, Any

class SmartSummaryMemory(BaseChatMemory):
    """æ™ºèƒ½æ‘˜è¦è®°å¿†"""

    llm: ChatOpenAI
    recent_k: int = 3  # ä¿ç•™æœ€è¿‘ K è½®åŸæ–‡
    summary: str = ""  # æ—©æœŸå¯¹è¯çš„æ€»ç»“
    recent_messages: List[BaseMessage] = []
    important_messages: List[str] = []  # æ‰‹åŠ¨æ ‡è®°çš„é‡è¦ä¿¡æ¯

    def __init__(self, llm, recent_k: int = 3, **kwargs):
        super().__init__(**kwargs)
        self.llm = llm
        self.recent_k = recent_k
        self.recent_messages = []
        self.important_messages = []

    def save_context(
        self,
        inputs: Dict[str, Any],
        outputs: Dict[str, Any],
        is_important: bool = False
    ) -> None:
        """
        ä¿å­˜ä¸Šä¸‹æ–‡

        å‚æ•°:
            is_important: æ˜¯å¦æ ‡è®°ä¸ºé‡è¦ä¿¡æ¯
        """
        user_input = inputs.get("input", "")
        ai_output = outputs.get("output", "")

        # å¦‚æœæ ‡è®°ä¸ºé‡è¦ï¼Œå•ç‹¬ä¿å­˜
        if is_important:
            self.important_messages.append(f"ç”¨æˆ·: {user_input}\nAI: {ai_output}")

        # æ·»åŠ åˆ°æœ€è¿‘æ¶ˆæ¯
        self.recent_messages.append(HumanMessage(content=user_input))
        self.recent_messages.append(AIMessage(content=ai_output))

        # å¦‚æœè¶…è¿‡çª—å£å¤§å°ï¼Œæ€»ç»“æ—§å¯¹è¯
        if len(self.recent_messages) > self.recent_k * 2:
            self._summarize_old_messages()

    def _summarize_old_messages(self):
        """æ€»ç»“æ—§æ¶ˆæ¯"""
        # å–å‡ºæœ€æ—§çš„2æ¡æ¶ˆæ¯ï¼ˆ1è½®å¯¹è¯ï¼‰
        old_messages = self.recent_messages[:2]
        self.recent_messages = self.recent_messages[2:]

        # æ„å»ºæ€»ç»“æç¤º
        old_conversation = "\n".join([
            f"{'ç”¨æˆ·' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}"
            for msg in old_messages
        ])

        prompt = f"""å·²æœ‰æ‘˜è¦ï¼š
{self.summary}

æ–°å¯¹è¯ï¼š
{old_conversation}

è¯·æ›´æ–°æ‘˜è¦ï¼ˆä¿ç•™å…³é”®ä¿¡æ¯ï¼‰ï¼š"""

        from langchain.schema import HumanMessage as HM
        new_summary = self.llm.invoke([HM(content=prompt)])
        self.summary = new_summary.content

    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """åŠ è½½è®°å¿†"""
        # æ„å»ºå®Œæ•´è®°å¿†
        memory_parts = []

        # 1. æ€»ç»“éƒ¨åˆ†
        if self.summary:
            memory_parts.append(f"[æ—©æœŸå¯¹è¯æ‘˜è¦]\n{self.summary}")

        # 2. é‡è¦ä¿¡æ¯
        if self.important_messages:
            memory_parts.append(f"\n[é‡è¦ä¿¡æ¯]\n" + "\n".join(self.important_messages))

        # 3. æœ€è¿‘å¯¹è¯
        if self.recent_messages:
            recent_str = "\n".join([
                f"{'ç”¨æˆ·' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}"
                for msg in self.recent_messages
            ])
            memory_parts.append(f"\n[æœ€è¿‘å¯¹è¯]\n{recent_str}")

        return {"history": "\n".join(memory_parts)}

    def mark_important(self, message: str):
        """æ‰‹åŠ¨æ ‡è®°é‡è¦ä¿¡æ¯"""
        self.important_messages.append(message)

    def clear(self):
        """æ¸…é™¤è®°å¿†"""
        self.summary = ""
        self.recent_messages = []
        self.important_messages = []

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    memory = SmartSummaryMemory(llm=llm, recent_k=2)

    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    conversations = [
        ("æˆ‘å«å¼ ä¸‰", "ä½ å¥½å¼ ä¸‰", False),
        ("æˆ‘åœ¨åŒ—äº¬å·¥ä½œ", "çŸ¥é“äº†", True),  # æ ‡è®°ä¸ºé‡è¦
        ("æˆ‘æ˜¯ç¨‹åºå‘˜", "å¾ˆå¥½çš„èŒä¸š", False),
        ("æˆ‘å–œæ¬¢Python", "Pythonå¾ˆæµè¡Œ", False),
        ("æˆ‘åœ¨å­¦AI", "AIå¾ˆæœ‰å‰é€”", False),
        ("æˆ‘çš„åå­—å’Œå·¥ä½œåœ°ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ", "...", False),
    ]

    for user_input, ai_output, is_important in conversations:
        memory.save_context(
            {"input": user_input},
            {"output": ai_output},
            is_important=is_important
        )
        print(f"\næ·»åŠ : {user_input}")

    # æŸ¥çœ‹æœ€ç»ˆè®°å¿†
    print("\n" + "=" * 60)
    print("æœ€ç»ˆè®°å¿†ç»“æ„ï¼š")
    print(memory.load_memory_variables({})['history'])
```

---

## äº”ã€å®æˆ˜é¡¹ç›®

### 5.1 é¡¹ç›®ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿ

```python
"""
é¡¹ç›®ï¼šå¸¦è®°å¿†çš„æ™ºèƒ½å®¢æœç³»ç»Ÿ
åŠŸèƒ½ï¼š
1. è®°ä½ç”¨æˆ·ä¿¡æ¯
2. ä¸Šä¸‹æ–‡è¿è´¯å¯¹è¯
3. æå–å…³é”®ä¿¡æ¯
4. ç”Ÿæˆå¯¹è¯æ‘˜è¦
"""
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationSummaryBufferMemory
from langchain.prompts import PromptTemplate
from datatime import datetime
from typing import Optional

class CustomerServiceBot:
    """æ™ºèƒ½å®¢æœæœºå™¨äºº"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

        # è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
        self.prompt = PromptTemplate(
            input_variables=["history", "input"],
            template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœäººå‘˜ï¼Œç‰¹ç‚¹ï¼š
- å‹å¥½ã€è€å¿ƒã€ä¸“ä¸š
- ä¸»åŠ¨è¯¢é—®å’Œè®°å½•ç”¨æˆ·ä¿¡æ¯ï¼ˆå§“åã€è”ç³»æ–¹å¼ã€é—®é¢˜ï¼‰
- æä¾›æœ‰å¸®åŠ©çš„è§£å†³æ–¹æ¡ˆ

{history}

å½“å‰æ—¶é—´ï¼š{current_time}
ç”¨æˆ·ï¼š{input}
å®¢æœï¼š"""
        )

        # åˆ›å»ºè®°å¿†ï¼ˆæ··åˆç­–ç•¥ï¼‰
        self.memory = ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=200
        )

        # åˆ›å»ºå¯¹è¯é“¾
        self.conversation = ConversationChain(
            llm=self.llm,
            memory=self.memory,
            prompt=self.prompt,
            verbose=False
        )

        # ç”¨æˆ·ä¿¡æ¯å­˜å‚¨
        self.user_info = {}
        self.conversation_started = datetime.now()

    def chat(self, user_input: str) -> str:
        """
        ä¸ç”¨æˆ·å¯¹è¯

        å‚æ•°:
            user_input: ç”¨æˆ·è¾“å…¥

        è¿”å›:
            å®¢æœå›å¤
        """
        # æ·»åŠ å½“å‰æ—¶é—´åˆ° prompt
        response = self.conversation.predict(
            input=user_input,
            current_time=datetime.now().strftime("%Y-%m-%d %H:%M")
        )

        # å°è¯•æå–ç”¨æˆ·ä¿¡æ¯ï¼ˆç®€å•å…³é”®è¯åŒ¹é…ï¼‰
        self._extract_user_info(user_input)

        return response

    def _extract_user_info(self, text: str):
        """æå–ç”¨æˆ·ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # æå–ç”µè¯å·ç 
        import re
        phone_pattern = r'1[3-9]\d{9}'
        phones = re.findall(phone_pattern, text)
        if phones:
            self.user_info['phone'] = phones[0]

        # æå–å§“åï¼ˆç®€å•è§„åˆ™ï¼‰
        if "æˆ‘å«" in text or "æˆ‘æ˜¯" in text:
            parts = text.split("æˆ‘å«" if "æˆ‘å«" in text else "æˆ‘æ˜¯")
            if len(parts) > 1:
                name = parts[1].split()[0].strip("ï¼Œã€‚ã€")
                if len(name) <= 4:
                    self.user_info['name'] = name

    def get_conversation_summary(self) -> str:
        """è·å–å¯¹è¯æ‘˜è¦"""
        duration = (datetime.now() - self.conversation_started).seconds
        minutes = duration // 60

        summary = f"""
å¯¹è¯æ‘˜è¦æŠ¥å‘Š
{'='*60}
å¯¹è¯æ—¶é•¿ï¼š{minutes} åˆ†é’Ÿ
ç”¨æˆ·ä¿¡æ¯ï¼š{self.user_info if self.user_info else 'æœªæ”¶é›†'}

å¯¹è¯å†å²ï¼š
{self.memory.load_memory_variables({})['history']}
{'='*60}
"""
        return summary

    def reset(self):
        """é‡ç½®å¯¹è¯"""
        self.memory.clear()
        self.user_info = {}
        self.conversation_started = datetime.now()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    bot = CustomerServiceBot()

    print("=" * 60)
    print("æ™ºèƒ½å®¢æœç³»ç»Ÿå¯åŠ¨ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼Œ'summary' æŸ¥çœ‹æ‘˜è¦ï¼‰")
    print("=" * 60)

    # æ¨¡æ‹Ÿå¯¹è¯ï¼ˆä¹Ÿå¯ä»¥æ”¹ä¸ºçœŸå®äº¤äº’ï¼‰
    test_dialogues = [
        "ä½ å¥½ï¼Œæˆ‘æƒ³å’¨è¯¢ä¸€ä¸‹ä½ ä»¬çš„äº§å“",
        "æˆ‘å«ææ˜ï¼Œæƒ³äº†è§£ä»·æ ¼",
        "æˆ‘çš„ç”µè¯æ˜¯13812345678",
        "ä½ ä»¬æœ‰ä»€ä¹ˆä¼˜æƒ æ´»åŠ¨å—ï¼Ÿ",
        "å¥½çš„ï¼Œæˆ‘å†è€ƒè™‘ä¸€ä¸‹",
    ]

    for user_input in test_dialogues:
        print(f"\nç”¨æˆ·: {user_input}")
        response = bot.chat(user_input)
        print(f"å®¢æœ: {response}")

    # æŸ¥çœ‹æ‘˜è¦
    print("\n" + bot.get_conversation_summary())
```

### 5.2 é¡¹ç›®ï¼šä¸ªæ€§åŒ–å­¦ä¹ åŠ©æ‰‹

```python
"""
é¡¹ç›®ï¼šä¸ªæ€§åŒ–å­¦ä¹ åŠ©æ‰‹
åŠŸèƒ½ï¼š
1. è®°ä½å­¦ç”Ÿçš„å­¦ä¹ è¿›åº¦
2. æ ¹æ®å†å²è°ƒæ•´æ•™å­¦å†…å®¹
3. è¿½è¸ªå­¦ä¹ ç›®æ ‡
"""
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationEntityMemory
from langchain.chains import ConversationChain
from typing import Dict, List

class LearningAssistant:
    """ä¸ªæ€§åŒ–å­¦ä¹ åŠ©æ‰‹"""

    def __init__(self, student_name: str):
        self.student_name = student_name
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

        # ä½¿ç”¨å®ä½“è®°å¿†ï¼ˆè®°ä½å­¦ä¹ ä¸»é¢˜ã€è¿›åº¦ç­‰ï¼‰
        self.memory = ConversationEntityMemory(llm=self.llm)

        self.conversation = ConversationChain(
            llm=self.llm,
            memory=self.memory,
            verbose=True
        )

        # å­¦ä¹ è¿›åº¦è¿½è¸ª
        self.learning_progress: Dict[str, str] = {}
        self.completed_topics: List[str] = []

    def teach(self, student_input: str) -> str:
        """
        æ•™å­¦å¯¹è¯

        å‚æ•°:
            student_input: å­¦ç”Ÿè¾“å…¥

        è¿”å›:
            åŠ©æ‰‹å›å¤
        """
        # æ·»åŠ ç³»ç»Ÿä¸Šä¸‹æ–‡
        contextualized_input = f"""å­¦ç”Ÿ {self.student_name} è¯´: {student_input}

è¯·ä½œä¸ºä¸€åè€å¿ƒçš„å¯¼å¸ˆå›å¤ï¼Œæ ¹æ®ä¹‹å‰çš„å¯¹è¯è°ƒæ•´æ•™å­¦å†…å®¹ã€‚"""

        response = self.conversation.predict(input=contextualized_input)

        # æ£€æµ‹æ˜¯å¦å®ŒæˆæŸä¸ªä¸»é¢˜
        if "å®Œæˆ" in student_input or "å­¦ä¼š" in student_input:
            self._extract_completed_topic(student_input)

        return response

    def _extract_completed_topic(self, text: str):
        """æå–å·²å®Œæˆçš„ä¸»é¢˜"""
        # ç®€åŒ–çš„ä¸»é¢˜æå–
        topics = ["Python", "Java", "æ•°æ®ç»“æ„", "ç®—æ³•", "æœºå™¨å­¦ä¹ "]
        for topic in topics:
            if topic in text and topic not in self.completed_topics:
                self.completed_topics.append(topic)
                print(f"âœ… å·²å®Œæˆä¸»é¢˜: {topic}")

    def get_progress_report(self) -> str:
        """ç”Ÿæˆå­¦ä¹ æŠ¥å‘Š"""
        # ä½¿ç”¨ LLM ç”ŸæˆæŠ¥å‘Š
        report_prompt = f"""æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆå­¦ä¹ æŠ¥å‘Šï¼š

å­¦ç”Ÿï¼š{self.student_name}
å·²å®Œæˆä¸»é¢˜ï¼š{', '.join(self.completed_topics) if self.completed_topics else 'æ— '}

å¯¹è¯å†å²ï¼š
{self.memory.load_memory_variables({})['history']}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æŠ¥å‘Šï¼š
1. å­¦ä¹ è¿›åº¦æ€»ç»“
2. æŒæ¡çš„çŸ¥è¯†ç‚¹
3. ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®
"""

        from langchain.schema import HumanMessage
        report = self.llm.invoke([HumanMessage(content=report_prompt)])
        return report.content

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    assistant = LearningAssistant(student_name="å°ç‹")

    # æ¨¡æ‹Ÿå­¦ä¹ å¯¹è¯
    dialogues = [
        "æˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹",
        "æˆ‘å·²ç»äº†è§£å˜é‡å’Œæ•°æ®ç±»å‹äº†",
        "èƒ½æ•™æˆ‘å¾ªç¯å—ï¼Ÿ",
        "æˆ‘å­¦ä¼šäº†forå¾ªç¯",
        "ä¸‹ä¸€æ­¥åº”è¯¥å­¦ä»€ä¹ˆï¼Ÿ",
    ]

    for dialogue in dialogues:
        print(f"\nå­¦ç”Ÿ: {dialogue}")
        response = assistant.teach(dialogue)
        print(f"åŠ©æ‰‹: {response}")

    # ç”Ÿæˆå­¦ä¹ æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("å­¦ä¹ æŠ¥å‘Š")
    print("=" * 60)
    print(assistant.get_progress_report())
```

---

## å…­ã€æœ¬å‘¨ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šMemory å¯¹æ¯”ï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰

**ä»»åŠ¡**ï¼šå¯¹æ¯”ä¸åŒ Memory ç±»å‹åœ¨é•¿å¯¹è¯ä¸­çš„è¡¨ç°ã€‚

**è¦æ±‚**ï¼š
1. ä½¿ç”¨ç›¸åŒçš„å¯¹è¯æµ‹è¯• BufferMemoryã€WindowMemoryã€SummaryMemory
2. æ¯”è¾ƒ Token æ¶ˆè€—
3. åˆ†æå„è‡ªä¼˜ç¼ºç‚¹

<details>
<summary>æŸ¥çœ‹æç¤º</summary>

```python
# åˆ›å»ºæµ‹è¯•å‡½æ•°
def test_memory(memory_type, conversations):
    # ç»Ÿè®¡ token ä½¿ç”¨
    # æµ‹è¯•è®°å¿†æ•ˆæœ
    pass
```
</details>

### ç»ƒä¹ 2ï¼šè‡ªå®šä¹‰ Memoryï¼ˆéš¾åº¦ï¼šâ­â­â­ï¼‰

**ä»»åŠ¡**ï¼šå®ç°ä¸€ä¸ª"ä¼˜å…ˆçº§è®°å¿†"ç³»ç»Ÿã€‚

**è¦æ±‚**ï¼š
1. ç”¨æˆ·å¯ä»¥æ ‡è®°é‡è¦å¯¹è¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
2. å†…å­˜æœ‰é™æ—¶ï¼Œä¼˜å…ˆä¿ç•™é«˜ä¼˜å…ˆçº§å¯¹è¯
3. æ”¯æŒæŒ‰ä¼˜å…ˆçº§æ£€ç´¢

<details>
<summary>æŸ¥çœ‹æç¤º</summary>

ç»§æ‰¿ `BaseChatMemory`ï¼Œæ·»åŠ ä¼˜å…ˆçº§å­—æ®µã€‚
</details>

### ç»ƒä¹ 3ï¼šå®æˆ˜é¡¹ç›®ï¼ˆéš¾åº¦ï¼šâ­â­â­ï¼‰

**ä»»åŠ¡**ï¼šæ„å»º"ä¸ªäººæ—¥è®°åŠ©æ‰‹"ã€‚

**è¦æ±‚**ï¼š
1. è®°ä½ç”¨æˆ·çš„æ—¥å¸¸æ´»åŠ¨
2. æå–å…³é”®äº‹ä»¶ï¼ˆå·¥ä½œã€ç”Ÿæ´»ã€æƒ…æ„Ÿï¼‰
3. ç”Ÿæˆæ¯å‘¨æ€»ç»“æŠ¥å‘Š

---

## ä¸ƒã€æœ¬å‘¨æ€»ç»“

### 7.1 çŸ¥è¯†ç‚¹æ¸…å•

- [x] Memory çš„æ¦‚å¿µå’Œé‡è¦æ€§
- [x] ConversationBufferMemoryï¼ˆå®Œæ•´ä¿å­˜ï¼‰
- [x] ConversationBufferWindowMemoryï¼ˆçª—å£ä¿å­˜ï¼‰
- [x] ConversationSummaryMemoryï¼ˆæ€»ç»“ä¿å­˜ï¼‰
- [x] ConversationSummaryBufferMemoryï¼ˆæ··åˆï¼‰
- [x] ConversationEntityMemoryï¼ˆå®ä½“æå–ï¼‰
- [x] VectorStoreRetrieverMemoryï¼ˆå‘é‡æ£€ç´¢ï¼‰
- [x] è‡ªå®šä¹‰ Memory å®ç°

### 7.2 Memory é€‰æ‹©æŒ‡å—

```mermaid
graph TD
    A[é€‰æ‹© Memory] --> B{å¯¹è¯é•¿åº¦?}
    B -->|çŸ­å¯¹è¯<10è½®| C[BufferMemory]
    B -->|ä¸­ç­‰å¯¹è¯| D{æ˜¯å¦éœ€è¦æ—©æœŸä¿¡æ¯?}
    B -->|é•¿å¯¹è¯>50è½®| E[å‘é‡è®°å¿†]

    D -->|ä¸éœ€è¦| F[WindowMemory]
    D -->|éœ€è¦| G{Tokené¢„ç®—?}

    G -->|å……è¶³| H[SummaryMemory]
    G -->|æœ‰é™| I[SummaryBufferMemory]

    style C fill:#C8E6C9
    style F fill:#FFE082
    style H fill:#81C784
    style I fill:#AED581
    style E fill:#CE93D8
```

### 7.3 æœ€ä½³å®è·µ

| åœºæ™¯ | æ¨è Memory | ç†ç”± |
|------|-----------|------|
| å®¢æœèŠå¤© | SummaryBufferMemory | å¹³è¡¡æˆæœ¬å’Œæ•ˆæœ |
| ä»£ç åŠ©æ‰‹ | WindowMemory | åªéœ€æœ€è¿‘ä¸Šä¸‹æ–‡ |
| æ•™è‚²è¾…å¯¼ | EntityMemory | è¿½è¸ªçŸ¥è¯†ç‚¹ |
| é•¿æœŸé™ªä¼´ | VectorMemory | è¯­ä¹‰æ£€ç´¢å†å² |

### 7.4 ä¸‹å‘¨é¢„ä¹ 

**ç¬¬7å‘¨ä¸»é¢˜ï¼šTools å·¥å…·é›†æˆ**

é¢„ä¹ å†…å®¹ï¼š
1. ä»€ä¹ˆæ˜¯ Toolï¼Ÿ
2. å¦‚ä½•é›†æˆå¤–éƒ¨ APIï¼Ÿ
3. è‡ªå®šä¹‰å·¥å…·å¼€å‘

**æ€è€ƒé—®é¢˜**ï¼š
- AI å¦‚ä½•è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Ÿ
- å¦‚ä½•ç¡®ä¿å·¥å…·è°ƒç”¨çš„å®‰å…¨æ€§ï¼Ÿ

---

::: tip å­¦ä¹ å»ºè®®
1. **æ ¹æ®åœºæ™¯é€‰æ‹©**ï¼šæ²¡æœ‰æœ€å¥½çš„ Memoryï¼Œåªæœ‰æœ€åˆé€‚çš„
2. **ç›‘æ§æˆæœ¬**ï¼šæ€»ç»“ç±» Memory ä¼šå¢åŠ  LLM è°ƒç”¨
3. **æµ‹è¯•é•¿å¯¹è¯**ï¼šç”¨å®é™…åœºæ™¯æµ‹è¯• Memory æ•ˆæœ
4. **ç»„åˆä½¿ç”¨**ï¼šå¯ä»¥åŒæ—¶ä½¿ç”¨å¤šç§ Memory ç­–ç•¥
:::

**æœ¬å‘¨å®Œæˆï¼ä¸‹å‘¨å­¦ä¹ å·¥å…·é›†æˆï¼ğŸš€**
