---
title: ç¬¬2å‘¨ - Messages å’Œ Prompts æ·±å…¥
date: 2025-01-22
permalink: /ai/langchain/week2-messages-prompts.html
tags:
  - LangChain
  - Prompt Engineering
  - AI
categories:
  - AI
  - LangChain
---

# ç¬¬2å‘¨ï¼šMessages å’Œ Prompts æ·±å…¥

::: tip æœ¬å‘¨å­¦ä¹ ç›®æ ‡
- ğŸ“ æŒæ¡ Prompt Engineering æ ¸å¿ƒæŠ€å·§
- ğŸ”§ ç†Ÿç»ƒä½¿ç”¨ PromptTemplate å’Œ ChatPromptTemplate
- ğŸ¯ å­¦ä¹  Few-shot Learning å’Œ Chain of Thought
- ğŸš€ èƒ½å¤Ÿè®¾è®¡é«˜è´¨é‡çš„æç¤ºè¯
- ğŸ“Š å®ç°ç»“æ„åŒ–è¾“å‡ºï¼ˆJSONã€Pydanticï¼‰
:::

## ä¸€ã€Prompt Engineering åŸºç¡€

### 1.1 ä»€ä¹ˆæ˜¯ Prompt Engineeringï¼Ÿ

**Prompt Engineering**ï¼ˆæç¤ºè¯å·¥ç¨‹ï¼‰æ˜¯è®¾è®¡å’Œä¼˜åŒ–è¾“å…¥æ–‡æœ¬ä»¥è·å¾—æ›´å¥½ AI è¾“å‡ºçš„æŠ€æœ¯å’Œè‰ºæœ¯ã€‚

#### ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

```mermaid
graph LR
    A[æ™®é€š Prompt] -->|æ¨¡ç³Š| B[è´¨é‡å·®çš„è¾“å‡º]
    C[ä¼˜åŒ–çš„ Prompt] -->|ç²¾ç¡®| D[é«˜è´¨é‡è¾“å‡º]

    style A fill:#FFCDD2
    style B fill:#EF5350
    style C fill:#C8E6C9
    style D fill:#66BB6A
```

**å¯¹æ¯”ç¤ºä¾‹ï¼š**

| å¯¹æ¯”é¡¹ | æ™®é€š Prompt | ä¼˜åŒ–åçš„ Prompt |
|--------|------------|----------------|
| **è¾“å…¥** | "ä»‹ç» Python" | "ä½œä¸ºç¼–ç¨‹æ•™å¸ˆï¼Œç”¨ 3 æ®µè¯å‘é›¶åŸºç¡€å­¦ç”Ÿä»‹ç» Pythonï¼ŒåŒ…æ‹¬ï¼š1)ä»€ä¹ˆæ˜¯ Python 2)ä¸»è¦ç”¨é€” 3)å­¦ä¹ å»ºè®®" |
| **è¾“å‡ºè´¨é‡** | ç¬¼ç»Ÿã€æ— é‡ç‚¹ | ç»“æ„æ¸…æ™°ã€é’ˆå¯¹æ€§å¼º |
| **å¯æ§æ€§** | ä½ | é«˜ |

### 1.2 Prompt è®¾è®¡çš„å…­å¤§åŸåˆ™

#### åŸåˆ™1ï¼šæ¸…æ™°æ˜ç¡®ï¼ˆClarityï¼‰

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

# âŒ ä¸å¥½çš„ Promptï¼ˆæ¨¡ç³Šï¼‰
bad_prompt = "è®²è®²æœºå™¨å­¦ä¹ "

# âœ… å¥½çš„ Promptï¼ˆæ¸…æ™°ï¼‰
good_prompt = """è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šæœºå™¨å­¦ä¹ ï¼Œè¦æ±‚ï¼š
1. å®šä¹‰æœºå™¨å­¦ä¹ ï¼ˆ1å¥è¯ï¼‰
2. ä¸¾ä¸€ä¸ªæ—¥å¸¸ç”Ÿæ´»ä¸­çš„ä¾‹å­
3. è¯´æ˜å®ƒä¸ä¼ ç»Ÿç¼–ç¨‹çš„åŒºåˆ«"""

response = llm.invoke([HumanMessage(content=good_prompt)])
print(response.content)
```

#### åŸåˆ™2ï¼šæä¾›ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰

```python
# âŒ ç¼ºä¹ä¸Šä¸‹æ–‡
bad_prompt = "è¿™ä¸ªæ–¹æ¡ˆå¯è¡Œå—ï¼Ÿ"

# âœ… æä¾›å®Œæ•´ä¸Šä¸‹æ–‡
good_prompt = """èƒŒæ™¯ï¼šæˆ‘ä»¬å…¬å¸æ˜¯ä¸€å®¶ 50 äººçš„ç”µå•†åˆåˆ›ä¼ä¸š
ç›®æ ‡ï¼šæé«˜ç”¨æˆ·ç•™å­˜ç‡
æ–¹æ¡ˆï¼šå¼•å…¥ä¼šå‘˜ç§¯åˆ†ç³»ç»Ÿï¼Œç”¨æˆ·æ¶ˆè´¹å¯è·å¾—ç§¯åˆ†ï¼Œç§¯åˆ†å¯å…‘æ¢ä¼˜æƒ åˆ¸

è¯·ä»æŠ€æœ¯å¯è¡Œæ€§ã€æˆæœ¬ã€é¢„æœŸæ•ˆæœä¸‰ä¸ªç»´åº¦è¯„ä¼°è¿™ä¸ªæ–¹æ¡ˆã€‚"""
```

#### åŸåˆ™3ï¼šæŒ‡å®šæ ¼å¼ï¼ˆFormatï¼‰

```python
# âŒ ä¸æŒ‡å®šæ ¼å¼
bad_prompt = "åˆ—å‡ºå­¦ä¹  Python çš„æ­¥éª¤"

# âœ… æŒ‡å®šè¾“å‡ºæ ¼å¼
good_prompt = """åˆ—å‡ºå­¦ä¹  Python çš„æ­¥éª¤ï¼Œä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

æ­¥éª¤ç¼–å· | æ­¥éª¤åç§° | æ—¶é—´æŠ•å…¥ | å­¦ä¹ èµ„æº
--------|---------|---------|----------
1       | ...     | ...     | ..."""
```

#### åŸåˆ™4ï¼šè®¾å®šè§’è‰²ï¼ˆRoleï¼‰

```python
from langchain.schema import SystemMessage, HumanMessage

# âœ… ä½¿ç”¨ SystemMessage è®¾å®šè§’è‰²
messages = [
    SystemMessage(content="""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ Python å¯¼å¸ˆï¼Œæ‹¥æœ‰ 10 å¹´æ•™å­¦ç»éªŒã€‚
ä½ çš„æ•™å­¦ç‰¹ç‚¹æ˜¯ï¼š
- å–„äºç”¨ç±»æ¯”å’Œä¾‹å­è§£é‡Šå¤æ‚æ¦‚å¿µ
- æ³¨é‡å®è·µï¼Œæä¾›å¯è¿è¡Œçš„ä»£ç ç¤ºä¾‹
- é¼“åŠ±å­¦ç”Ÿæ€è€ƒå’Œæé—®"""),

    HumanMessage(content="ä»€ä¹ˆæ˜¯ Python è£…é¥°å™¨ï¼Ÿ")
]

response = llm.invoke(messages)
print(response.content)
```

#### åŸåˆ™5ï¼šåˆ†æ­¥å¼•å¯¼ï¼ˆStep-by-stepï¼‰

```python
# âœ… åˆ†æ­¥éª¤å¼•å¯¼ AI æ€è€ƒ
prompt = """è¯·å¸®æˆ‘åˆ†æè¿™æ®µä»£ç çš„æ€§èƒ½é—®é¢˜ï¼ŒæŒ‰ä»¥ä¸‹æ­¥éª¤ï¼š

ä»£ç ï¼š
```python
result = []
for i in range(10000):
    result.append(i ** 2)
æ­¥éª¤1ï¼šè¯†åˆ«ä»£ç çš„ä¸»è¦æ“ä½œ
æ­¥éª¤2ï¼šåˆ†ææ—¶é—´å¤æ‚åº¦
æ­¥éª¤3ï¼šæŒ‡å‡ºæ½œåœ¨çš„æ€§èƒ½ç“¶é¢ˆ
æ­¥éª¤4ï¼šæä¾›ä¼˜åŒ–å»ºè®®ï¼ˆé™„ä»£ç ï¼‰"""
```

#### åŸåˆ™6ï¼šæä¾›ç¤ºä¾‹ï¼ˆExamplesï¼‰

```python
# âœ… Few-shot Learningï¼šæä¾›ç¤ºä¾‹å¼•å¯¼è¾“å‡ºæ ¼å¼
prompt = """å°†ä»¥ä¸‹å¥å­æ”¹å†™ä¸ºæ›´æ­£å¼çš„è¡¨è¾¾æ–¹å¼ã€‚

ç¤ºä¾‹1:
è¾“å…¥ï¼šè¿™ä¸ªæ–¹æ³•æŒºå¥½ç”¨çš„
è¾“å‡ºï¼šè¯¥æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„å®ç”¨æ€§

ç¤ºä¾‹2:
è¾“å…¥ï¼šä»£ç è·‘ä¸èµ·æ¥
è¾“å‡ºï¼šä»£ç æ— æ³•æ­£å¸¸æ‰§è¡Œ

ç°åœ¨è¯·æ”¹å†™ï¼š
è¾“å…¥ï¼šè¿™bugå¤ªéš¾æäº†"""
```

### 1.3 Prompt è®¾è®¡æ¨¡å¼

#### æ¨¡å¼1ï¼šè§’è‰²æ‰®æ¼”ï¼ˆRole Playingï¼‰

```python
"""
è§’è‰²æ‰®æ¼”æ¨¡å¼ï¼šè®© AI æ‰®æ¼”ç‰¹å®šä¸“å®¶
é€‚ç”¨åœºæ™¯ï¼šéœ€è¦ç‰¹å®šé¢†åŸŸçŸ¥è¯†æˆ–ç‰¹å®šé£æ ¼çš„å›ç­”
"""
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

llm = ChatOpenAI(model="gpt-3.5-turbo")

# è§’è‰²å®šä¹‰
roles = {
    "æ¶æ„å¸ˆ": """ä½ æ˜¯ä¸€ä½èµ„æ·±è½¯ä»¶æ¶æ„å¸ˆï¼Œæ“…é•¿ï¼š
- ç³»ç»Ÿè®¾è®¡å’ŒæŠ€æœ¯é€‰å‹
- åˆ†ææ€§èƒ½ç“¶é¢ˆ
- æƒè¡¡æŠ€æœ¯æ–¹æ¡ˆçš„åˆ©å¼Š""",

    "ä»£ç å®¡æŸ¥å‘˜": """ä½ æ˜¯ä¸€ä½ä¸¥æ ¼çš„ä»£ç å®¡æŸ¥å‘˜ï¼Œå…³æ³¨ï¼š
- ä»£ç å¯è¯»æ€§å’Œç»´æŠ¤æ€§
- æ½œåœ¨çš„ bug å’Œå®‰å…¨é—®é¢˜
- æœ€ä½³å®è·µå’Œè®¾è®¡æ¨¡å¼"""
}

# ä½¿ç”¨æ¶æ„å¸ˆè§’è‰²
messages = [
    SystemMessage(content=roles["æ¶æ„å¸ˆ"]),
    HumanMessage(content="å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜å¹¶å‘çš„ç§’æ€ç³»ç»Ÿï¼Ÿ")
]

response = llm.invoke(messages)
print(response.content)
```

#### æ¨¡å¼2ï¼šä»»åŠ¡åˆ†è§£ï¼ˆTask Decompositionï¼‰

```python
"""
ä»»åŠ¡åˆ†è§£æ¨¡å¼ï¼šå°†å¤æ‚ä»»åŠ¡æ‹†åˆ†ä¸ºå¤šä¸ªå­ä»»åŠ¡
é€‚ç”¨åœºæ™¯ï¼šéœ€è¦å¤„ç†å¤æ‚é—®é¢˜æˆ–å¤šæ­¥éª¤ä»»åŠ¡
"""
prompt = """è¯·å¸®æˆ‘è®¾è®¡ä¸€ä¸ªåœ¨çº¿æ•™è‚²å¹³å°ï¼Œåˆ†ä»¥ä¸‹é˜¶æ®µè¿›è¡Œï¼š

é˜¶æ®µ1 - éœ€æ±‚åˆ†æï¼š
- æ ¸å¿ƒåŠŸèƒ½åˆ—è¡¨
- ç”¨æˆ·è§’è‰²ï¼ˆå­¦ç”Ÿã€æ•™å¸ˆã€ç®¡ç†å‘˜ï¼‰

é˜¶æ®µ2 - æŠ€æœ¯é€‰å‹ï¼š
- å‰ç«¯æ¡†æ¶æ¨è
- åç«¯æ¡†æ¶æ¨è
- æ•°æ®åº“é€‰æ‹©

é˜¶æ®µ3 - ç³»ç»Ÿæ¶æ„ï¼š
- ç»˜åˆ¶ç³»ç»Ÿæ¶æ„å›¾ï¼ˆæ–‡å­—æè¿°ï¼‰
- è¯´æ˜å„æ¨¡å—èŒè´£

è¯·ä¸€æ­¥æ­¥å®Œæˆæ¯ä¸ªé˜¶æ®µã€‚"""
```

#### æ¨¡å¼3ï¼šå¯¹æ¯”åˆ†æï¼ˆComparisonï¼‰

```python
"""
å¯¹æ¯”åˆ†ææ¨¡å¼ï¼šè®© AI æ¯”è¾ƒå¤šä¸ªæ–¹æ¡ˆ
é€‚ç”¨åœºæ™¯ï¼šæŠ€æœ¯é€‰å‹ã€æ–¹æ¡ˆè¯„ä¼°
"""
prompt = """è¯·å¯¹æ¯” MySQL å’Œ PostgreSQLï¼Œä½¿ç”¨è¡¨æ ¼å½¢å¼ï¼š

å¯¹æ¯”ç»´åº¦ | MySQL | PostgreSQL | å»ºè®®åœºæ™¯
--------|-------|------------|----------
æ€§èƒ½    | ...   | ...        | ...
æ‰©å±•æ€§  | ...   | ...        | ...
åŠŸèƒ½å®Œæ•´æ€§ | ... | ...        | ...
ç¤¾åŒºæ”¯æŒ | ...  | ...        | ...
å­¦ä¹ æ›²çº¿ | ...  | ...        | ...

æœ€åç»™å‡ºé€‰å‹å»ºè®®ã€‚"""
```

#### æ¨¡å¼4ï¼šé”™è¯¯è¯Šæ–­ï¼ˆDebuggingï¼‰

```python
"""
é”™è¯¯è¯Šæ–­æ¨¡å¼ï¼šå¸®åŠ©åˆ†æå’Œä¿®å¤é—®é¢˜
é€‚ç”¨åœºæ™¯ï¼šè°ƒè¯•ä»£ç ã€æ’æŸ¥é”™è¯¯
"""
prompt = """æˆ‘çš„ä»£ç å‡ºç°äº†é”™è¯¯ï¼Œè¯·å¸®æˆ‘è¯Šæ–­å’Œä¿®å¤ã€‚

ä»£ç ï¼š
```python
def calculate_average(numbers):
    return sum(numbers) / len(numbers)

result = calculate_average([])
print(result)

é”™è¯¯ä¿¡æ¯ï¼š
ZeroDivisionError: division by zero

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
1. **é”™è¯¯åŸå› **ï¼š...
2. **ä¿®å¤æ–¹æ¡ˆ**ï¼š...ï¼ˆé™„ä¿®å¤åçš„ä»£ç ï¼‰
3. **é¢„é˜²æªæ–½**ï¼š...ï¼ˆå¦‚ä½•é¿å…ç±»ä¼¼é”™è¯¯ï¼‰"""
```

## äºŒã€LangChain Prompt Templates

### 2.1 PromptTemplate åŸºç¡€

#### 2.1.1 ä»€ä¹ˆæ˜¯ PromptTemplateï¼Ÿ

**PromptTemplate** æ˜¯ LangChain æä¾›çš„æ¨¡æ¿ç³»ç»Ÿï¼Œç”¨äºåˆ›å»ºå¯å¤ç”¨ã€å¯å‚æ•°åŒ–çš„æç¤ºè¯ã€‚

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**
- âœ… **å¯å¤ç”¨**ï¼šä¸€æ¬¡å®šä¹‰ï¼Œå¤šæ¬¡ä½¿ç”¨
- âœ… **å‚æ•°åŒ–**ï¼šåŠ¨æ€å¡«å……å˜é‡
- âœ… **å¯ç»´æŠ¤**ï¼šé›†ä¸­ç®¡ç†æç¤ºè¯
- âœ… **å¯æµ‹è¯•**ï¼šæ–¹ä¾¿è¿›è¡Œ A/B æµ‹è¯•

#### 2.1.2 åŸºæœ¬ç”¨æ³•

```python
"""
PromptTemplate åŸºç¡€ç¤ºä¾‹
"""
from langchain.prompts import PromptTemplate

# æ–¹æ³•1ï¼šä½¿ç”¨ from_templateï¼ˆæ¨èï¼‰
template = "ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œè¯·ç”¨{language}å›ç­”ï¼š{question}"

prompt = PromptTemplate.from_template(template)

# æ ¼å¼åŒ–è¾“å‡º
formatted = prompt.format(
    role="Python ä¸“å®¶",
    language="ç®€ä½“ä¸­æ–‡",
    question="ä»€ä¹ˆæ˜¯åˆ—è¡¨æ¨å¯¼å¼ï¼Ÿ"
)

print(formatted)
# è¾“å‡ºï¼šä½ æ˜¯ä¸€ä¸ªPython ä¸“å®¶ï¼Œè¯·ç”¨ç®€ä½“ä¸­æ–‡å›ç­”ï¼šä»€ä¹ˆæ˜¯åˆ—è¡¨æ¨å¯¼å¼ï¼Ÿ
```

```python
# æ–¹æ³•2ï¼šæ˜¾å¼æŒ‡å®šå˜é‡ï¼ˆæ›´ä¸¥æ ¼ï¼‰
prompt = PromptTemplate(
    input_variables=["role", "language", "question"],  # æ˜ç¡®å˜é‡åˆ—è¡¨
    template="ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œè¯·ç”¨{language}å›ç­”ï¼š{question}"
)

# å¦‚æœç¼ºå°‘å˜é‡ï¼Œä¼šæŠ¥é”™
try:
    prompt.format(role="ä¸“å®¶")  # ç¼ºå°‘ language å’Œ question
except KeyError as e:
    print(f"é”™è¯¯ï¼šç¼ºå°‘å˜é‡ {e}")
```

#### 2.1.3 é«˜çº§ç‰¹æ€§

**ç‰¹æ€§1ï¼šéƒ¨åˆ†å˜é‡ï¼ˆPartial Variablesï¼‰**

```python
"""
éƒ¨åˆ†å˜é‡ï¼šé¢„å…ˆå¡«å……éƒ¨åˆ†å˜é‡
é€‚ç”¨åœºæ™¯ï¼šæŸäº›å˜é‡å›ºå®šä¸å˜ï¼Œå¦‚æ—¥æœŸã€ç³»ç»Ÿä¿¡æ¯
"""
from datetime import datetime

# å®šä¹‰æ¨¡æ¿
template = """å½“å‰æ—¥æœŸï¼š{date}
ç”¨æˆ·é—®é¢˜ï¼š{question}
è¯·æä¾›è¯¦ç»†å›ç­”ã€‚"""

prompt = PromptTemplate(
    input_variables=["question"],
    template=template,
    partial_variables={
        "date": lambda: datetime.now().strftime("%Y-%m-%d")  # åŠ¨æ€è·å–æ—¥æœŸ
    }
)

# ä½¿ç”¨æ—¶åªéœ€æä¾› question
formatted = prompt.format(question="ä»Šå¤©æ˜¯å‡ å·ï¼Ÿ")
print(formatted)
# è¾“å‡ºï¼š
# å½“å‰æ—¥æœŸï¼š2025-01-22
# ç”¨æˆ·é—®é¢˜ï¼šä»Šå¤©æ˜¯å‡ å·ï¼Ÿ
# è¯·æä¾›è¯¦ç»†å›ç­”ã€‚
```

**ç‰¹æ€§2ï¼šæ¨¡æ¿éªŒè¯**

```python
"""
æ¨¡æ¿éªŒè¯ï¼šç¡®ä¿æ¨¡æ¿å®‰å…¨å’Œæœ‰æ•ˆ
"""
from langchain.prompts import PromptTemplate

# âœ… éªŒè¯å˜é‡åæ˜¯å¦åˆæ³•
try:
    prompt = PromptTemplate(
        input_variables=["user-name"],  # å˜é‡ååŒ…å« '-'ï¼Œä¸åˆæ³•
        template="Hello {user-name}"
    )
except ValueError as e:
    print(f"éªŒè¯é”™è¯¯ï¼š{e}")

# âœ… æ­£ç¡®å†™æ³•
prompt = PromptTemplate(
    input_variables=["user_name"],  # ä½¿ç”¨ä¸‹åˆ’çº¿
    template="Hello {user_name}"
)
```

**ç‰¹æ€§3ï¼šæ¨¡æ¿ç»„åˆ**

```python
"""
æ¨¡æ¿ç»„åˆï¼šå°†å¤šä¸ªæ¨¡æ¿ç»„åˆæˆå¤æ‚æç¤ºè¯
"""
from langchain.prompts import PromptTemplate

# å®šä¹‰å­æ¨¡æ¿
system_template = PromptTemplate.from_template(
    "ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œä½ çš„ä»»åŠ¡æ˜¯{task}ã€‚"
)

context_template = PromptTemplate.from_template(
    "èƒŒæ™¯ä¿¡æ¯ï¼š{context}"
)

question_template = PromptTemplate.from_template(
    "ç”¨æˆ·é—®é¢˜ï¼š{question}"
)

# ç»„åˆæ¨¡æ¿
combined_prompt = (
    system_template.format(role="æ•°æ®åˆ†æå¸ˆ", task="åˆ†ææ•°æ®å¹¶ç»™å‡ºå»ºè®®") + "\n\n" +
    context_template.format(context="å…¬å¸Q1é”€å”®é¢ä¸‹é™15%") + "\n\n" +
    question_template.format(question="å¦‚ä½•æé«˜é”€å”®é¢ï¼Ÿ")
)

print(combined_prompt)
```

### 2.2 ChatPromptTemplate

#### 2.2.1 èŠå¤©æ¨¡å‹ä¸“ç”¨æ¨¡æ¿

**ChatPromptTemplate** æ˜¯ä¸ºèŠå¤©æ¨¡å‹ï¼ˆå¦‚ ChatGPTï¼‰è®¾è®¡çš„æ¨¡æ¿ï¼Œæ”¯æŒå¤šè§’è‰²æ¶ˆæ¯ã€‚

```python
"""
ChatPromptTemplate åŸºç¡€ç”¨æ³•
"""
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# åˆ›å»ºèŠå¤©æ¨¡æ¿
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œæ“…é•¿{skill}ã€‚"),
    ("human", "{user_input}")
])

# æ ¼å¼åŒ–
messages = chat_prompt.format_messages(
    role="Python å¯¼å¸ˆ",
    skill="ç”¨ç®€å•ä¾‹å­è§£é‡Šå¤æ‚æ¦‚å¿µ",
    user_input="ä»€ä¹ˆæ˜¯é—­åŒ…ï¼Ÿ"
)

# è°ƒç”¨æ¨¡å‹
llm = ChatOpenAI(model="gpt-3.5-turbo")
response = llm.invoke(messages)
print(response.content)
```

#### 2.2.2 æ¶ˆæ¯æ¨¡æ¿ç±»å‹

LangChain æ”¯æŒå¤šç§æ¶ˆæ¯æ¨¡æ¿ï¼š

```python
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    AIMessagePromptTemplate
)

# æ–¹æ³•1ï¼šä½¿ç”¨å…ƒç»„ï¼ˆç®€æ´ï¼‰
template1 = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯{role}"),
    ("human", "{input}"),
    ("ai", "æˆ‘ç†è§£ä½ çš„é—®é¢˜æ˜¯å…³äº{topic}"),
    ("human", "{followup}")
])

# æ–¹æ³•2ï¼šä½¿ç”¨æ¶ˆæ¯æ¨¡æ¿ç±»ï¼ˆçµæ´»ï¼‰
template2 = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template("ä½ æ˜¯{role}"),
    HumanMessagePromptTemplate.from_template("{input}"),
    AIMessagePromptTemplate.from_template("æˆ‘ç†è§£ä½ çš„é—®é¢˜æ˜¯å…³äº{topic}"),
    HumanMessagePromptTemplate.from_template("{followup}")
])

# ä¸¤ç§æ–¹æ³•æ•ˆæœç›¸åŒ
```

#### 2.2.3 å®æˆ˜æ¡ˆä¾‹ï¼šæ™ºèƒ½å®¢æœ

```python
"""
å®æˆ˜æ¡ˆä¾‹ï¼šæ„å»ºæ™ºèƒ½å®¢æœç³»ç»Ÿ
åŠŸèƒ½ï¼šæ ¹æ®ç”¨æˆ·é—®é¢˜ç±»å‹ï¼Œä½¿ç”¨ä¸åŒçš„å›å¤æ¨¡æ¿
"""
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from enum import Enum

class QuestionType(Enum):
    """é—®é¢˜ç±»å‹æšä¸¾"""
    TECHNICAL = "technical"  # æŠ€æœ¯é—®é¢˜
    SALES = "sales"          # é”€å”®å’¨è¯¢
    COMPLAINT = "complaint"  # æŠ•è¯‰å»ºè®®

# å®šä¹‰ä¸åŒç±»å‹çš„æ¨¡æ¿
templates = {
    QuestionType.TECHNICAL: ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ”¯æŒä¸“å®¶ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹è´¨ï¼š
- è€å¿ƒç»†è‡´ï¼Œé€æ­¥å¼•å¯¼ç”¨æˆ·è§£å†³é—®é¢˜
- æä¾›å…·ä½“çš„æ“ä½œæ­¥éª¤
- å¿…è¦æ—¶è¦æ±‚ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯"""),
        ("human", "ç”¨æˆ·é—®é¢˜ï¼š{question}\näº§å“ï¼š{product}")
    ]),

    QuestionType.SALES: ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é”€å”®é¡¾é—®ï¼Œå·¥ä½œæ–¹å¼ï¼š
- äº†è§£å®¢æˆ·éœ€æ±‚
- æ¨èåˆé€‚çš„äº§å“æˆ–æ–¹æ¡ˆ
- å¼ºè°ƒäº§å“ä¼˜åŠ¿å’Œä»·å€¼"""),
        ("human", "å®¢æˆ·å’¨è¯¢ï¼š{question}\né¢„ç®—èŒƒå›´ï¼š{budget}")
    ]),

    QuestionType.COMPLAINT: ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯å®¢æˆ·å…³ç³»ä¸“å®¶ï¼Œå¤„ç†åŸåˆ™ï¼š
- é¦–å…ˆè¡¨è¾¾æ­‰æ„å’Œç†è§£
- æ‰¿è¯ºä¼šè®¤çœŸå¯¹å¾…é—®é¢˜
- æä¾›è§£å†³æ–¹æ¡ˆæˆ–åç»­è·Ÿè¿›è®¡åˆ’"""),
        ("human", "å®¢æˆ·åé¦ˆï¼š{question}\nè®¢å•å·ï¼š{order_id}")
    ])
}

def handle_customer_query(
    question: str,
    question_type: QuestionType,
    **kwargs
):
    """å¤„ç†å®¢æˆ·æŸ¥è¯¢"""
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

    # é€‰æ‹©æ¨¡æ¿
    template = templates[question_type]

    # æ ¼å¼åŒ–æ¶ˆæ¯
    messages = template.format_messages(question=question, **kwargs)

    # è°ƒç”¨æ¨¡å‹
    response = llm.invoke(messages)
    return response.content

# æµ‹è¯•ä¸åŒåœºæ™¯
print("=== æŠ€æœ¯æ”¯æŒåœºæ™¯ ===")
tech_response = handle_customer_query(
    question="è½¯ä»¶æ— æ³•å¯åŠ¨ï¼Œæ˜¾ç¤º Error 404",
    question_type=QuestionType.TECHNICAL,
    product="æ•°æ®åˆ†æè½¯ä»¶ v2.0"
)
print(tech_response)

print("\n=== é”€å”®å’¨è¯¢åœºæ™¯ ===")
sales_response = handle_customer_query(
    question="æˆ‘éœ€è¦ä¸€ä¸ªé€‚åˆå°å›¢é˜Ÿçš„é¡¹ç›®ç®¡ç†å·¥å…·",
    question_type=QuestionType.SALES,
    budget="æ¯æœˆ500å…ƒä»¥å†…"
)
print(sales_response)

print("\n=== æŠ•è¯‰å¤„ç†åœºæ™¯ ===")
complaint_response = handle_customer_query(
    question="è®¢å•å»¶è¿Ÿäº†3å¤©æ‰å‘è´§ï¼Œéå¸¸ä¸æ»¡æ„",
    question_type=QuestionType.COMPLAINT,
    order_id="ORD20250122001"
)
print(complaint_response)
```

### 2.3 Few-shot Prompt Templates

#### 2.3.1 ä»€ä¹ˆæ˜¯ Few-shot Learningï¼Ÿ

**Few-shot Learning**ï¼ˆå°‘æ ·æœ¬å­¦ä¹ ï¼‰æ˜¯é€šè¿‡æä¾›å°‘é‡ç¤ºä¾‹æ¥å¼•å¯¼ AI ç†è§£ä»»åŠ¡æ¨¡å¼çš„æŠ€æœ¯ã€‚

```mermaid
graph LR
    A[Few-shot Learning] --> B[æä¾›ç¤ºä¾‹]
    B --> C[AI å­¦ä¹ æ¨¡å¼]
    C --> D[åº”ç”¨åˆ°æ–°è¾“å…¥]

    style A fill:#E1F5FE
    style D fill:#81C784
```

**åº”ç”¨åœºæ™¯ï¼š**
- æ ¼å¼åŒ–è¾“å‡ºï¼ˆJSONã€è¡¨æ ¼ï¼‰
- ç‰¹å®šé£æ ¼çš„æ–‡æœ¬ç”Ÿæˆ
- åˆ†ç±»ä»»åŠ¡
- ä¿¡æ¯æå–

#### 2.3.2 FewShotPromptTemplate ä½¿ç”¨

```python
"""
Few-shot æ¨¡æ¿ç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†ç±»
"""
from langchain.prompts import FewShotPromptTemplate, PromptTemplate

# 1. å®šä¹‰ç¤ºä¾‹
examples = [
    {
        "input": "è¿™éƒ¨ç”µå½±å¤ªç²¾å½©äº†ï¼Œå¼ºçƒˆæ¨èï¼",
        "output": "æ­£é¢"
    },
    {
        "input": "æµªè´¹æ—¶é—´ï¼Œå®Œå…¨ä¸å€¼å¾—çœ‹ã€‚",
        "output": "è´Ÿé¢"
    },
    {
        "input": "è¿˜å¯ä»¥ï¼Œä¸ç®—ç‰¹åˆ«å¥½ä¹Ÿä¸ç®—å·®ã€‚",
        "output": "ä¸­æ€§"
    }
]

# 2. å®šä¹‰ç¤ºä¾‹æ ¼å¼æ¨¡æ¿
example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="è¾“å…¥ï¼š{input}\nè¾“å‡ºï¼š{output}"
)

# 3. åˆ›å»º Few-shot æ¨¡æ¿
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,                  # ç¤ºä¾‹åˆ—è¡¨
    example_prompt=example_prompt,      # ç¤ºä¾‹æ ¼å¼
    prefix="è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰ï¼š\n",  # å‰ç¼€
    suffix="\nè¾“å…¥ï¼š{input}\nè¾“å‡ºï¼š",    # åç¼€
    input_variables=["input"]           # è¾“å…¥å˜é‡
)

# 4. ä½¿ç”¨æ¨¡æ¿
formatted = few_shot_prompt.format(input="è¿™å®¶é¤å…çš„æœåŠ¡å¾ˆå¥½ï¼Œä½†èœå“ä¸€èˆ¬ã€‚")
print(formatted)

# 5. è°ƒç”¨ LLM
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
response = llm.invoke([HumanMessage(content=formatted)])
print(f"\nAI åˆ†ç±»ç»“æœï¼š{response.content}")
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰ï¼š

è¾“å…¥ï¼šè¿™éƒ¨ç”µå½±å¤ªç²¾å½©äº†ï¼Œå¼ºçƒˆæ¨èï¼
è¾“å‡ºï¼šæ­£é¢

è¾“å…¥ï¼šæµªè´¹æ—¶é—´ï¼Œå®Œå…¨ä¸å€¼å¾—çœ‹ã€‚
è¾“å‡ºï¼šè´Ÿé¢

è¾“å…¥ï¼šè¿˜å¯ä»¥ï¼Œä¸ç®—ç‰¹åˆ«å¥½ä¹Ÿä¸ç®—å·®ã€‚
è¾“å‡ºï¼šä¸­æ€§

è¾“å…¥ï¼šè¿™å®¶é¤å…çš„æœåŠ¡å¾ˆå¥½ï¼Œä½†èœå“ä¸€èˆ¬ã€‚
è¾“å‡ºï¼š

AI åˆ†ç±»ç»“æœï¼šä¸­æ€§
```

#### 2.3.3 åŠ¨æ€é€‰æ‹©ç¤ºä¾‹

```python
"""
åŠ¨æ€ç¤ºä¾‹é€‰æ‹©ï¼šæ ¹æ®è¾“å…¥ç›¸ä¼¼åº¦é€‰æ‹©æœ€ç›¸å…³çš„ç¤ºä¾‹
é€‚ç”¨åœºæ™¯ï¼šç¤ºä¾‹åº“å¾ˆå¤§ï¼Œä½†åªæƒ³å±•ç¤ºæœ€ç›¸å…³çš„å‡ ä¸ª
"""
from langchain.prompts.example_selector import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# 1. å‡†å¤‡å¤§é‡ç¤ºä¾‹
examples = [
    {"input": "Python å¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ", "output": "ä½¿ç”¨ open() å‡½æ•°"},
    {"input": "Python å¦‚ä½•å†™å…¥æ–‡ä»¶ï¼Ÿ", "output": "ä½¿ç”¨ open() å‡½æ•°é…åˆ write() æ–¹æ³•"},
    {"input": "å¦‚ä½•åœ¨ Python ä¸­åˆ›å»ºåˆ—è¡¨ï¼Ÿ", "output": "ä½¿ç”¨æ–¹æ‹¬å· [] æˆ– list()"},
    {"input": "Python å­—å…¸å¦‚ä½•æ·»åŠ å…ƒç´ ï¼Ÿ", "output": "ä½¿ç”¨ dict[key] = value"},
    {"input": "å¦‚ä½•éå† Python åˆ—è¡¨ï¼Ÿ", "output": "ä½¿ç”¨ for å¾ªç¯"},
    # ... æ›´å¤šç¤ºä¾‹
]

# 2. åˆ›å»ºè¯­ä¹‰ç›¸ä¼¼åº¦é€‰æ‹©å™¨
example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,                           # ç¤ºä¾‹åˆ—è¡¨
    OpenAIEmbeddings(),                # åµŒå…¥æ¨¡å‹
    Chroma,                            # å‘é‡å­˜å‚¨
    k=2                                # é€‰æ‹©æœ€ç›¸å…³çš„ 2 ä¸ªç¤ºä¾‹
)

# 3. åˆ›å»º Few-shot æ¨¡æ¿
few_shot_prompt = FewShotPromptTemplate(
    example_selector=example_selector,  # ä½¿ç”¨é€‰æ‹©å™¨è€Œéå›ºå®šç¤ºä¾‹
    example_prompt=PromptTemplate(
        input_variables=["input", "output"],
        template="Q: {input}\nA: {output}"
    ),
    prefix="è¯·å›ç­” Python ç¼–ç¨‹é—®é¢˜ï¼š",
    suffix="\nQ: {input}\nA:",
    input_variables=["input"]
)

# 4. æµ‹è¯•ï¼šç›¸ä¼¼é—®é¢˜ä¼šé€‰æ‹©ç›¸å…³ç¤ºä¾‹
formatted = few_shot_prompt.format(input="Python æ€ä¹ˆè¯»å– CSV æ–‡ä»¶ï¼Ÿ")
print(formatted)
# ä¼šè‡ªåŠ¨é€‰æ‹©ä¸"è¯»å–æ–‡ä»¶"ç›¸å…³çš„ç¤ºä¾‹
```

---

## ä¸‰ã€Chain of Thoughtï¼ˆæ€ç»´é“¾ï¼‰

### 3.1 ä»€ä¹ˆæ˜¯æ€ç»´é“¾ï¼Ÿ

**Chain of Thought (CoT)** æ˜¯ä¸€ç§å¼•å¯¼ AI é€æ­¥æ€è€ƒå’Œæ¨ç†çš„æŠ€æœ¯ï¼Œè€Œéç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚

#### å¯¹æ¯”æ•ˆæœ

```python
"""
å¯¹æ¯”æ ‡å‡†æç¤ºè¯ vs æ€ç»´é“¾æç¤ºè¯
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# âŒ æ ‡å‡†æç¤ºè¯ï¼ˆå¯èƒ½å‡ºé”™ï¼‰
standard_prompt = "å¦‚æœä¸€ä¸ªè‹¹æœ5å…ƒ,ä¹°17ä¸ªè‹¹æœ,ä»˜äº†100å…ƒ,åº”è¯¥æ‰¾å›å¤šå°‘é’±ï¼Ÿ"

# âœ… æ€ç»´é“¾æç¤ºè¯
cot_prompt = """å¦‚æœä¸€ä¸ªè‹¹æœ5å…ƒ,ä¹°17ä¸ªè‹¹æœ,ä»˜äº†100å…ƒ,åº”è¯¥æ‰¾å›å¤šå°‘é’±ï¼Ÿ

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ€è€ƒï¼š
1. è®¡ç®—æ€»ä»·
2. è®¡ç®—æ‰¾é›¶
3. ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ"""

print("=== æ ‡å‡†æç¤ºè¯ç»“æœ ===")
response1 = llm.invoke([HumanMessage(content=standard_prompt)])
print(response1.content)

print("\n=== æ€ç»´é“¾ç»“æœ ===")
response2 = llm.invoke([HumanMessage(content=cot_prompt)])
print(response2.content)
```

### 3.2 æ€ç»´é“¾æ¨¡å¼

#### æ¨¡å¼1ï¼šæ˜¾å¼æ­¥éª¤å¼•å¯¼

```python
"""
æ˜¾å¼æ­¥éª¤å¼•å¯¼ï¼šæ˜ç¡®åˆ—å‡ºæ€è€ƒæ­¥éª¤
é€‚ç”¨åœºæ™¯ï¼šæ•°å­¦è®¡ç®—ã€é€»è¾‘æ¨ç†ã€é—®é¢˜åˆ†æ
"""
cot_template = """é—®é¢˜ï¼š{question}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤åˆ†æï¼š
æ­¥éª¤1ï¼šç†è§£é—®é¢˜ - è¯†åˆ«å·²çŸ¥ä¿¡æ¯å’ŒæœªçŸ¥ä¿¡æ¯
æ­¥éª¤2ï¼šåˆ¶å®šç­–ç•¥ - ç¡®å®šè§£å†³æ–¹æ¡ˆ
æ­¥éª¤3ï¼šæ‰§è¡Œè®¡ç®— - é€æ­¥è®¡ç®—
æ­¥éª¤4ï¼šéªŒè¯ç­”æ¡ˆ - æ£€æŸ¥ç»“æœåˆç†æ€§

æœ€ç»ˆç­”æ¡ˆï¼š[åœ¨è¿™é‡Œç»™å‡ºç­”æ¡ˆ]"""

# ç¤ºä¾‹
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

prompt = PromptTemplate.from_template(cot_template)
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

question = "ä¸€ä¸ªæ°´æ± æœ‰ä¸¤ä¸ªè¿›æ°´ç®¡å’Œä¸€ä¸ªå‡ºæ°´ç®¡ã€‚ç”²ç®¡æ¯å°æ—¶è¿›æ°´12å¨ï¼Œä¹™ç®¡æ¯å°æ—¶è¿›æ°´8å¨ï¼Œä¸™ç®¡æ¯å°æ—¶å‡ºæ°´10å¨ã€‚å¦‚æœä¸‰ç®¡åŒæ—¶å¼€æ”¾ï¼Œ6å°æ—¶åæ°´æ± æœ‰å¤šå°‘å¨æ°´ï¼Ÿ"

formatted = prompt.format(question=question)
response = llm.invoke([HumanMessage(content=formatted)])
print(response.content)
```

#### æ¨¡å¼2ï¼šFew-shot CoT

```python
"""
Few-shot CoTï¼šé€šè¿‡ç¤ºä¾‹å±•ç¤ºæ€ç»´è¿‡ç¨‹
é€‚ç”¨åœºæ™¯ï¼šå¤æ‚æ¨ç†ä»»åŠ¡
"""
from langchain.prompts import FewShotPromptTemplate, PromptTemplate

# å®šä¹‰å¸¦æ€ç»´è¿‡ç¨‹çš„ç¤ºä¾‹
cot_examples = [
    {
        "question": "å°æ˜æœ‰15ä¸ªè‹¹æœï¼Œåƒæ‰3ä¸ªï¼Œåˆä¹°äº†8ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ",
        "reasoning": """è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒï¼š
1. å¼€å§‹æœ‰ï¼š15ä¸ªè‹¹æœ
2. åƒæ‰åå‰©ï¼š15 - 3 = 12ä¸ª
3. å†ä¹°8ä¸ªåï¼š12 + 8 = 20ä¸ª""",
        "answer": "20ä¸ª"
    },
    {
        "question": "å¦‚æœ3ä¸ªäººç”¨6å°æ—¶åšå®Œä¸€é¡¹å·¥ä½œï¼Œ5ä¸ªäººéœ€è¦å¤šå°‘å°æ—¶ï¼Ÿ",
        "reasoning": """è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒï¼š
1. å·¥ä½œæ€»é‡ = 3äºº Ã— 6å°æ—¶ = 18äººÂ·å°æ—¶
2. 5ä¸ªäººå®ŒæˆåŒæ ·å·¥ä½œï¼š18äººÂ·å°æ—¶ Ã· 5äºº = 3.6å°æ—¶""",
        "answer": "3.6å°æ—¶"
    }
]

# åˆ›å»ºæ¨¡æ¿
example_prompt = PromptTemplate(
    input_variables=["question", "reasoning", "answer"],
    template="""é—®é¢˜ï¼š{question}
æ€è€ƒè¿‡ç¨‹ï¼š{reasoning}
ç­”æ¡ˆï¼š{answer}"""
)

few_shot_cot = FewShotPromptTemplate(
    examples=cot_examples,
    example_prompt=example_prompt,
    prefix="è¯·åƒç¤ºä¾‹ä¸€æ ·ï¼Œå±•ç¤ºå®Œæ•´çš„æ€è€ƒè¿‡ç¨‹ï¼š\n",
    suffix="\né—®é¢˜ï¼š{question}\næ€è€ƒè¿‡ç¨‹ï¼š",
    input_variables=["question"]
)

# ä½¿ç”¨
formatted = few_shot_cot.format(
    question="8ä¸ªå·¥äºº10å¤©å®Œæˆä¸€é¡¹å·¥ç¨‹,å¦‚æœè¦6å¤©å®Œæˆ,éœ€è¦å¤šå°‘å·¥äººï¼Ÿ"
)
print(formatted)
```

### 3.3 Self-Consistencyï¼ˆè‡ªæˆ‘ä¸€è‡´æ€§ï¼‰

**Self-Consistency** æ˜¯ CoT çš„å¢å¼ºç‰ˆæœ¬ï¼šç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„ï¼Œé€‰æ‹©æœ€ä¸€è‡´çš„ç­”æ¡ˆã€‚

```python
"""
Self-Consistency å®ç°
åŸç†ï¼šå¤šæ¬¡é‡‡æ ·ï¼ŒæŠ•ç¥¨é€‰æ‹©æœ€é¢‘ç¹çš„ç­”æ¡ˆ
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from collections import Counter

def self_consistency_reasoning(question: str, num_samples: int = 5):
    """
    ä½¿ç”¨è‡ªæˆ‘ä¸€è‡´æ€§æ–¹æ³•æ±‚è§£é—®é¢˜

    å‚æ•°:
        question: é—®é¢˜
        num_samples: é‡‡æ ·æ¬¡æ•°

    è¿”å›:
        æœ€ä¸€è‡´çš„ç­”æ¡ˆ
    """
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)  # ä½¿ç”¨è¾ƒé«˜æ¸©åº¦å¢åŠ å¤šæ ·æ€§

    cot_prompt = f"""{question}

è¯·é€æ­¥æ€è€ƒå¹¶ç»™å‡ºç­”æ¡ˆã€‚æœ€åç”¨"ç­”æ¡ˆ:"æ˜ç¡®æ ‡æ³¨æœ€ç»ˆç»“æœã€‚"""

    # ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„
    answers = []
    reasonings = []

    for i in range(num_samples):
        response = llm.invoke([HumanMessage(content=cot_prompt)])
        content = response.content

        # æå–ç­”æ¡ˆï¼ˆå‡è®¾æ ¼å¼ä¸º "ç­”æ¡ˆ: XXX"ï¼‰
        if "ç­”æ¡ˆ:" in content or "ç­”æ¡ˆï¼š" in content:
            answer_part = content.split("ç­”æ¡ˆ")[-1].strip(": ï¼š")
            answers.append(answer_part)
            reasonings.append(content)

    # æŠ•ç¥¨é€‰æ‹©æœ€ä¸€è‡´çš„ç­”æ¡ˆ
    if answers:
        most_common_answer = Counter(answers).most_common(1)[0][0]

        print(f"=== ç”Ÿæˆäº† {len(answers)} ä¸ªæ¨ç†è·¯å¾„ ===")
        for i, (ans, reasoning) in enumerate(zip(answers, reasonings), 1):
            print(f"\nè·¯å¾„ {i} ç­”æ¡ˆï¼š{ans}")

        print(f"\n=== æœ€ç»ˆç­”æ¡ˆï¼ˆå‡ºç° {Counter(answers)[most_common_answer]} æ¬¡ï¼‰===")
        print(most_common_answer)

        return most_common_answer
    else:
        return "æ— æ³•æå–ç­”æ¡ˆ"

# æµ‹è¯•
question = "ä¸€ä¸ªæ•°é™¤ä»¥5ä½™3,é™¤ä»¥7ä½™4,è¿™ä¸ªæ•°æœ€å°æ˜¯å¤šå°‘ï¼Ÿ"
self_consistency_reasoning(question, num_samples=3)
```

---

## å››ã€ç»“æ„åŒ–è¾“å‡º

### 4.1 ä¸ºä»€ä¹ˆéœ€è¦ç»“æ„åŒ–è¾“å‡ºï¼Ÿ

AI é»˜è®¤è¾“å‡ºæ˜¯è‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼Œä½†ç¨‹åºéœ€è¦ç»“æ„åŒ–æ•°æ®ï¼ˆJSONã€XMLï¼‰æ¥å¤„ç†ã€‚

#### åº”ç”¨åœºæ™¯
- ğŸ” **ä¿¡æ¯æå–**ï¼šä»æ–‡æœ¬ä¸­æå–å®ä½“ã€å…³ç³»
- ğŸ“Š **æ•°æ®ç”Ÿæˆ**ï¼šç”Ÿæˆæµ‹è¯•æ•°æ®ã€é…ç½®æ–‡ä»¶
- ğŸ¤– **API é›†æˆ**ï¼šå°† AI è¾“å‡ºä¼ é€’ç»™å…¶ä»–ç³»ç»Ÿ

### 4.2 è¾“å‡º JSON æ ¼å¼

#### æ–¹æ³•1ï¼šPrompt å¼•å¯¼

```python
"""
æ–¹æ³•1ï¼šé€šè¿‡ Prompt å¼•å¯¼è¾“å‡º JSON
ä¼˜ç‚¹ï¼šç®€å•ç›´æ¥
ç¼ºç‚¹ï¼šæ ¼å¼ä¸ç¨³å®šï¼Œéœ€è¦è§£æå’ŒéªŒè¯
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
import json

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

prompt = """è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ä¿¡æ¯ï¼Œè¾“å‡º JSON æ ¼å¼ï¼š

æ–‡æœ¬ï¼šå¼ ä¸‰æ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œä»Šå¹´28å²ï¼Œæ“…é•¿Pythonå’ŒJavaï¼Œå·¥ä½œäºåŒ—äº¬çš„æŸç§‘æŠ€å…¬å¸ã€‚

è¦æ±‚è¾“å‡ºæ ¼å¼ï¼š
{
  "name": "å§“å",
  "age": å¹´é¾„(æ•°å­—),
  "skills": ["æŠ€èƒ½1", "æŠ€èƒ½2"],
  "location": "å·¥ä½œåœ°ç‚¹"
}

åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

response = llm.invoke([HumanMessage(content=prompt)])
print("åŸå§‹è¾“å‡ºï¼š")
print(response.content)

# è§£æ JSON
try:
    data = json.loads(response.content)
    print("\nè§£æåçš„æ•°æ®ï¼š")
    print(f"å§“åï¼š{data['name']}")
    print(f"å¹´é¾„ï¼š{data['age']}")
    print(f"æŠ€èƒ½ï¼š{', '.join(data['skills'])}")
except json.JSONDecodeError:
    print("JSON è§£æå¤±è´¥")
```

#### æ–¹æ³•2ï¼šä½¿ç”¨ OutputParser

```python
"""
æ–¹æ³•2ï¼šä½¿ç”¨ LangChain çš„ OutputParser
ä¼˜ç‚¹ï¼šè‡ªåŠ¨è§£æå’ŒéªŒè¯ï¼Œæ›´ç¨³å®š
"""
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field
from typing import List

# 1. å®šä¹‰æ•°æ®æ¨¡å‹
class PersonInfo(BaseModel):
    """äººå‘˜ä¿¡æ¯æ¨¡å‹"""
    name: str = Field(description="å§“å")
    age: int = Field(description="å¹´é¾„")
    skills: List[str] = Field(description="æŠ€èƒ½åˆ—è¡¨")
    location: str = Field(description="å·¥ä½œåœ°ç‚¹")

# 2. åˆ›å»ºè§£æå™¨
parser = PydanticOutputParser(pydantic_object=PersonInfo)

# 3. åˆ›å»ºå¸¦æ ¼å¼è¯´æ˜çš„ Prompt
prompt = PromptTemplate(
    template="""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ä¿¡æ¯ï¼š

{text}

{format_instructions}""",
    input_variables=["text"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

# 4. ä½¿ç”¨
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# æ ¼å¼åŒ– prompt
formatted = prompt.format(
    text="æå››æ˜¯ä¸€åæ•°æ®ç§‘å­¦å®¶ï¼Œ35å²ï¼Œç²¾é€šPythonã€Rå’ŒSQLï¼Œåœ¨ä¸Šæµ·å·¥ä½œã€‚"
)

print("å‘é€çš„ Promptï¼š")
print(formatted)
print("\n" + "=" * 60)

# è°ƒç”¨ LLM
response = llm.invoke([HumanMessage(content=formatted)])

# è§£æè¾“å‡º
person = parser.parse(response.content)

print("è§£æåçš„ç»“æ„åŒ–æ•°æ®ï¼š")
print(f"å§“åï¼š{person.name}")
print(f"å¹´é¾„ï¼š{person.age}")
print(f"æŠ€èƒ½ï¼š{person.skills}")
print(f"åœ°ç‚¹ï¼š{person.location}")

# å¯ä»¥ç›´æ¥è®¿é—®å±æ€§ï¼Œäº«å—ç±»å‹æ£€æŸ¥
print(f"\n{person.name}çš„å¹´é¾„æ˜¯{person.age}å²")  # ç±»å‹å®‰å…¨
```

### 4.3 å¤æ‚ç»“æ„åŒ–è¾“å‡º

```python
"""
å¤æ‚åµŒå¥—ç»“æ„çš„è¾“å‡º
åœºæ™¯ï¼šä»èŒä½æè¿°ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
"""
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List, Optional

# å®šä¹‰åµŒå¥—çš„æ•°æ®æ¨¡å‹
class JobRequirement(BaseModel):
    """èŒä½è¦æ±‚"""
    education: str = Field(description="å­¦å†è¦æ±‚")
    experience: str = Field(description="ç»éªŒè¦æ±‚")
    skills: List[str] = Field(description="æŠ€èƒ½è¦æ±‚")

class JobInfo(BaseModel):
    """èŒä½ä¿¡æ¯"""
    title: str = Field(description="èŒä½åç§°")
    company: str = Field(description="å…¬å¸åç§°")
    location: str = Field(description="å·¥ä½œåœ°ç‚¹")
    salary_range: Optional[str] = Field(description="è–ªèµ„èŒƒå›´", default=None)
    requirements: JobRequirement = Field(description="ä»»èŒè¦æ±‚")
    responsibilities: List[str] = Field(description="å·¥ä½œèŒè´£")

# åˆ›å»ºè§£æå™¨å’Œæç¤ºè¯
parser = PydanticOutputParser(pydantic_object=JobInfo)

prompt_template = """è¯·ä»ä»¥ä¸‹æ‹›è˜ä¿¡æ¯ä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼š

{job_description}

{format_instructions}"""

from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["job_description"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

# æµ‹è¯•æ•°æ®
job_desc = """
é«˜çº§Pythonå·¥ç¨‹å¸ˆ - TechCorpç§‘æŠ€æœ‰é™å…¬å¸

åœ°ç‚¹ï¼šæ·±åœ³
è–ªèµ„ï¼š25K-40K

èŒä½è¦æ±‚ï¼š
- æœ¬ç§‘åŠä»¥ä¸Šå­¦å†ï¼Œè®¡ç®—æœºç›¸å…³ä¸“ä¸š
- 5å¹´ä»¥ä¸ŠPythonå¼€å‘ç»éªŒ
- ç²¾é€šDjangoã€FastAPIç­‰Webæ¡†æ¶
- ç†Ÿæ‚‰MySQLã€Redisã€MongoDB
- æœ‰å¾®æœåŠ¡æ¶æ„ç»éªŒ

å·¥ä½œèŒè´£ï¼š
1. è´Ÿè´£åç«¯APIå¼€å‘å’Œç»´æŠ¤
2. ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ï¼Œè§£å†³æŠ€æœ¯éš¾é¢˜
3. å‚ä¸æ¶æ„è®¾è®¡å’ŒæŠ€æœ¯é€‰å‹
4. æŒ‡å¯¼åˆçº§å·¥ç¨‹å¸ˆ
"""

# æ‰§è¡Œ
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
formatted = prompt.format(job_description=job_desc)
response = llm.invoke([HumanMessage(content=formatted)])

# è§£æ
job_info = parser.parse(response.content)

# æ‰“å°ç»“æ„åŒ–æ•°æ®
print("=" * 60)
print(f"èŒä½ï¼š{job_info.title}")
print(f"å…¬å¸ï¼š{job_info.company}")
print(f"åœ°ç‚¹ï¼š{job_info.location}")
print(f"è–ªèµ„ï¼š{job_info.salary_range}")
print(f"\nä»»èŒè¦æ±‚ï¼š")
print(f"  å­¦å†ï¼š{job_info.requirements.education}")
print(f"  ç»éªŒï¼š{job_info.requirements.experience}")
print(f"  æŠ€èƒ½ï¼š{', '.join(job_info.requirements.skills)}")
print(f"\nå·¥ä½œèŒè´£ï¼š")
for i, resp in enumerate(job_info.responsibilities, 1):
    print(f"  {i}. {resp}")
print("=" * 60)
```

### 4.4 å®æˆ˜ï¼šæ„å»ºä¿¡æ¯æŠ½å–ç³»ç»Ÿ

```python
"""
å®æˆ˜é¡¹ç›®ï¼šæ–°é—»ä¿¡æ¯æŠ½å–ç³»ç»Ÿ
åŠŸèƒ½ï¼šä»æ–°é—»æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯
"""
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from typing import List
from datetime import datetime

# 1. å®šä¹‰æ–°é—»æ•°æ®æ¨¡å‹
class NewsArticle(BaseModel):
    """æ–°é—»æ–‡ç« æ•°æ®æ¨¡å‹"""
    title: str = Field(description="æ–°é—»æ ‡é¢˜")
    category: str = Field(description="æ–°é—»ç±»åˆ«ï¼ˆç§‘æŠ€/è´¢ç»/å¨±ä¹/ä½“è‚²/å…¶ä»–ï¼‰")
    keywords: List[str] = Field(description="å…³é”®è¯åˆ—è¡¨ï¼Œ3-5ä¸ª")
    summary: str = Field(description="æ–°é—»æ‘˜è¦ï¼Œ100å­—ä»¥å†…")
    entities: dict = Field(description="å®ä½“ä¿¡æ¯ï¼ŒåŒ…å«äººç‰©ã€åœ°ç‚¹ã€ç»„ç»‡ç­‰")
    sentiment: str = Field(description="æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰")

# 2. åˆ›å»ºè§£æå™¨
parser = PydanticOutputParser(pydantic_object=NewsArticle)

# 3. åˆ›å»ºæç¤ºè¯æ¨¡æ¿
prompt = PromptTemplate(
    template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æç³»ç»Ÿã€‚è¯·ä»ä»¥ä¸‹æ–°é—»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼š

æ–°é—»å†…å®¹ï¼š
{news_text}

{format_instructions}

æ³¨æ„ï¼š
1. æå–çš„ä¿¡æ¯è¦å‡†ç¡®
2. æ‘˜è¦è¦ç®€æ˜æ‰¼è¦
3. å…³é”®è¯è¦å…·æœ‰ä»£è¡¨æ€§""",
    input_variables=["news_text"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

# 4. æ„å»ºæ–°é—»åˆ†æå™¨ç±»
class NewsAnalyzer:
    """æ–°é—»åˆ†æå™¨"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        self.parser = parser
        self.prompt = prompt

    def analyze(self, news_text: str) -> NewsArticle:
        """
        åˆ†ææ–°é—»æ–‡æœ¬

        å‚æ•°:
            news_text: æ–°é—»åŸæ–‡

        è¿”å›:
            ç»“æ„åŒ–çš„æ–°é—»æ•°æ®
        """
        from langchain.schema import HumanMessage

        # æ ¼å¼åŒ–æç¤ºè¯
        formatted = self.prompt.format(news_text=news_text)

        # è°ƒç”¨ LLM
        response = self.llm.invoke([HumanMessage(content=formatted)])

        # è§£æè¾“å‡º
        article = self.parser.parse(response.content)

        return article

    def batch_analyze(self, news_list: List[str]) -> List[NewsArticle]:
        """æ‰¹é‡åˆ†æ"""
        return [self.analyze(news) for news in news_list]

# 5. æµ‹è¯•
analyzer = NewsAnalyzer()

test_news = """
OpenAI å‘å¸ƒ GPT-5 æ¨¡å‹ï¼Œæ€§èƒ½å¤§å¹…æå‡

æ—§é‡‘å±±ï¼Œ2025å¹´1æœˆ22æ—¥ - äººå·¥æ™ºèƒ½ç ”ç©¶å…¬å¸ OpenAI ä»Šå¤©å®£å¸ƒæ¨å‡ºå…¶æœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹ GPT-5ã€‚
æ ¹æ® OpenAI CEO Sam Altman çš„ä»‹ç»ï¼ŒGPT-5 åœ¨æ¨ç†èƒ½åŠ›ã€å‡†ç¡®æ€§å’Œå®‰å…¨æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚

æ–°æ¨¡å‹åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å‰ä»£äº§å“ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚
OpenAI è¡¨ç¤ºï¼ŒGPT-5 å°†é¦–å…ˆå‘ä¼ä¸šç”¨æˆ·å¼€æ”¾ï¼Œéšåé€æ­¥å‘ä¸ªäººå¼€å‘è€…æä¾›è®¿é—®æƒé™ã€‚

ä¸šç•Œä¸“å®¶è®¤ä¸ºï¼Œè¿™ä¸€å‘å¸ƒå°†è¿›ä¸€æ­¥æ¨åŠ¨ AI æŠ€æœ¯åœ¨å„è¡Œä¸šçš„åº”ç”¨ï¼Œä½†åŒæ—¶ä¹Ÿå¼•å‘äº†å…³äº AI ä¼¦ç†å’Œå®‰å…¨çš„è®¨è®ºã€‚
"""

result = analyzer.analyze(test_news)

# æ‰“å°ç»“æœ
print("=" * 60)
print(f"æ ‡é¢˜ï¼š{result.title}")
print(f"ç±»åˆ«ï¼š{result.category}")
print(f"å…³é”®è¯ï¼š{', '.join(result.keywords)}")
print(f"\næ‘˜è¦ï¼š")
print(result.summary)
print(f"\nå®ä½“ä¿¡æ¯ï¼š")
for entity_type, entities in result.entities.items():
    print(f"  {entity_type}ï¼š{entities}")
print(f"\næƒ…æ„Ÿå€¾å‘ï¼š{result.sentiment}")
print("=" * 60)
```

---

## äº”ã€æœ¬å‘¨ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šè®¾è®¡å¤šè§’è‰²å¯¹è¯ï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰

**ä»»åŠ¡**ï¼šåˆ›å»ºä¸€ä¸ª"è¾©è®ºç³»ç»Ÿ"ï¼Œè®©ä¸¤ä¸ª AI è§’è‰²å°±åŒä¸€è¯é¢˜è¿›è¡Œè¾©è®ºã€‚

**è¦æ±‚**ï¼š
1. ä½¿ç”¨ ChatPromptTemplate å®šä¹‰æ­£æ–¹å’Œåæ–¹è§’è‰²
2. å®ç°è‡³å°‘ 3 è½®è¾©è®º
3. æ¯è½®è¾©è®ºè®°å½•åŒæ–¹è§‚ç‚¹

<details>
<summary>æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>

```python
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# å®šä¹‰åŒæ–¹è§’è‰²
pro_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯æ­£æ–¹è¾©æ‰‹ï¼Œè§‚ç‚¹ï¼š{topic_pro}ã€‚è¯·ç”¨æœ‰åŠ›çš„è®ºæ®æ”¯æŒä½ çš„è§‚ç‚¹ã€‚"),
    ("human", "{context}")
])

con_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯åæ–¹è¾©æ‰‹ï¼Œè§‚ç‚¹ï¼š{topic_con}ã€‚è¯·ç”¨æœ‰åŠ›çš„è®ºæ®åé©³å¯¹æ–¹ã€‚"),
    ("human", "{context}")
])

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

# è¾©è®ºä¸»é¢˜
topic = "è¿œç¨‹åŠå…¬æ˜¯å¦åº”è¯¥æˆä¸ºä¸»æµå·¥ä½œæ¨¡å¼"

# 3 è½®è¾©è®º
debate_history = []

for round_num in range(1, 4):
    print(f"\n{'='*60}\nç¬¬ {round_num} è½®è¾©è®º\n{'='*60}")

    # æ­£æ–¹å‘è¨€
    context = f"è¿™æ˜¯ç¬¬{round_num}è½®è¾©è®ºã€‚" + ("\n".join(debate_history[-2:]) if debate_history else "")
    pro_msg = pro_template.format_messages(
        topic_pro="è¿œç¨‹åŠå…¬åº”è¯¥æˆä¸ºä¸»æµ",
        context=context
    )
    pro_response = llm.invoke(pro_msg)
    print(f"\næ­£æ–¹ï¼š{pro_response.content}")
    debate_history.append(f"æ­£æ–¹ç¬¬{round_num}è½®ï¼š{pro_response.content}")

    # åæ–¹å‘è¨€
    con_msg = con_template.format_messages(
        topic_con="è¿œç¨‹åŠå…¬ä¸åº”è¯¥æˆä¸ºä¸»æµ",
        context=context + f"\næ­£æ–¹åˆšæ‰è¯´ï¼š{pro_response.content}"
    )
    con_response = llm.invoke(con_msg)
    print(f"\nåæ–¹ï¼š{con_response.content}")
    debate_history.append(f"åæ–¹ç¬¬{round_num}è½®ï¼š{con_response.content}")
```
</details>

### ç»ƒä¹ 2ï¼šFew-shot æ–‡æœ¬åˆ†ç±»å™¨ï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰

**ä»»åŠ¡**ï¼šæ„å»ºä¸€ä¸ªç¼–ç¨‹è¯­è¨€è¯†åˆ«å™¨ï¼Œæ ¹æ®ä»£ç ç‰‡æ®µåˆ¤æ–­ç¼–ç¨‹è¯­è¨€ã€‚

**è¦æ±‚**ï¼š
1. ä½¿ç”¨ FewShotPromptTemplate
2. æä¾›è‡³å°‘ 5 ä¸ªä¸åŒè¯­è¨€çš„ç¤ºä¾‹
3. æµ‹è¯•å‡†ç¡®ç‡

<details>
<summary>æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>

```python
from langchain.prompts import FewShotPromptTemplate, PromptTemplate

examples = [
    {"code": "print('Hello')", "language": "Python"},
    {"code": "System.out.println(\"Hello\");", "language": "Java"},
    {"code": "console.log('Hello');", "language": "JavaScript"},
    {"code": "fmt.Println(\"Hello\")", "language": "Go"},
    {"code": "echo 'Hello';", "language": "PHP"},
]

example_prompt = PromptTemplate(
    input_variables=["code", "language"],
    template="ä»£ç ï¼š{code}\nè¯­è¨€ï¼š{language}"
)

few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="è¯·è¯†åˆ«ä»¥ä¸‹ä»£ç çš„ç¼–ç¨‹è¯­è¨€ï¼š\n",
    suffix="\nä»£ç ï¼š{code}\nè¯­è¨€ï¼š",
    input_variables=["code"]
)

from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# æµ‹è¯•
test_cases = [
    "def add(a, b): return a + b",
    "function add(a, b) { return a + b; }",
    "public int add(int a, int b) { return a + b; }"
]

for code in test_cases:
    formatted = few_shot_prompt.format(code=code)
    response = llm.invoke([HumanMessage(content=formatted)])
    print(f"ä»£ç ï¼š{code}\nè¯†åˆ«ä¸ºï¼š{response.content}\n")
```
</details>

### ç»ƒä¹ 3ï¼šæ„å»ºç®€å†è§£æå™¨ï¼ˆéš¾åº¦ï¼šâ­â­â­ï¼‰

**ä»»åŠ¡**ï¼šä»ç®€å†æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚

**è¦æ±‚**ï¼š
1. ä½¿ç”¨ Pydantic å®šä¹‰ç®€å†æ•°æ®æ¨¡å‹
2. åŒ…å«ï¼šåŸºæœ¬ä¿¡æ¯ã€æ•™è‚²ç»å†ã€å·¥ä½œç»å†ã€æŠ€èƒ½
3. å¤„ç†è‡³å°‘ 3 ä»½ä¸åŒæ ¼å¼çš„ç®€å†

<details>
<summary>æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>

```python
from pydantic import BaseModel, Field
from typing import List, Optional
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

class Education(BaseModel):
    school: str
    degree: str
    major: str
    year: str

class WorkExperience(BaseModel):
    company: str
    position: str
    duration: str
    responsibilities: List[str]

class Resume(BaseModel):
    name: str
    email: str
    phone: str
    education: List[Education]
    work_experience: List[WorkExperience]
    skills: List[str]

parser = PydanticOutputParser(pydantic_object=Resume)

prompt = PromptTemplate(
    template="""è¯·ä»ä»¥ä¸‹ç®€å†ä¸­æå–ä¿¡æ¯ï¼š

{resume_text}

{format_instructions}""",
    input_variables=["resume_text"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

resume_text = """
å¼ ä¼Ÿ
Email: zhangwei@example.com | ç”µè¯ï¼š138-0000-0000

æ•™è‚²èƒŒæ™¯ï¼š
- æ¸…åå¤§å­¦ï¼Œè®¡ç®—æœºç§‘å­¦ç¡•å£«ï¼Œ2018-2020
- åŒ—äº¬å¤§å­¦ï¼Œè½¯ä»¶å·¥ç¨‹å­¦å£«ï¼Œ2014-2018

å·¥ä½œç»å†ï¼š
å­—èŠ‚è·³åŠ¨ | é«˜çº§åç«¯å·¥ç¨‹å¸ˆ | 2020.7-è‡³ä»Š
- è´Ÿè´£æ¨èç³»ç»Ÿåç«¯å¼€å‘
- ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ï¼ŒQPS æå‡ 50%
- å¸¦é¢† 3 äººå°ç»„å®Œæˆå¾®æœåŠ¡æ‹†åˆ†

æŠ€èƒ½ï¼šPython, Go, Redis, Kafka, MySQL
"""

formatted = prompt.format(resume_text=resume_text)
response = llm.invoke([HumanMessage(content=formatted)])
resume = parser.parse(response.content)

print(f"å§“åï¼š{resume.name}")
print(f"è”ç³»æ–¹å¼ï¼š{resume.email} / {resume.phone}")
print("\næ•™è‚²ç»å†ï¼š")
for edu in resume.education:
    print(f"  {edu.school} - {edu.degree} - {edu.major} ({edu.year})")
print("\nå·¥ä½œç»å†ï¼š")
for work in resume.work_experience:
    print(f"  {work.company} - {work.position} ({work.duration})")
    for resp in work.responsibilities:
        print(f"    - {resp}")
print(f"\næŠ€èƒ½ï¼š{', '.join(resume.skills)}")
```
</details>

---

## å…­ã€æœ¬å‘¨æ€»ç»“

### 6.1 çŸ¥è¯†ç‚¹æ¸…å•

- [x] Prompt Engineering å…­å¤§åŸåˆ™
- [x] PromptTemplate åŸºç¡€å’Œé«˜çº§ç”¨æ³•
- [x] ChatPromptTemplate å¤šè§’è‰²å¯¹è¯
- [x] Few-shot Learning åŸç†å’Œåº”ç”¨
- [x] Chain of Thought æ€ç»´é“¾æŠ€æœ¯
- [x] ç»“æ„åŒ–è¾“å‡ºï¼ˆJSONã€Pydanticï¼‰
- [x] PydanticOutputParser ä½¿ç”¨
- [x] å¤æ‚åµŒå¥—æ•°æ®æ¨¡å‹å®šä¹‰

### 6.2 æœ€ä½³å®è·µæ€»ç»“

| åœºæ™¯ | æ¨èåšæ³• | é¿å…åšæ³• |
|------|---------|---------|
| **é€šç”¨å¯¹è¯** | ä½¿ç”¨ ChatPromptTemplate + æ¸…æ™°çš„è§’è‰²å®šä¹‰ | æ··ç”¨å¤šç§æ¶ˆæ¯æ ¼å¼ |
| **æ ¼å¼åŒ–è¾“å‡º** | Few-shot + æ˜ç¡®ç¤ºä¾‹ | ä»…ç”¨è‡ªç„¶è¯­è¨€æè¿° |
| **å¤æ‚æ¨ç†** | Chain of Thought | ç›´æ¥è¯¢é—®ç­”æ¡ˆ |
| **ä¿¡æ¯æå–** | Pydantic + OutputParser | æ‰‹åŠ¨è§£æå­—ç¬¦ä¸² |
| **å¯å¤ç”¨æç¤ºè¯** | ä½¿ç”¨æ¨¡æ¿å’Œå˜é‡ | ç¡¬ç¼–ç æç¤ºè¯ |

### 6.3 å¸¸è§é”™è¯¯ä¸è§£å†³

**é”™è¯¯1ï¼šæ¸©åº¦è®¾ç½®ä¸å½“**
```python
# âŒ æ•°å­¦è®¡ç®—ç”¨é«˜æ¸©åº¦
llm = ChatOpenAI(temperature=0.9)  # ç»“æœä¸ç¨³å®š

# âœ… äº‹å®æ€§ä»»åŠ¡ç”¨ä½æ¸©åº¦
llm = ChatOpenAI(temperature=0)
```

**é”™è¯¯2ï¼šPrompt è¿‡äºå¤æ‚**
```python
# âŒ ä¸€ä¸ª Prompt åšå¤ªå¤šäº‹
prompt = "åˆ†æè¿™æ®µä»£ç ï¼Œæ‰¾å‡ºbugï¼Œä¼˜åŒ–æ€§èƒ½ï¼Œå†™å•å…ƒæµ‹è¯•ï¼Œç”Ÿæˆæ–‡æ¡£"

# âœ… åˆ†è§£ä¸ºå¤šä¸ªæ­¥éª¤
prompts = [
    "åˆ†æè¿™æ®µä»£ç ï¼Œåˆ—å‡ºæ½œåœ¨çš„ bug",
    "æä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®",
    "ä¸ºå…³é”®å‡½æ•°ç¼–å†™å•å…ƒæµ‹è¯•",
    "ç”Ÿæˆ API æ–‡æ¡£"
]
```

### 6.4 ä¸‹å‘¨é¢„ä¹ 

**ç¬¬3å‘¨ä¸»é¢˜ï¼šModels æ·±å…¥**

é¢„ä¹ å†…å®¹ï¼š
1. ä¸åŒ LLM æä¾›å•†çš„å¯¹æ¯”ï¼ˆOpenAIã€Anthropicã€å¼€æºæ¨¡å‹ï¼‰
2. æ¨¡å‹å‚æ•°è¯¦è§£ï¼ˆmax_tokensã€top_pã€presence_penaltyï¼‰
3. æµå¼è¾“å‡ºï¼ˆStreamingï¼‰

**æ€è€ƒé—®é¢˜**ï¼š
- ä»€ä¹ˆæ—¶å€™åº”è¯¥ä½¿ç”¨æµå¼è¾“å‡ºï¼Ÿ
- å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Ÿ
- å¦‚ä½•ä¼˜åŒ– Token ä½¿ç”¨ï¼Ÿ

---

::: tip å­¦ä¹ å»ºè®®
1. **å¤šåšå®éªŒ**ï¼šå°è¯•ä¸åŒçš„ temperature å’Œ Prompt é£æ ¼
2. **å¯¹æ¯”æ•ˆæœ**ï¼šåŒä¸€ä»»åŠ¡ç”¨ä¸åŒ Promptï¼Œæ¯”è¾ƒè¾“å‡ºè´¨é‡
3. **å»ºç«‹æ¨¡æ¿åº“**ï¼šæ”¶é›†ä¼˜ç§€çš„ Prompt æ¨¡æ¿
4. **å…³æ³¨ç»†èŠ‚**ï¼šå°æ”¹åŠ¨å¯èƒ½å¸¦æ¥å¤§çš„æ•ˆæœå·®å¼‚
:::

**ç»§ç»­åŠ æ²¹ï¼ä¸‹å‘¨è§ï¼ğŸš€**
