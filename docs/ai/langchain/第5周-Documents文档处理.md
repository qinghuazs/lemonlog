---
title: ç¬¬5å‘¨ - Documents æ–‡æ¡£å¤„ç†
date: 2025-02-12
permalink: /ai/langchain/week5-documents.html
tags:
  - LangChain
  - Documents
  - RAG
categories:
  - AI
  - LangChain
---

# ç¬¬5å‘¨ï¼šDocuments æ–‡æ¡£å¤„ç†

::: tip æœ¬å‘¨å­¦ä¹ ç›®æ ‡
- ğŸ“„ æŒæ¡æ–‡æ¡£åŠ è½½ï¼ˆDocumentLoaderï¼‰
- âœ‚ï¸ ç†è§£æ–‡æ¡£åˆ†å‰²ï¼ˆTextSplitterï¼‰ç­–ç•¥
- ğŸ” å­¦ä¹ æ–‡æ¡£æ£€ç´¢åŸºç¡€
- ğŸ¯ å¤„ç†å¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆPDFã€Wordã€Markdownï¼‰
- ğŸ’¡ æ„å»ºæ–‡æ¡£é—®ç­”ç³»ç»Ÿ
:::

## ä¸€ã€æ–‡æ¡£å¤„ç†åŸºç¡€

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦æ–‡æ¡£å¤„ç†ï¼Ÿ

åœ¨æ„å»º RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿæ—¶ï¼Œæ–‡æ¡£å¤„ç†æ˜¯å…³é”®æ­¥éª¤ï¼š

```mermaid
graph LR
    A[åŸå§‹æ–‡æ¡£] --> B[åŠ è½½<br/>DocumentLoader]
    B --> C[åˆ†å‰²<br/>TextSplitter]
    C --> D[å‘é‡åŒ–<br/>Embeddings]
    D --> E[å­˜å‚¨<br/>Vector Store]
    E --> F[æ£€ç´¢+ç”Ÿæˆ<br/>RAG]

    style A fill:#FFE0B2
    style F fill:#C8E6C9
```

**æ ¸å¿ƒé—®é¢˜ï¼š**
1. **ä¸Šä¸‹æ–‡çª—å£é™åˆ¶**ï¼šLLM æ— æ³•å¤„ç†è¶…é•¿æ–‡æ¡£
2. **æ£€ç´¢æ•ˆç‡**ï¼šéœ€è¦å¿«é€Ÿæ‰¾åˆ°ç›¸å…³å†…å®¹
3. **æ ¼å¼å¤šæ ·æ€§**ï¼šPDFã€Wordã€HTML ç­‰æ ¼å¼ä¸åŒ

### 1.2 Document å¯¹è±¡

LangChain çš„ **Document** æ˜¯æ–‡æ¡£çš„æ ‡å‡†è¡¨ç¤ºï¼š

```python
"""
Document å¯¹è±¡ç»“æ„
"""
from langchain.schema import Document

# åˆ›å»º Document
doc = Document(
    page_content="è¿™æ˜¯æ–‡æ¡£çš„å®é™…å†…å®¹",  # å¿…éœ€ï¼šæ–‡æœ¬å†…å®¹
    metadata={                          # å¯é€‰ï¼šå…ƒæ•°æ®
        "source": "example.pdf",
        "page": 1,
        "author": "å¼ ä¸‰"
    }
)

print(f"å†…å®¹ï¼š{doc.page_content}")
print(f"å…ƒæ•°æ®ï¼š{doc.metadata}")
```

---

## äºŒã€æ–‡æ¡£åŠ è½½ï¼ˆDocumentLoaderï¼‰

### 2.1 æ–‡æœ¬æ–‡ä»¶åŠ è½½

#### 2.1.1 TextLoader

```python
"""
åŠ è½½çº¯æ–‡æœ¬æ–‡ä»¶
"""
from langchain.document_loaders import TextLoader

# åŠ è½½å•ä¸ªæ–‡æœ¬æ–‡ä»¶
loader = TextLoader("document.txt", encoding="utf-8")
documents = loader.load()

print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
print(f"ç¬¬ä¸€ä¸ªæ–‡æ¡£ï¼š\n{documents[0].page_content[:200]}...")
print(f"å…ƒæ•°æ®ï¼š{documents[0].metadata}")
```

#### 2.1.2 DirectoryLoaderï¼ˆæ‰¹é‡åŠ è½½ï¼‰

```python
"""
æ‰¹é‡åŠ è½½ç›®å½•ä¸­çš„æ–‡ä»¶
"""
from langchain.document_loaders import DirectoryLoader, TextLoader

# åŠ è½½ç›®å½•ä¸­æ‰€æœ‰ .txt æ–‡ä»¶
loader = DirectoryLoader(
    path="./documents/",
    glob="**/*.txt",          # åŒ¹é…æ¨¡å¼
    loader_cls=TextLoader,    # ä½¿ç”¨çš„åŠ è½½å™¨
    loader_kwargs={"encoding": "utf-8"}
)

documents = loader.load()
print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

# æŸ¥çœ‹æ¯ä¸ªæ–‡æ¡£çš„æ¥æº
for doc in documents:
    print(f"æ¥æºï¼š{doc.metadata['source']}")
```

### 2.2 PDF æ–‡ä»¶åŠ è½½

```python
"""
åŠ è½½ PDF æ–‡ä»¶
"""
# å®‰è£…ï¼špip install pypdf
from langchain.document_loaders import PyPDFLoader

# æ–¹æ³•1ï¼šPyPDFLoaderï¼ˆæ¯é¡µä¸€ä¸ª Documentï¼‰
loader = PyPDFLoader("example.pdf")
pages = loader.load()

print(f"PDF å…± {len(pages)} é¡µ")
for i, page in enumerate(pages):
    print(f"ç¬¬ {i+1} é¡µï¼š{page.page_content[:100]}...")
    print(f"å…ƒæ•°æ®ï¼š{page.metadata}")
```

```python
"""
æ›´é«˜çº§çš„ PDF åŠ è½½å™¨
"""
# å®‰è£…ï¼špip install pdfplumber
from langchain.document_loaders import PDFPlumberLoader

# PDFPlumberï¼šæ›´å¥½çš„è¡¨æ ¼æ”¯æŒ
loader = PDFPlumberLoader("example.pdf")
documents = loader.load()
```

### 2.3 Word æ–‡æ¡£åŠ è½½

```python
"""
åŠ è½½ Word æ–‡æ¡£
"""
# å®‰è£…ï¼špip install python-docx
from langchain.document_loaders import Docx2txtLoader

loader = Docx2txtLoader("example.docx")
documents = loader.load()

print(f"å†…å®¹ï¼š\n{documents[0].page_content}")
```

### 2.4 Markdown å’Œ HTML

```python
"""
åŠ è½½ Markdown æ–‡ä»¶
"""
from langchain.document_loaders import UnstructuredMarkdownLoader

loader = UnstructuredMarkdownLoader("README.md")
documents = loader.load()
```

```python
"""
åŠ è½½ç½‘é¡µ
"""
from langchain.document_loaders import WebBaseLoader

# ä» URL åŠ è½½
loader = WebBaseLoader("https://example.com")
documents = loader.load()

print(f"ç½‘é¡µæ ‡é¢˜ï¼š{documents[0].metadata.get('title', 'N/A')}")
print(f"å†…å®¹ï¼š{documents[0].page_content[:200]}...")
```

### 2.5 è‡ªå®šä¹‰åŠ è½½å™¨

```python
"""
è‡ªå®šä¹‰æ–‡æ¡£åŠ è½½å™¨
åœºæ™¯ï¼šåŠ è½½è‡ªå®šä¹‰æ ¼å¼çš„æ–‡ä»¶
"""
from langchain.document_loaders.base import BaseLoader
from langchain.schema import Document
from typing import List
import json

class JSONLoader(BaseLoader):
    """JSON æ–‡ä»¶åŠ è½½å™¨"""

    def __init__(self, file_path: str, content_key: str = "content"):
        self.file_path = file_path
        self.content_key = content_key

    def load(self) -> List[Document]:
        """åŠ è½½ JSON æ–‡ä»¶"""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        documents = []

        if isinstance(data, list):
            # JSON æ•°ç»„
            for i, item in enumerate(data):
                content = item.get(self.content_key, "")
                metadata = {k: v for k, v in item.items() if k != self.content_key}
                metadata["source"] = self.file_path
                metadata["index"] = i

                documents.append(Document(
                    page_content=content,
                    metadata=metadata
                ))
        else:
            # JSON å¯¹è±¡
            content = data.get(self.content_key, "")
            metadata = {k: v for k, v in data.items() if k != self.content_key}
            metadata["source"] = self.file_path

            documents.append(Document(
                page_content=content,
                metadata=metadata
            ))

        return documents

# ä½¿ç”¨ç¤ºä¾‹
loader = JSONLoader("data.json", content_key="text")
documents = loader.load()
```

---

## ä¸‰ã€æ–‡æ¡£åˆ†å‰²ï¼ˆTextSplitterï¼‰

### 3.1 ä¸ºä»€ä¹ˆè¦åˆ†å‰²æ–‡æ¡£ï¼Ÿ

**é—®é¢˜ï¼š**
- LLM ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼ˆGPT-3.5: 4K, GPT-4: 8K/32Kï¼‰
- é•¿æ–‡æ¡£æ— æ³•ä¸€æ¬¡å¤„ç†
- æ£€ç´¢éœ€è¦ç²¾ç¡®åŒ¹é…ç›¸å…³ç‰‡æ®µ

**è§£å†³æ–¹æ¡ˆï¼š**
å°†é•¿æ–‡æ¡£åˆ†å‰²æˆæ›´å°çš„å—ï¼ˆchunksï¼‰

```mermaid
graph LR
    A[é•¿æ–‡æ¡£<br/>10000 å­—] --> B[åˆ†å‰²å™¨]
    B --> C[Chunk 1<br/>500 å­—]
    B --> D[Chunk 2<br/>500 å­—]
    B --> E[Chunk 3<br/>500 å­—]
    B --> F[...]

    style A fill:#FFE0B2
    style C fill:#C8E6C9
    style D fill:#C8E6C9
    style E fill:#C8E6C9
    style F fill:#C8E6C9
```

### 3.2 CharacterTextSplitter

æœ€åŸºç¡€çš„åˆ†å‰²å™¨ï¼ŒæŒ‰å­—ç¬¦æ•°åˆ†å‰²ï¼š

```python
"""
CharacterTextSplitter ç¤ºä¾‹
"""
from langchain.text_splitter import CharacterTextSplitter

text = """LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚
å®ƒæä¾›äº†æ ‡å‡†åŒ–çš„æ¥å£å’Œå·¥å…·é“¾ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿè½»æ¾æ„å»ºå¤æ‚çš„ AI åº”ç”¨ã€‚

LangChain çš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š
1. Modelsï¼šä¸ LLM äº¤äº’çš„æ¥å£
2. Promptsï¼šç®¡ç†å’Œä¼˜åŒ–è¾“å…¥æ–‡æœ¬
3. Memoryï¼šå­˜å‚¨å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡
4. Chainsï¼šç»„åˆå¤šä¸ªç»„ä»¶çš„æµç¨‹
5. Agentsï¼šæ ¹æ®è¾“å…¥åŠ¨æ€å†³å®šè¡ŒåŠ¨çš„æ™ºèƒ½ä½“
"""

# åˆ›å»ºåˆ†å‰²å™¨
splitter = CharacterTextSplitter(
    separator="\n\n",         # åˆ†å‰²ç¬¦ï¼ˆä¼˜å…ˆæŒ‰æ­¤åˆ†å‰²ï¼‰
    chunk_size=100,           # æ¯å—æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=20,         # å—ä¹‹é—´é‡å å­—ç¬¦æ•°
    length_function=len       # è®¡ç®—é•¿åº¦çš„å‡½æ•°
)

chunks = splitter.split_text(text)

print(f"åˆ†å‰²æˆ {len(chunks)} å—ï¼š")
for i, chunk in enumerate(chunks, 1):
    print(f"\nå— {i} ({len(chunk)} å­—ç¬¦):")
    print(chunk)
    print("-" * 60)
```

**å…³é”®å‚æ•°ï¼š**
- `chunk_size`ï¼šæ¯å—çš„ç›®æ ‡å¤§å°
- `chunk_overlap`ï¼šé‡å éƒ¨åˆ†ï¼Œé¿å…åˆ‡æ–­è¯­ä¹‰

```python
# å¯è§†åŒ–é‡å 
"""
åŸæ–‡ï¼šABCDEFGHIJ
chunk_size=5, chunk_overlap=2

å—1: ABCDE
å—2:    DEFGH  ï¼ˆä¸å—1é‡å  DEï¼‰
å—3:       GHIJ  ï¼ˆä¸å—2é‡å  GHï¼‰
"""
```

### 3.3 RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰

é€’å½’åœ°æŒ‰å¤šä¸ªåˆ†éš”ç¬¦å°è¯•åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼š

```python
"""
RecursiveCharacterTextSplitter ç¤ºä¾‹
æ¨èä½¿ç”¨ï¼Œæ™ºèƒ½ä¿æŒæ®µè½å’Œå¥å­å®Œæ•´
"""
from langchain.text_splitter import RecursiveCharacterTextSplitter

text = """# LangChain å…¥é—¨æŒ‡å—

## ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ

LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ã€‚å®ƒå¸®åŠ©å¼€å‘è€…æ„å»º AI åº”ç”¨ã€‚

## æ ¸å¿ƒæ¦‚å¿µ

LangChain åŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š
- Models
- Prompts
- Memory

æ¯ä¸ªç»„ä»¶éƒ½æœ‰ç‰¹å®šä½œç”¨ã€‚"""

splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=20,
    separators=["\n\n", "\n", "ã€‚", " ", ""]  # åˆ†å‰²ä¼˜å…ˆçº§
)

chunks = splitter.split_text(text)

for i, chunk in enumerate(chunks, 1):
    print(f"å— {i}:\n{chunk}\n{'='*60}")
```

**åˆ†å‰²ç­–ç•¥ï¼š**
1. å…ˆå°è¯•æŒ‰ `\n\n`ï¼ˆæ®µè½ï¼‰åˆ†å‰²
2. å¦‚æœå—ä»ç„¶å¤ªå¤§ï¼ŒæŒ‰ `\n`ï¼ˆè¡Œï¼‰åˆ†å‰²
3. å†å¤§å°±æŒ‰ `ã€‚`ï¼ˆå¥å­ï¼‰åˆ†å‰²
4. æœ€åæŒ‰ç©ºæ ¼æˆ–å­—ç¬¦åˆ†å‰²

### 3.4 TokenTextSplitter

æŒ‰ Token æ•°é‡åˆ†å‰²ï¼ˆæ›´ç²¾ç¡®ï¼‰ï¼š

```python
"""
TokenTextSplitter ç¤ºä¾‹
é€‚ç”¨äºéœ€è¦ç²¾ç¡®æ§åˆ¶ Token æ•°çš„åœºæ™¯
"""
from langchain.text_splitter import TokenTextSplitter

text = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬..." * 100

splitter = TokenTextSplitter(
    chunk_size=100,      # æœ€å¤§ 100 tokens
    chunk_overlap=10     # é‡å  10 tokens
)

chunks = splitter.split_text(text)
print(f"åˆ†å‰²æˆ {len(chunks)} å—")

# éªŒè¯ token æ•°
import tiktoken
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

for i, chunk in enumerate(chunks[:3], 1):
    token_count = len(encoding.encode(chunk))
    print(f"å— {i}: {token_count} tokens")
```

### 3.5 Markdown å’Œä»£ç åˆ†å‰²å™¨

```python
"""
MarkdownTextSplitterï¼šä¿æŒ Markdown ç»“æ„
"""
from langchain.text_splitter import MarkdownTextSplitter

markdown_text = """# æ ‡é¢˜1

## å°æ ‡é¢˜1.1
å†…å®¹1

## å°æ ‡é¢˜1.2
å†…å®¹2

# æ ‡é¢˜2
å†…å®¹3"""

splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)
chunks = splitter.split_text(markdown_text)

for chunk in chunks:
    print(f"å—:\n{chunk}\n{'='*60}")
```

```python
"""
ä»£ç åˆ†å‰²å™¨ï¼šæŒ‰è¯­è¨€æ™ºèƒ½åˆ†å‰²
"""
from langchain.text_splitter import (
    Language,
    RecursiveCharacterTextSplitter
)

# Python ä»£ç åˆ†å‰²
python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=50,
    chunk_overlap=0
)

code = """
def hello():
    print("Hello")

def world():
    print("World")

class MyClass:
    def __init__(self):
        self.value = 0
"""

chunks = python_splitter.split_text(code)
for i, chunk in enumerate(chunks, 1):
    print(f"ä»£ç å— {i}:\n{chunk}\n{'='*60}")
```

### 3.6 åˆ†å‰²ç­–ç•¥å¯¹æ¯”

| åˆ†å‰²å™¨ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|--------|------|------|---------|
| **CharacterTextSplitter** | ç®€å•ç›´æ¥ | å¯èƒ½åˆ‡æ–­è¯­ä¹‰ | ç®€å•æ–‡æœ¬ |
| **RecursiveCharacterTextSplitter** | ä¿æŒè¯­ä¹‰å®Œæ•´ | ç¨æ…¢ | å¤§å¤šæ•°åœºæ™¯ï¼ˆæ¨èï¼‰ |
| **TokenTextSplitter** | ç²¾ç¡®æ§åˆ¶ Token | éœ€è¦ç¼–ç å™¨ | ä¸¥æ ¼ Token é™åˆ¶ |
| **MarkdownTextSplitter** | ä¿æŒç»“æ„ | ä»…é™ Markdown | Markdown æ–‡æ¡£ |
| **Language-specific** | ä»£ç è¯­ä¹‰å®Œæ•´ | ä»…é™ä»£ç  | ä»£ç æ–‡æ¡£ |

---

## å››ã€å®æˆ˜é¡¹ç›®

### 4.1 é¡¹ç›®ï¼šæ–‡æ¡£é—®ç­”ç³»ç»Ÿ

```python
"""
é¡¹ç›®ï¼šæ„å»ºç®€å•çš„æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
æµç¨‹ï¼šåŠ è½½æ–‡æ¡£ -> åˆ†å‰² -> å›ç­”é—®é¢˜
"""
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from typing import List

class SimpleDocQA:
    """ç®€å•æ–‡æ¡£é—®ç­”ç³»ç»Ÿï¼ˆæ— å‘é‡æ£€ç´¢ï¼‰"""

    def __init__(self, file_path: str):
        self.file_path = file_path
        self.chunks = []
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

        # åŠ è½½å’Œåˆ†å‰²æ–‡æ¡£
        self._load_and_split()

    def _load_and_split(self):
        """åŠ è½½å¹¶åˆ†å‰²æ–‡æ¡£"""
        # 1. åŠ è½½
        loader = TextLoader(self.file_path, encoding="utf-8")
        documents = loader.load()

        # 2. åˆ†å‰²
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        self.chunks = splitter.split_documents(documents)

        print(f"æ–‡æ¡£å·²åˆ†å‰²æˆ {len(self.chunks)} å—")

    def _simple_search(self, query: str, top_k: int = 3) -> List[str]:
        """
        ç®€å•çš„å…³é”®è¯æœç´¢ï¼ˆæ— å‘é‡åŒ–ï¼‰
        å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨å‘é‡æ£€ç´¢
        """
        # è®¡ç®—æ¯ä¸ªå—ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼ˆç®€å•çš„å…³é”®è¯åŒ¹é…ï¼‰
        scored_chunks = []

        for chunk in self.chunks:
            score = sum(1 for word in query.split() if word in chunk.page_content)
            scored_chunks.append((score, chunk.page_content))

        # æ’åºå¹¶è¿”å› top_k
        scored_chunks.sort(reverse=True, key=lambda x: x[0])
        return [chunk for score, chunk in scored_chunks[:top_k] if score > 0]

    def ask(self, question: str) -> str:
        """
        å›ç­”é—®é¢˜

        å‚æ•°:
            question: ç”¨æˆ·é—®é¢˜

        è¿”å›:
            AI çš„å›ç­”
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£å—
        relevant_chunks = self._simple_search(question, top_k=3)

        if not relevant_chunks:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æ¡£ä¸­æ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # 2. æ„å»ºæç¤ºè¯
        context = "\n\n---\n\n".join(relevant_chunks)

        prompt = ChatPromptTemplate.from_template("""æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚
å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æ–‡æ¡£ä¸­æ²¡æœ‰æåˆ°"ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š""")

        # 3. è°ƒç”¨ LLM
        chain = prompt | self.llm
        response = chain.invoke({
            "context": context,
            "question": question
        })

        return response.content

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_content = """
    LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚

    æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š
    1. Modelsï¼šä¸ LLM äº¤äº’çš„æ¥å£ï¼Œæ”¯æŒ OpenAIã€Anthropic ç­‰æä¾›å•†ã€‚
    2. Promptsï¼šæç¤ºè¯æ¨¡æ¿ç³»ç»Ÿï¼Œæ”¯æŒå˜é‡å’Œ Few-shot Learningã€‚
    3. Memoryï¼šå¯¹è¯å†å²ç®¡ç†ï¼ŒåŒ…æ‹¬ ConversationBufferMemory ç­‰ã€‚
    4. Chainsï¼šç»„åˆå¤šä¸ªç»„ä»¶ï¼Œå®ç°å¤æ‚å·¥ä½œæµã€‚
    5. Agentsï¼šæ™ºèƒ½å†³ç­–ç³»ç»Ÿï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·å®Œæˆä»»åŠ¡ã€‚

    LangChain æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ŒåŒ…æ‹¬ PDFã€Wordã€Markdown ç­‰ã€‚

    æ–‡æ¡£åˆ†å‰²ç­–ç•¥åŒ…æ‹¬ï¼š
    - CharacterTextSplitterï¼šæŒ‰å­—ç¬¦åˆ†å‰²
    - RecursiveCharacterTextSplitterï¼šé€’å½’åˆ†å‰²ï¼Œæ¨èä½¿ç”¨
    - TokenTextSplitterï¼šæŒ‰ Token åˆ†å‰²

    æ¨èçš„åˆ†å‰²å‚æ•°ï¼šchunk_size=500-1000ï¼Œchunk_overlap=50-200ã€‚
    """

    with open("langchain_intro.txt", "w", encoding="utf-8") as f:
        f.write(test_content)

    # åˆ›å»ºé—®ç­”ç³»ç»Ÿ
    qa_system = SimpleDocQA("langchain_intro.txt")

    # æé—®
    questions = [
        "LangChain æœ‰å“ªäº›æ ¸å¿ƒç»„ä»¶ï¼Ÿ",
        "æ¨èä½¿ç”¨å“ªç§æ–‡æ¡£åˆ†å‰²å™¨ï¼Ÿ",
        "chunk_size åº”è¯¥è®¾ç½®å¤šå°‘ï¼Ÿ",
        "LangChain æ”¯æŒ Java å—ï¼Ÿ"  # æ–‡æ¡£ä¸­æ²¡æœ‰çš„ä¿¡æ¯
    ]

    for q in questions:
        print(f"\né—®é¢˜ï¼š{q}")
        answer = qa_system.ask(q)
        print(f"å›ç­”ï¼š{answer}")
        print("-" * 60)
```

### 4.2 é¡¹ç›®ï¼šæ™ºèƒ½æ–‡æ¡£æ€»ç»“å™¨

```python
"""
é¡¹ç›®ï¼šæ™ºèƒ½æ–‡æ¡£æ€»ç»“å™¨
åŠŸèƒ½ï¼š
1. åŠ è½½é•¿æ–‡æ¡£
2. åˆ†å—å¤„ç†
3. ç”Ÿæˆæ¯å—æ‘˜è¦
4. åˆå¹¶æˆæœ€ç»ˆæ‘˜è¦
"""
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain

class DocumentSummarizer:
    """æ–‡æ¡£æ€»ç»“å™¨"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

    def summarize(self, file_path: str, max_summary_length: int = 200) -> dict:
        """
        æ€»ç»“æ–‡æ¡£

        å‚æ•°:
            file_path: æ–‡æ¡£è·¯å¾„
            max_summary_length: æœ€ç»ˆæ‘˜è¦æœ€å¤§é•¿åº¦

        è¿”å›:
            åŒ…å«åˆ†å—æ‘˜è¦å’Œæœ€ç»ˆæ‘˜è¦çš„å­—å…¸
        """
        # 1. åŠ è½½æ–‡æ¡£
        loader = TextLoader(file_path, encoding="utf-8")
        docs = loader.load()

        # 2. åˆ†å‰²
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100
        )
        chunks = splitter.split_documents(docs)

        print(f"æ–‡æ¡£åˆ†å‰²æˆ {len(chunks)} å—")

        # 3. æ€»ç»“æ¯ä¸ªå—ï¼ˆMap é˜¶æ®µï¼‰
        chunk_summaries = []

        for i, chunk in enumerate(chunks, 1):
            print(f"æ­£åœ¨æ€»ç»“ç¬¬ {i}/{len(chunks)} å—...")

            prompt = ChatPromptTemplate.from_template(
                "ç”¨1-2å¥è¯æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\n\n{text}"
            )

            chain = prompt | self.llm
            summary = chain.invoke({"text": chunk.page_content})

            chunk_summaries.append(summary.content)

        # 4. åˆå¹¶æ‘˜è¦ï¼ˆReduce é˜¶æ®µï¼‰
        print("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ‘˜è¦...")

        combined_text = "\n".join(chunk_summaries)

        final_prompt = ChatPromptTemplate.from_template(
            f"""ä»¥ä¸‹æ˜¯æ–‡æ¡£çš„åˆ†æ®µæ‘˜è¦ã€‚è¯·å°†å®ƒä»¬æ•´åˆæˆä¸€ä¸ªè¿è´¯çš„æ‘˜è¦ï¼ˆ{max_summary_length}å­—ä»¥å†…ï¼‰ï¼š

{{summaries}}

æœ€ç»ˆæ‘˜è¦ï¼š"""
        )

        final_chain = final_prompt | self.llm
        final_summary = final_chain.invoke({"summaries": combined_text})

        return {
            "chunk_summaries": chunk_summaries,
            "final_summary": final_summary.content,
            "num_chunks": len(chunks)
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    long_document = """
    äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£åœ¨æ·±åˆ»æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œã€‚ä»è‡ªåŠ¨é©¾é©¶æ±½è½¦åˆ°æ™ºèƒ½åŠ©æ‰‹ï¼ŒAI æŠ€æœ¯å·²ç»æ¸—é€åˆ°ç”Ÿæ´»çš„å„ä¸ªæ–¹é¢ã€‚

    åœ¨åŒ»ç–—é¢†åŸŸï¼ŒAI å¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­ç–¾ç—…ã€‚æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥åˆ†æåŒ»ç–—å½±åƒï¼Œè¯†åˆ«è‚¿ç˜¤å’Œå…¶ä»–å¼‚å¸¸ã€‚
    è¿™ä¸ä»…æé«˜äº†è¯Šæ–­å‡†ç¡®ç‡ï¼Œè¿˜ç¼©çŸ­äº†è¯Šæ–­æ—¶é—´ã€‚

    æ•™è‚²é¢†åŸŸä¹Ÿåœ¨ç»å† AI é©å‘½ã€‚æ™ºèƒ½æ•™å­¦ç³»ç»Ÿå¯ä»¥æ ¹æ®å­¦ç”Ÿçš„å­¦ä¹ è¿›åº¦æä¾›ä¸ªæ€§åŒ–å†…å®¹ã€‚
    è¿™ç§è‡ªé€‚åº”å­¦ä¹ æ–¹å¼æé«˜äº†æ•™å­¦æ•ˆç‡ã€‚

    ç„¶è€Œï¼ŒAI ä¹Ÿå¸¦æ¥äº†æŒ‘æˆ˜ã€‚éšç§é—®é¢˜ã€å°±ä¸šå½±å“ã€ç®—æ³•åè§ç­‰éƒ½éœ€è¦è®¤çœŸå¯¹å¾…ã€‚
    æˆ‘ä»¬éœ€è¦åœ¨æ¨åŠ¨æŠ€æœ¯å‘å±•çš„åŒæ—¶ï¼Œç¡®ä¿ AI çš„è´Ÿè´£ä»»ä½¿ç”¨ã€‚

    æœªæ¥ï¼ŒAI å°†ç»§ç»­å‘å±•ã€‚é‡å­è®¡ç®—ã€ç¥ç»ç½‘ç»œçš„çªç ´å¯èƒ½å¸¦æ¥æ–°çš„å¯èƒ½æ€§ã€‚
    æˆ‘ä»¬åº”è¯¥ä»¥å¼€æ”¾ä½†è°¨æ…çš„æ€åº¦æ‹¥æŠ±è¿™ä¸ª AI æ—¶ä»£ã€‚
    """ * 3  # é‡å¤3æ¬¡ä½¿æ–‡æ¡£æ›´é•¿

    with open("ai_article.txt", "w", encoding="utf-8") as f:
        f.write(long_document)

    # æ€»ç»“æ–‡æ¡£
    summarizer = DocumentSummarizer()
    result = summarizer.summarize("ai_article.txt", max_summary_length=150)

    print("\n" + "=" * 60)
    print("åˆ†å—æ‘˜è¦ï¼š")
    for i, summary in enumerate(result["chunk_summaries"], 1):
        print(f"{i}. {summary}")

    print("\n" + "=" * 60)
    print("æœ€ç»ˆæ‘˜è¦ï¼š")
    print(result["final_summary"])
    print("=" * 60)
```

---

## äº”ã€æœ¬å‘¨ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šå¤šæ ¼å¼æ–‡æ¡£åŠ è½½å™¨ï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰

**ä»»åŠ¡**ï¼šåˆ›å»ºä¸€ä¸ªç»Ÿä¸€çš„æ–‡æ¡£åŠ è½½å™¨ï¼Œè‡ªåŠ¨è¯†åˆ«æ–‡ä»¶ç±»å‹ï¼ˆtxtã€pdfã€docxï¼‰å¹¶åŠ è½½ã€‚

<details>
<summary>æŸ¥çœ‹æç¤º</summary>

æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©ä¸åŒçš„ Loaderã€‚
</details>

### ç»ƒä¹ 2ï¼šæ™ºèƒ½åˆ†å‰²å‚æ•°é€‰æ‹©ï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰

**ä»»åŠ¡**ï¼šæ ¹æ®æ–‡æ¡£é•¿åº¦å’Œç±»å‹ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„ `chunk_size` å’Œ `chunk_overlap`ã€‚

<details>
<summary>æŸ¥çœ‹æç¤º</summary>

- çŸ­æ–‡æ¡£ï¼ˆ<1000å­—ï¼‰ï¼šchunk_size=200
- ä¸­ç­‰æ–‡æ¡£ï¼ˆ1000-5000å­—ï¼‰ï¼šchunk_size=500
- é•¿æ–‡æ¡£ï¼ˆ>5000å­—ï¼‰ï¼šchunk_size=1000
</details>

### ç»ƒä¹ 3ï¼šæ–‡æ¡£å¯¹æ¯”ç³»ç»Ÿï¼ˆéš¾åº¦ï¼šâ­â­â­ï¼‰

**ä»»åŠ¡**ï¼šæ„å»ºä¸€ä¸ªç³»ç»Ÿï¼Œæ¯”è¾ƒä¸¤ä¸ªæ–‡æ¡£çš„å¼‚åŒã€‚

**è¦æ±‚**ï¼š
1. åŠ è½½ä¸¤ä¸ªæ–‡æ¡£
2. æå–å…³é”®ä¿¡æ¯
3. å¯¹æ¯”å·®å¼‚
4. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

---

## å…­ã€æœ¬å‘¨æ€»ç»“

### 6.1 çŸ¥è¯†ç‚¹æ¸…å•

- [x] Document å¯¹è±¡ç»“æ„
- [x] å¤šç§æ–‡æ¡£åŠ è½½å™¨ï¼ˆTextã€PDFã€Wordã€Markdownï¼‰
- [x] æ–‡æ¡£åˆ†å‰²ç­–ç•¥
- [x] RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰
- [x] åˆ†å‰²å‚æ•°é€‰æ‹©ï¼ˆchunk_sizeã€chunk_overlapï¼‰
- [x] æ–‡æ¡£é—®ç­”å’Œæ€»ç»“é¡¹ç›®

### 6.2 æœ€ä½³å®è·µ

**åˆ†å‰²å‚æ•°æ¨èï¼š**

| åœºæ™¯ | chunk_size | chunk_overlap | è¯´æ˜ |
|------|-----------|--------------|------|
| ç²¾ç¡®é—®ç­” | 300-500 | 50-100 | å°å—ï¼Œç²¾ç¡®åŒ¹é… |
| é€šç”¨æ£€ç´¢ | 500-1000 | 100-200 | å¹³è¡¡å¤§å° |
| é•¿æ–‡æœ¬ç”Ÿæˆ | 1000-2000 | 200-300 | å¤§å—ï¼Œä¿æŒä¸Šä¸‹æ–‡ |

### 6.3 ä¸‹å‘¨é¢„ä¹ 

**ç¬¬6å‘¨ä¸»é¢˜ï¼šMemory è®°å¿†ç³»ç»Ÿ**

é¢„ä¹ å†…å®¹ï¼š
1. å¯¹è¯å†å²ç®¡ç†
2. ConversationBufferMemory
3. å‘é‡è®°å¿†ï¼ˆVectorStoreMemoryï¼‰

**æ€è€ƒé—®é¢˜**ï¼š
- å¦‚ä½•å­˜å‚¨å’Œæ£€ç´¢å¯¹è¯å†å²ï¼Ÿ
- å¦‚ä½•é¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Ÿ

---

::: tip å­¦ä¹ å»ºè®®
1. **å¤šå®éªŒåˆ†å‰²å‚æ•°**ï¼šä¸åŒæ–‡æ¡£ç±»å‹éœ€è¦ä¸åŒå‚æ•°
2. **å…³æ³¨è¯­ä¹‰å®Œæ•´æ€§**ï¼šä¼˜å…ˆä½¿ç”¨ RecursiveCharacterTextSplitter
3. **æµ‹è¯•çœŸå®æ–‡æ¡£**ï¼šç”¨å®é™…é¡¹ç›®ä¸­çš„æ–‡æ¡£æµ‹è¯•
4. **ç›‘æ§ Token ä½¿ç”¨**ï¼šåˆ†å‰²åæ³¨æ„ Token æ•°é‡
:::

**æœ¬å‘¨å®Œæˆï¼å‡†å¤‡è¿›å…¥è®°å¿†ç³»ç»Ÿï¼ğŸš€**
